# Introduzione alla sezione {.unnumbered}

Nei capitoli precedenti abbiamo visto come l’inferenza bayesiana possa essere condotta in casi semplici, sfruttando coppie coniugate di distribuzioni o metodi diretti come l’approssimazione su griglia. Questi strumenti ci hanno permesso di capire in modo intuitivo la logica dell’aggiornamento bayesiano, ma hanno anche mostrato i loro limiti: al crescere della complessità dei modelli, i calcoli analitici diventano rapidamente impraticabili e l’approccio su griglia soffre della cosiddetta *maledizione della dimensionalità*.

Per affrontare problemi realistici, dobbiamo introdurre strumenti più potenti. È qui che entrano in gioco i *metodi Monte Carlo a catena di Markov (MCMC)*. Queste procedure ci permettono di generare campioni dalla distribuzione a posteriori anche quando non possiamo descriverla con una formula chiusa, rendendo così l’inferenza bayesiana applicabile a modelli complessi e a più parametri.

In questa sezione esploreremo passo dopo passo le idee alla base degli algoritmi MCMC. Partiremo dal concetto di simulazione Monte Carlo e dall’algoritmo di Metropolis, per poi introdurre strumenti più avanzati che hanno rivoluzionato la pratica dell’inferenza bayesiana, come il linguaggio Stan. Lavoreremo su esempi concreti – dalla stima di una proporzione al confronto tra due gruppi, fino ai modelli di Poisson per dati di conteggio – per mostrare come i metodi MCMC rendano possibile un’analisi che sarebbe inaccessibile con gli strumenti analitici.

Infine, discuteremo i *modelli gerarchici bayesiani*, che estendono ulteriormente le possibilità dell’inferenza permettendo di rappresentare strutture di dati multilivello, frequenti in psicologia e nelle scienze sociali. Questi modelli ci offriranno un primo sguardo a quella che diventerà una delle applicazioni più importanti dell’approccio bayesiano moderno.
