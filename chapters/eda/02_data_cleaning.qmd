# Flusso di lavoro per la pulizia dei dati {#sec-data-cleaning}

::: {.epigraph}
> “Se i dati non sono puliti, le statistiche non hanno senso.”
>
> -- **E.T. Jaynes**, Fisico e Statistico
:::

## Introduzione {.unnumbered .unlisted}

Nonostante la fase più interessante di un progetto di analisi dei dati sia quella in cui si riesce a rispondere alla domanda che ha dato avvio all'indagine, gran parte del tempo di un analista è in realtà dedicata a una fase preliminare: la pulizia e il preprocessing dei dati, operazioni che vengono svolte ancor prima dell'analisi esplorativa.

In questo capitolo, esamineremo un caso concreto di *data cleaning* e preprocessing, seguendo il tutorial di [Crystal Lewis](https://cghlewis.com/blog/data_clean_03/). Il problema viene presentato come segue:

> I am managing data for a longitudinal randomized controlled trial (RCT) study. For this RCT, schools are randomized to either a treatment or control group. Students who are in a treatment school receive a program to boost their math self-efficacy. Data is collected on all students in two waves (wave 1 is in the fall of a school year, and wave 2 is collected in the spring). At this point in time, we have collected wave 1 of our student survey on a paper form and we set up a data entry database for staff to enter the information into. Data has been double-entered, checked for entry errors, and has been exported in a csv format (“w1_mathproj_stu_svy_raw.csv”) to a folder (called “data”) where it is waiting to be cleaned.

Crystal Lewis elenca i seguenti passaggi da seguire nel processo di data cleaning:

1. Revisione dei dati.
2. Regolazione del numero di casi.
3. De-identificazione dei dati.
4. Eliminazione delle colonne irrilevanti.
5. Divisione delle colonne, se necessario.
6. Ridenominazione delle variabili.
7. Trasformazione/normalizzazione delle variabili.
8. Standardizzazione delle variabili.
9. Aggiornamento dei tipi di variabili, se necessario.
10. Ricodifica delle variabili.
11. Creazione di eventuali variabili necessarie.
12. Gestione dei valori mancanti, se necessario.
13. Aggiunta di metadati, se necessario.
14. Validazione dei dati.
15. Fusione e/o unione dei dati, se necessario.
16. Trasformazione dei dati, se necessario.
17. Salvataggio dei dati puliti.

Sebbene l'ordine di questi passaggi sia flessibile e possa essere adattato alle esigenze specifiche, c'è un passaggio che non dovrebbe mai essere saltato: il primo, ovvero la revisione dei dati. Senza una revisione preliminare, l'analista rischia di sprecare ore a pulire i dati per poi scoprire che mancano dei partecipanti, che i dati non sono organizzati come previsto o, peggio ancora, che si sta lavorando con i dati sbagliati.


### Panoramica del capitolo {.unnumbered .unlisted}

- Verificare, pulire e trasformare i dati per l'analisi.
- Documentare il dataset con un dizionario e note esplicative.
- Assicurare validità, unicità e organizzazione dei dati.
- Applicare regole coerenti per denominazione e codifica.

::: {.callout-tip collapse=true}
## Prerequisiti

- Leggere [Cleaning sample data in standardized way](https://cghlewis.com/blog/data_clean_03/) di Crystal Lewis.
- Leggere [Getting Started Creating Data Dictionaries: How to Create a Shareable Data Set](https://journals.sagepub.com/doi/full/10.1177/2515245920928007) di @buchanan2021getting.
- Consultare il capitolo [Documentation](https://datamgmtinedresearch.com/document#document-dictionary) di *Data Management in Large-Scale Education Research*.
- Consultare [How to Make a Data Dictionary](https://help.osf.io/article/217-how-to-make-a-data-dictionary).
- Consultare [data dictionary template](https://osf.io/ynqcu).
:::

::: {.callout-caution collapse=true title="Preparazione del Notebook"}
```{r}
here::here("code", "_common.R") |> 
  source()

# Load packages
if (!requireNamespace("pacman")) install.packages("pacman")
pacman::p_load(mice, labelled, haven, pointblank, mice, purrr, knitr)
```
:::


## Tutorial

Questo tutorial segue i passaggi descritti da [Crystal Lewis](https://cghlewis.com/blog/data_clean_03/) per illustrare le buone pratiche nella gestione e pulizia dei dati.


### Organizzazione dei dati

Un principio fondamentale nella gestione dei dati è preservare l'integrità dei dati grezzi. I dati originali *non devono mai essere modificati direttamente*. È quindi consigliabile strutturare i dati in una directory denominata `data`, suddivisa in due sottocartelle:

- `raw`: contiene i dati originali, mantenuti inalterati.  
- `processed`: destinata ai dati ripuliti e preprocessati.  

Ad esempio, per avviare il processo di pulizia, importiamo i dati da un file denominato `w1_mathproj_stu_svy_raw.csv`. Tutte le operazioni dovranno essere effettuate utilizzando percorsi relativi alla *home directory* del progetto, che definiremo come primo passo.

### Passaggi del tutorial

#### Importare e esaminare i dati

Importiamo i dati utilizzando la funzione `import()` della libreria `rio` e visualizziamo i primi valori di ciascuna colonna per verificarne la corretta importazione:

```{r}
# Importa i dati
svy <- rio::import(here::here("data", "w1_mathproj_stu_svy_raw.csv"))

# Esamina la struttura del dataset
glimpse(svy)
```

Per controllare visivamente i dati, possiamo esaminare le prime e le ultime righe del `data frame`:

```{r}
# Visualizza le prime righe
svy |> 
  head()

# Visualizza le ultime righe
svy |> 
  tail()
```

#### Individuare e rimuovere i duplicati

In questa fase, eseguiamo alcune modifiche necessarie al `data frame`, come rimuovere duplicati e ordinare i dati:

- *Verifica duplicati*: controlliamo i record duplicati nel dataset.
- *Rimuovi duplicati*: manteniamo solo la prima occorrenza.
- *Ordina per data*: organizziamo i record in ordine crescente rispetto alla variabile `svy_date`.
- *Esamina i dati puliti*: controlliamo il risultato delle modifiche.

```{r}
# Identifica i duplicati basati su 'stu_id'
duplicates <- 
  svy[duplicated(svy$stu_id) | duplicated(svy$stu_id, fromLast = TRUE), ]
```

```{r}
# Visualizza i duplicati trovati
duplicates
```

```{r}
# Ordina per 'svy_date' in ordine crescente
svy <- svy[order(svy$svy_date), ]
```

```{r}
# Rimuove i duplicati mantenendo la prima occorrenza
svy <- svy[!duplicated(svy$stu_id), ]
```

```{r}
# Esamina il dataset finale
print(svy)
```

Verifichiamo le dimensioni del dataset pulito per assicurarci che le operazioni siano state eseguite correttamente:

```{r}
# Controlla il numero di righe e colonne
svy |> 
  dim()
```

#### De-identificazione dei dati

```{r}
# Rimuovi la colonna 'svy_date'
svy <- svy |>
  dplyr::select(-svy_date)

# Mostra i nomi delle colonne rimaste
names(svy)
```

#### Rimuovere le colonne non necessarie

Nel caso presente, la rimozione di colonne non è necessaria. Tuttavia, in molti progetti di analisi dei dati, soprattutto quando i dati vengono raccolti utilizzando software di terze parti o strumenti specifici per esperimenti psicologici, è comune trovarsi con colonne che non sono pertinenti allo studio in corso. 

Queste colonne possono includere dati come identificatori interni, timestamp generati automaticamente, informazioni di debug, o variabili che non sono rilevanti per l'analisi che si intende condurre. Quando tali colonne sono irrilevanti per la ricerca, possono essere rimosse per semplificare il dataset e ridurre il rischio di confusione o errori durante l'analisi. Rimuovere le colonne non necessarie non solo rende il dataset più gestibile, ma aiuta anche a focalizzare l'analisi sulle variabili che realmente importano per rispondere alle domande di ricerca.

#### Dividere le colonne secondo necessità

Nel caso presente, questa operazione non è necessaria. Tuttavia, se si lavora con un dataset che include una colonna chiamata "NomeCompleto", contenente sia il nome che il cognome di uno studente, per esempio, è buona pratica separare questa colonna in due colonne distinte, "Nome" e "Cognome". Questa suddivisione facilita l'analisi e la manipolazione dei dati, rendendoli più organizzati e accessibili.

#### Rinominare le colonne

È importante assegnare nomi chiari alle colonne del dataset. Utilizzare nomi di variabili comprensibili aiuta a rendere l'analisi dei dati più intuitiva e a ridurre il rischio di errori interpretativi.

Esempi di buone pratiche:

- Evita nomi di colonne come "x" o acronimi incomprensibili. Questi possono creare confusione durante l'analisi, specialmente se il dataset viene condiviso con altri ricercatori o se viene ripreso dopo un lungo periodo di tempo.
- Invece, cerca di utilizzare nomi di variabili che descrivano chiaramente il contenuto della colonna. Ad esempio, invece di "x1" o "VAR123", un nome come "ansia_base" o "liv_autoefficacia" è molto più comprensibile e immediato.
- Per i nomi composti, utilizza un separatore come il trattino basso `_`. Ad esempio, se stai lavorando con dati relativi a un test psicologico, potresti avere colonne chiamate "test_ansia_pre" e "test_ansia_post" per indicare i risultati del test di ansia prima e dopo un intervento.

Esempi di nomi di colonne ben scelti:

- *Nome generico:* `TS`, `AE`
  - *Nome migliore:* `tempo_studio`, `auto_efficacia`
- *Nome generico:* `S1`, `S2`
  - *Nome migliore:* `stress_situazione1`, `stress_situazione2`
- *Nome generico:* `Q1`, `Q2`
  - *Nome migliore:* `qualità_sonno_sett1`, `qualità_sonno_sett2`

#### Trasformare le variabili

Nel caso presente non si applica, ma è un passo importante in molte analisi dei dati. 

Esempi di trasformazione delle variabili:

- *Logaritmo di una variabile:* Immaginiamo di avere una variabile che misura i tempi di reazione dei partecipanti a un esperimento. Se i tempi di reazione hanno una distribuzione fortemente asimmetrica (con alcuni valori molto elevati), potrebbe essere utile applicare una trasformazione logaritmica per rendere la distribuzione più simmetrica e migliorare l'interpretabilità dei risultati.

- *Codifica delle variabili categoriche:* Se è presente una variabile categorica come il "tipo di intervento" con valori come "cognitivo", "comportamentale" e "farmacologico", potrebbe essere necessario trasformare questa variabile in variabili dummy (ad esempio, `intervento_cognitivo`, `intervento_comportamentale`, `intervento_farmacologico`), dove ogni variabile assume il valore 0 o 1 a seconda della presenza o meno di quel tipo di intervento. Questo è utile quando si utilizzano tecniche di regressione.


#### Standardizzazione delle variabili

La standardizzazione è utile quando si desidera rendere comparabili variabili misurate su scale diverse, ad esempio per confronti tra gruppi o per inclusione in modelli di regressione.  

**Esempio.**  Supponiamo di avere una variabile che misura il livello di ansia su una scala da 0 a 100. Per standardizzarla:  

1. *Sottrai la media* del campione dalla variabile.  
2. *Dividi per la deviazione standard*.  

Il risultato è una variabile con media pari a 0 e deviazione standard pari a 1.  

**Vantaggi della standardizzazione**:  

- facilita l’interpretazione dei coefficienti in un modello di regressione; 
- permette un confronto diretto tra variabili che hanno unità di misura diverse.

#### Normalizzazione delle variabili

La normalizzazione consiste nel ridimensionare i dati su una scala predefinita, spesso compresa tra 0 e 1. Questo processo è particolarmente utile in analisi multivariate, dove variabili con scale molto diverse potrebbero influenzare in modo sproporzionato i risultati.  

**Esempio.** Hai dati su:  

- Ore di sonno (misurate in ore, da 0 a 24).  
- Livello di stress (misurato su una scala da 1 a 50).  
- Auto-efficacia (misurata su una scala da 0 a 100).  

Per garantire che ogni variabile abbia lo stesso peso nell’analisi, puoi normalizzarle usando una formula come:  

$$
x_{\text{norm}} = \frac{x - \text{min}(x)}{\text{max}(x) - \text{min}(x)} .
$$

**Perché Standardizzare o Normalizzare?**

Trasformare le variabili è cruciale per:  

1. *Gestire scale diverse*: Riduce il rischio che variabili con valori numericamente più grandi dominino i risultati.  
2. *Garantire validità e interpretabilità*: Facilita il confronto tra dati provenienti da fonti o gruppi diversi.  
3. *Evitare problemi numerici nei modelli statistici*: Alcuni algoritmi di machine learning o tecniche di ottimizzazione richiedono che i dati siano su scale simili per funzionare correttamente.

In conclusione, la scelta tra standardizzazione e normalizzazione dipende dal contesto dell’analisi e dagli obiettivi specifici. Entrambi i processi sono strumenti indispensabili per garantire che i dati siano trattati in modo adeguato, portando a risultati robusti e facilmente interpretabili.

#### Aggiornare i tipi delle variabili

Nel caso presente non è necessario. Supponiamo invece di avere una colonna in un dataset psicologico che contiene punteggi di un questionario, ma i dati sono stati importati come stringhe (testo) invece che come numeri. Per eseguire calcoli statistici, sarà necessario convertire questa colonna da stringa a numerico. 

In `R`, si potrebbe usare il seguente codice:

```{r eval=FALSE}
# Supponiamo di avere un data frame chiamato 'df' con una colonna 'punteggio' importata come carattere
df$punteggio <- as.numeric(df$punteggio)

# Ora la colonna 'punteggio' è stata convertita in un tipo numerico ed è possibile eseguire calcoli su di essa
```

In questo esempio, la funzione `as.numeric()` viene utilizzata per convertire la colonna `punteggio` in un formato numerico, permettendo di eseguire analisi quantitative sui dati. 

Un altro caso molto comune si verifica quando si importano dati da file Excel. Spesso capita che, all'interno di una cella di una colonna che dovrebbe contenere solo valori numerici, venga inserito erroneamente uno o più caratteri alfanumerici. Di conseguenza, l'intera colonna viene interpretata come di tipo alfanumerico, anche se i valori dovrebbero essere numerici. In questi casi, è fondamentale individuare la cella problematica, correggere manualmente il valore errato, e poi riconvertire l'intera colonna da alfanumerica a numerica.

#### Ricodificare le variabili

Anche se in questo caso non è necessario, la ricodifica delle variabili è una pratica molto comune nelle analisi dei dati psicologici.

Per esempio, consideriamo una variabile categoriale con modalità descritte da stringhe poco comprensibili, che vengono ricodificate con nomi più chiari e comprensibili.

Supponiamo di avere un DataFrame chiamato `df` con una colonna `tipo_intervento` che contiene le modalità `"CT"`, `"BT"`, e `"MT"` per rappresentare rispettivamente "Terapia Cognitiva", "Terapia Comportamentale" e "Terapia Mista". Queste abbreviazioni potrebbero non essere immediatamente chiare a chiunque analizzi i dati, quindi decidiamo di ricodificarle con nomi più espliciti. Ecco come farlo in `R`:

```{r}
# Tibble chiamato 'df' con una colonna 'tipo_intervento'
df <- tibble(tipo_intervento = c("CT", "BT", "MT", "CT", "BT"))
df
```

```{r}
# Ricodifica delle modalità della variabile 'tipo_intervento' in nomi più comprensibili
df <- df %>%
  mutate(tipo_intervento_ricodificato = dplyr::recode(
    tipo_intervento,
    "CT" = "Terapia Cognitiva",
    "BT" = "Terapia Comportamentale",
    "MT" = "Terapia Mista"
  ))

# Mostra il tibble con la nuova colonna ricodificata
df
```

#### Aggiungere nuove variabili nel data frame

Nel caso presente non è richiesto, ma aggiungere nuove variabili a un DataFrame è un'operazione comune durante l'analisi dei dati. Un esempio è il calcolo dell'indice di massa corporea (BMI).

Supponiamo di avere un DataFrame chiamato `df` che contiene le colonne `peso_kg` (peso in chilogrammi) e `altezza_m` (altezza in metri) per ciascun partecipante a uno studio psicologico. Per arricchire il dataset, possiamo calcolare il BMI per ogni partecipante e aggiungerlo come una nuova variabile.

Il BMI si calcola con la formula:

$$ \text{BMI} = \frac{\text{peso in kg}}{\text{altezza in metri}^2} .$$

Ecco come aggiungere la nuova colonna.

```{r}
# Supponiamo di avere un tibble chiamato 'df' con le colonne 'peso_kg' e 'altezza_m'
df <- tibble(
  peso_kg = c(70, 85, 60, 95),
  altezza_m = c(1.75, 1.80, 1.65, 1.90)
)
df
```

```{r}
# Calcola il BMI e aggiungilo come una nuova colonna 'BMI'
df <- df %>%
  mutate(BMI = peso_kg / (altezza_m^2))

# Mostra il tibble con la nuova variabile aggiunta
df
```

### Affrontare il problema dei dati mancanti

I dati mancanti sono un problema comune nelle ricerche psicologiche e in molte altre discipline. Quando mancano delle informazioni in un dataset, possono verificarsi gravi problemi per l'analisi statistica, come risultati distorti, riduzione della precisione delle stime o, in alcuni casi, l'impossibilità di applicare alcuni algoritmi.

#### Perché i dati mancanti sono un problema?

Immagina di voler capire il rendimento medio di una classe in un test, ma alcuni studenti hanno lasciato delle risposte in bianco. Se ignoriamo le risposte mancanti o eliminiamo gli studenti con dati incompleti, rischiamo di ottenere una stima che non rappresenta correttamente la realtà. Questo succede perché:

- *Bias dei risultati*: Se i dati mancanti non sono casuali (ad esempio, i dati mancanti sono presenti più spesso in studenti con basso rendimento), le conclusioni possono essere errate.
- *Riduzione della potenza statistica*: Eliminare dati incompleti riduce il numero totale di osservazioni, rendendo più difficile trovare risultati significativi.
- *Impossibilità di applicare alcuni metodi*: Molti algoritmi statistici richiedono dati completi e non funzionano con valori mancanti.

#### Come gestire i dati mancanti?

Ci sono diversi modi per affrontare i dati mancanti. Vediamo prima i metodi più semplici e poi quelli più avanzati.

Esaminiamo il data frame iniziale:

```{r}
svy |> 
  summary()
```

Si noti la presenza di dati mancanti sulle variabili `math3` e `math4`.

```{r}
dim(svy)
```

1. **Esclusione dei Casi Incompleti (Complete Case Analysis).**  
   Un approccio comune, ma spesso non ideale, è quello di analizzare solo i casi completi, eliminando tutte le righe con valori mancanti. In R, questo si può fare con il seguente comando:

   ```{r}
   svy_comp <- svy |>
     drop_na()
   ```
   
   In questo modo abbiamo escluso tutte le righe nelle quali sono presenti dei dati mancanti (in questo caso, una sola riga).
   
   ```{r}
   dim(svy_comp)
   ```
   
   **Limite**: Questo metodo può introdurre bias se i dati mancanti non sono casuali e riduce il campione, compromettendo l'affidabilità delle analisi.

2. **Imputazione Semplice**  
   Sostituire i valori mancanti con stime semplici:
   - *Media o mediana*: Per le variabili numeriche, sostituire i valori mancanti con la media o la mediana. Questo metodo è facile da implementare, ma può ridurre la variabilità nei dati.
   - *Moda*: Per le variabili categoriche, sostituire i valori mancanti con il valore più frequente. Tuttavia, può introdurre distorsioni se i dati sono molto eterogenei.

3. **Imputazione Multipla**  
   Un approccio più avanzato è l'imputazione multipla, che utilizza modelli statistici per stimare i valori mancanti in modo iterativo. L'idea di base è semplice: ogni valore mancante viene stimato tenendo conto delle relazioni con tutte le altre variabili.

   *Vantaggi*:
   - Mantiene la variabilità dei dati.
   - Preserva le relazioni tra le variabili.
   - Riduce il rischio di bias rispetto ai metodi semplici.

#### Applicazione pratica: imputazione multipla con `mice` in R

Supponiamo di avere un dataset con alcune colonne numeriche che contengono valori mancanti. Possiamo utilizzare il pacchetto `mice` per imputare i dati mancanti.

Selezioniamo le colonne numeriche da imputare.

```{r}
numeric_columns <- c("math1", "math2", "math3", "math4")
svy <- svy %>%
  mutate(across(all_of(numeric_columns), as.numeric))
```

Eseguiamo l'imputazione multipla.

```{r}
imputed <- mice(
  svy[numeric_columns], 
  m = 1, 
  maxit = 10, 
  method = "norm.predict", 
  seed = 123
)
```

Ottieniamo il dataset con i valori imputati.

```{r}
svy_imputed <- complete(imputed)
```

Arrotondiamo i valori imputati (se necessario).

```{r}
svy_imputed <- svy_imputed %>%
  mutate(across(everything(), round))
```

Sostituiamo i valori imputati nel dataset originale.

```{r}
svy[numeric_columns] <- svy_imputed
```

Esaminiamo il risultato ottenuto.

```{r}
svy |> 
  summary()
```

#### Come funziona l’imputazione multipla?

1. Ogni variabile con valori mancanti viene modellata come funzione delle altre variabili.
2. I valori mancanti vengono stimati iterativamente. In ogni iterazione, si utilizza l'output precedente come input per migliorare le stime.
3. Dopo un numero sufficiente di iterazioni, le stime si stabilizzano (convergenza).

In conclusione, l'imputazione multipla è una tecnica avanzata che permette di gestire i dati mancanti preservando la qualità delle analisi. Rispetto ai metodi semplici, consente di mantenere la variabilità e ridurre il rischio di bias, rendendola una scelta ideale per analisi psicologiche e di ricerca. 

#### Aggiungere i metadati

I *metadati* sono informazioni che descrivono i dati stessi, come etichette di variabili, etichette di valori, informazioni sull'origine dei dati, unità di misura e altro ancora. Questi metadati sono essenziali per comprendere, documentare e condividere correttamente un dataset.

In R, i metadati sono gestiti in modo molto dettagliato e strutturato attraverso pacchetti come `haven`, `labelled`, e `Hmisc`. Questi pacchetti consentono di associare etichette ai dati, come etichette di variabili e di valori, e persino di gestire i valori mancanti con etichette specifiche.

- *Etichette di variabili*: Si possono aggiungere direttamente alle colonne di un DataFrame usando funzioni come `labelled::set_variable_labels()`.
- *Etichette di valori*: Possono essere aggiunte a variabili categoriali utilizzando `labelled::labelled()`.
- *Valori mancanti*: In R, è possibile etichettare specifici valori come mancanti usando `labelled::na_values<-`.

Questi strumenti rendono molto facile documentare un dataset all'interno del processo di analisi, assicurando che tutte le informazioni critiche sui dati siano facilmente accessibili e ben documentate.

Esaminiamo un esempio pratico. Consideriamo nuovamente il data set `svy`:

```{r}
glimpse(svy)
```

Si noti che la variabile `math2` contiene un valore inamissibile, probabilmente un errore di battitura. Questo fa in modo che `math2` sia di tipo `char` mentre dovrebbe essere una variabile numerica. Correggiamo.

```{r}
# Correzione di `math2`: Rimuovi valori non validi e converti in numerico
svy$math2 <- gsub("\\n", "", svy$math2)  # Rimuovi caratteri non validi come '\n'
svy$math2 <- as.numeric(svy$math2)      # Converte la variabile in numerico

# Visualizzazione del dataset corretto
print(svy)
```

Definiamo le etichette di valore per le variabili `math1:math4`.

```{r}
value_labels_math <- set_names(
  as.numeric(names(c(
    `1` = "strongly disagree",
    `2` = "disagree",
    `3` = "agree",
    `4` = "strongly agree"
  ))),
  c("strongly disagree", "disagree", "agree", "strongly agree")
)
```

Aggiungiamo le etichette di valore alle colonne `math1:math4`.

```{r}
svy <- svy %>%
  mutate(across(starts_with("math"), ~ labelled(., labels = value_labels_math)))
```

Verifica delle etichette.

```{r}
val_labels(svy$math1)
```

```{r}
svy
```

##### Utilizzo delle etichette in R con variabili numeriche

Le etichette dei valori (*value labels*) vengono utilizzate per rendere più leggibili e interpretabili le variabili numeriche, associando ad ogni valore un'etichetta descrittiva. Questo approccio è particolarmente utile in ambiti come la ricerca  psicologica, dove le risposte ai questionari sono spesso codificate con numeri (ad esempio, 1 = "Strongly Disagree", 2 = "Disagree", ecc.) ma rappresentano concetti qualitativi.

**Vantaggi dell’Uso delle Etichette**

1. *Chiarezza nelle Analisi*: Le etichette descrittive rendono i dati più facilmente comprensibili senza dover ricordare il significato numerico di ciascun valore.  
2. *Documentazione Integrata*: Permettono di incorporare metadati direttamente nelle variabili, migliorando la trasparenza e riducendo il rischio di interpretazioni errate.
3. *Compatibilità con Software Statistici*: Molti strumenti (ad esempio, SPSS o Stata) utilizzano etichette di valori. Il pacchetto `haven` in R consente di gestire facilmente i dati etichettati esportati/importati da questi software.

**Manipolazione di Variabili Etichettate**

Anche se una variabile numerica è etichettata con `labelled` (ad esempio, tramite il pacchetto `haven`), essa conserva la sua natura numerica e può essere utilizzata in calcoli, modelli statistici, e trasformazioni. Le etichette non alterano il valore sottostante, ma lo arricchiscono con informazioni aggiuntive. 

In conclusione, le etichette dei valori migliorano l'interpretabilità dei dati senza comprometterne la manipolabilità. Questo approccio è ideale per mantenere le variabili numeriche pienamente funzionali per analisi statistiche, mentre le etichette descrittive forniscono un contesto chiaro e leggibile.

#### Validazione dei dati

La validazione dei dati è un passaggio fondamentale per garantire che il dataset soddisfi i criteri previsti e sia pronto per le analisi successive. Questo processo include il controllo della coerenza e della correttezza dei dati in base a specifiche regole definite dal dizionario dei dati. Alcune verifiche comuni includono:

- *Unicità delle righe*: Assicurarsi che ogni riga sia unica, verificando l'assenza di ID duplicati.
- *Validità degli ID*: Controllare che gli ID rientrino in un intervallo previsto (es. numerico).
- *Valori accettabili nelle variabili categoriali*: Verificare che variabili come `grade_level`, `int` e le colonne `math` contengano esclusivamente valori appartenenti a un set di valori validi.

Il pacchetto [pointblank](https://rstudio.github.io/pointblank/) fornisce strumenti flessibili e intuitivi per eseguire verifiche di validazione e generare report dettagliati. Questo pacchetto consente di:

- *Definire le regole di validazione*: Specificare controlli come unicità, intervalli di valori e appartenenza a insiemi predefiniti.
- *Eseguire i controlli*: Applicare le regole di validazione su un dataset per identificare eventuali discrepanze.
- *Generare report interattivi*: Creare un riepilogo chiaro e visivo dei controlli, evidenziando eventuali errori o anomalie.

Con `pointblank`, è possibile integrare la validazione dei dati come parte di un workflow strutturato, garantendo la qualità dei dati in modo sistematico e ripetibile.

```r
create_agent(svy) %>%
  rows_distinct(columns = vars(stu_id)) %>%
  col_vals_between(columns = c(stu_id), 
                   left = 1300, right = 1400, na_pass = TRUE) %>%
  col_vals_in_set(columns = c(grade_level), 
                  set = c(9, 10, 11, 12, NA)) %>%
  col_vals_in_set(columns = c(int),
                  set = c(0, 1, NA)) %>%
  col_vals_in_set(columns = c(math1:math4),
                  set = c(1, 2, 3, 4, NA)) %>%
  interrogate()
```

Il dataset ripulito soddisfa tutte le aspettative delineate da Crystal Lewis.

- *Completo*: Tutti i dati raccolti sono stati inseriti e/o recuperati. Non dovrebbero esserci dati estranei che non appartengono al dataset (come duplicati o partecipanti non autorizzati). 
- *Valido*: Le variabili rispettano i vincoli definiti nel tuo dizionario dei dati. Ricorda che il dizionario dei dati specifica i nomi delle variabili, i tipi, i range, le categorie e altre informazioni attese.
- *Accurato*: Sebbene non sia sempre possibile determinare l'accuratezza dei valori durante il processo di pulizia dei dati (ovvero, se un valore è realmente corretto o meno), in alcuni casi è possibile valutarla sulla base della conoscenza pregressa riguardante quel partecipante o caso specifico.
- *Coerente*: I valori sono allineati tra le varie fonti. Ad esempio, la data di nascita raccolta attraverso un sondaggio studentesco dovrebbe avere un formato corrispondere alla data di nascita raccolta dal distretto scolastico.
- *Uniforme*: I dati sono standardizzati attraverso i moduli e nel tempo. Ad esempio, lo stato di partecipazione ai programmi di pranzo gratuito o a prezzo ridotto è sempre fornito come una variabile numerica con la stessa rappresentazione, oppure il nome della scuola è sempre scritto in modo coerente in tutto il dataset.
- *De-identificato*: Tutte le informazioni personali identificabili (PII) sono state rimosse dal dataset per proteggere la riservatezza dei partecipanti (se richiesto dal comitato etico/consenso informato).
- *Interpretabile*: I dati hanno nomi di variabili leggibili sia da umani che dal computer, e sono presenti etichette di variabili e valori laddove necessario per facilitare l'interpretazione.
- *Analizzabile*: Il dataset è in un formato rettangolare (righe e colonne), leggibile dal computer e conforme alle regole di base della struttura dei dati.

Una volta completati i 14 passaggi precedenti, è possibile esportare questo dataset ripulito nella cartella `processed` per le successive analisi statistiche.

#### Unire e/o aggiungere dati se necessario

In questo passaggio, è possibile unire o aggiungere colonne o righe presenti in file diversi. È importante eseguire nuovamente i controlli di validazione dopo l'unione/aggiunta di nuovi dati.

#### Trasformare i dati se necessario

Esistono vari motivi per cui potrebbe essere utile memorizzare i dati in formato `long` o `wide`. In questo passaggio, è possibile ristrutturare i dati secondo le esigenze.

#### Salvare il dataset pulito finale

L'ultimo passaggio del processo di pulizia consiste nell'esportare o salvare il dataset pulito. Come accennato in precedenza, può essere utile esportare/salvare il dataset in più di un formato di file (ad esempio, un file `.csv` e un file `.parquet`).

## Organizzazione dei file e informazioni aggiuntive

Infine, è essenziale includere una documentazione adeguata per garantire che le informazioni siano interpretate correttamente, sia da altri utenti che da te stesso, se dovessi tornare a lavorare su questo progetto in futuro. La documentazione minima da fornire dovrebbe includere:

- *Documentazione a livello di progetto*: Questa sezione fornisce informazioni contestuali sul perché e come i dati sono stati raccolti. È utile per chiunque voglia comprendere lo scopo e la metodologia del progetto.
- *Metadati a livello di progetto*: Se condividi i dati in un repository pubblico o privato, è importante includere metadati a livello di progetto. Questi metadati forniscono informazioni dettagliate che facilitano la ricerca, la comprensione e la consultabilità dei dati. I metadati a livello di progetto possono includere descrizioni generali del progetto, parole chiave, e riferimenti bibliografici.
- *Dizionario dei dati*: Un documento che descrive tutte le variabili presenti nel dataset, inclusi i loro nomi, tipi, range di valori, categorie e qualsiasi altra informazione rilevante. Questo strumento è fondamentale per chiunque voglia comprendere o analizzare i dati.
- *README*: Un file che fornisce una panoramica rapida dei file inclusi nel progetto, spiegando cosa contengono e come sono interconnessi. Il README è spesso il primo documento consultato e serve a orientare l'utente tra i vari file e risorse del progetto. 
Questa documentazione non solo aiuta a mantenere il progetto organizzato, ma è anche cruciale per facilitare la collaborazione e l'archiviazione a lungo termine.

## Dizionario dei dati

Approfondiamo qui il problema della creazione del Dizionario dei dati. 

Un dizionario dei dati è un documento che descrive le caratteristiche di ciascuna variabile in un dataset. Include informazioni come il nome della variabile, il tipo di dato, il range di valori, le categorie (per le variabili categoriche), e altre informazioni rilevanti. Questo strumento è essenziale per comprendere e analizzare correttamente il dataset.

Si presti particolare attenzione alle [guide di stile](https://datamgmtinedresearch.com/style) per la denominazione delle variabili e la codifica dei valori delle risposte.

### Esempio in `R`

Ecco come tradurre i passi per creare un dizionario dei dati in R, utilizzando il pacchetto `tibble` per creare il dizionario e `writexl` o `readr` per esportarlo in formato `.xlsx` o `.csv`.

1. *Identificare le variabili*: Elencare tutte le variabili presenti nel dataset.
2. *Descrivere ogni variabile*: Per ciascuna variabile, definire il tipo (ad esempio, `integer`, `numeric`, `character`), il range di valori accettabili o le categorie, e fornire una descrizione chiara.
3. *Salvare il dizionario dei dati*: Il dizionario può essere salvato in un file `.csv` o `.xlsx` per una facile consultazione.

```r
svy
```

Creeremo un dizionario dei dati per un dataset di esempio e lo salveremo sia in formato CSV che Excel.

```r
library(tibble)
library(readr)
library(writexl)

# Creazione del Dizionario dei Dati
data_dict <- tibble(
  `Variable Name` = c(
    "stu_id",
    "svy_date",
    "grade_level",
    "math1",
    "math2",
    "math3",
    "math4"
  ),
  `Type` = c(
    "integer",
    "datetime",
    "integer",
    "integer",
    "integer",
    "numeric",
    "numeric"
  ),
  `Description` = c(
    "Student ID",
    "Survey Date",
    "Grade Level",
    "Math Response 1 (1: Strongly Disagree, 4: Strongly Agree)",
    "Math Response 2 (1: Strongly Disagree, 4: Strongly Agree)",
    "Math Response 3 (1: Strongly Disagree, 4: Strongly Agree)",
    "Math Response 4 (1: Strongly Disagree, 4: Strongly Agree)"
  ),
  `Range/Values` = c(
    "1347-1399",
    "2023-02-13 to 2023-02-14",
    "9-12",
    "1-4",
    "1-4",
    "1.0-4.0 (NA allowed)",
    "1.0-4.0 (NA allowed)"
  )
)

# Visualizza il Dizionario dei Dati
print(data_dict)

# Salva il Dizionario dei Dati in un file CSV
write_csv(data_dict, here::here("data", "processed", "data_dictionary.csv"))

# Salva il Dizionario dei Dati in un file Excel
write_xlsx(data_dict, here::here("data", "processed", "data_dictionary.xlsx"))
```

Output Atteso: file CSV (`data_dictionary.csv`).

```r
data_dict <- rio::import(
  here::here("data", "processed", "data_dictionary.csv")
)
```

```r
print(data_dict)
```

#### Uso del pacchetto `dataMeta`

Il pacchetto `dataMeta` è progettato per generare metadati e dizionari dei dati in modo strutturato.

```{r}
library(dataMeta)
library(tibble)

# Descrizioni delle variabili
variable_descriptions <- c(
  "Student ID",
  "Grade Level",
  "Math Response 1 (1: Strongly Disagree, 4: Strongly Agree)",
  "Math Response 2 (1: Strongly Disagree, 4: Strongly Agree)",
  "Math Response 3 (1: Strongly Disagree, 4: Strongly Agree)",
  "Math Response 4 (1: Strongly Disagree, 4: Strongly Agree)"
)

var_type <- c(1, 0, 0, 0, 0, 0)

linker <- build_linker(
  svy, 
  variable_description = variable_descriptions, 
  variable_type = var_type
)

dict <- build_dict(
  my.data = svy, 
  linker = linker, 
  option_description = NULL, 
  prompt_varopts = FALSE
)

kable(dict, format = "html", caption = "Data dictionary for original dataset")
```

#### Uso del pacchetto `skimr`

Il pacchetto `skimr` è utile per generare riassunti dettagliati delle variabili, che possono essere utilizzati come base per un dizionario.

```{r}
library(skimr)

# Riassunto del dataset
skim_dict <- skim(svy)

kable(skim_dict, format = "html", caption = "Data dictionary for original dataset")
```

## Riflessioni conclusive {.unnumbered .unlisted}

Nel processo di analisi dei dati, la fase di pulizia e pre-elaborazione è cruciale per garantire la qualità e l'integrità dei risultati finali. Sebbene questa fase possa sembrare meno interessante rispetto all'analisi vera e propria, essa costituisce la base su cui si costruiscono tutte le successive elaborazioni e interpretazioni. Attraverso una serie di passaggi strutturati, come quelli illustrati in questo capitolo, è possibile trasformare dati grezzi e disordinati in un dataset pulito, coerente e pronto per l'analisi. La cura nella gestione dei dati, dalla rimozione di duplicati alla creazione di un dizionario dei dati, è fondamentale per ottenere risultati affidabili e riproducibili.


## Esercizi {.unnumbered .unlisted}

::: {.callout-tip title="Esercizio" collapse="true"}
In questo esercizio, applicherai le tecniche di pulizia e preprocessing dei dati utilizzando il dataset SWLS. Il tuo compito è seguire i passaggi descritti per trasformare il dataset in una forma pronta per l'analisi.

**Istruzioni**

1. **Importa i dati SWLS** (Survey of Life Satisfaction Scale). Introduci almeno due dati mancanti nei dati e almeno un duplicato.
2. **Controlla i dati**: verifica la struttura e individua eventuali anomalie.
3. **Pulisci i dati**:
   - Rimuovi eventuali duplicati.
   - Gestisci i valori mancanti in modo appropriato.
   - Rinomina le variabili per una maggiore chiarezza.
   - Standardizza alcune variabili per l'analisi.
4. **Documenta le modifiche effettuate**.
5. **Esporta il dataset pulito**.

**Consegna**

- Salva il tuo file Quarto con il nome `swls_cleaning.qmd`.
- Usa questo header YAML:

  ```
  ---
  title: "Assegnamento: Pulizia e Preprocessing dei Dati SWLS"
  author: "Nome Studente"
  date: "`r Sys.Date()`"
  format: html
  ---
```
- Assicurati che il codice sia commentato e spiegato chiaramente.
- Esporta il dataset pulito e allegalo alla consegna.
:::


::: {.callout-note collapse=true title="Informazioni sull'ambiente di sviluppo"}
```{r}
sessionInfo()
```
:::

## Bibliografia {.unnumbered .unlisted}

