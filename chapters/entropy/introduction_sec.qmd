# Introduzione alla sezione {.unnumbered .unlisted}

Nelle sezioni precedenti abbiamo imparato a *descrivere* e *stimare* modelli, soprattutto in chiave bayesiana. Con *Entropia* facciamo un passo laterale: introduciamo gli strumenti della *teoria dell’informazione* per dare un contenuto quantitativo all’idea di *incertezza* e per collegarla, in modo operativo, alla *valutazione dei modelli*.

Partiremo dall’*entropia di Shannon* come misura media della “sorpresa” di una distribuzione: massima quando gli esiti sono equiprobabili, minima quando uno solo è (quasi) certo. Questo introduce il bit come unità naturale dell’informazione e connette l’entropia a problemi concreti di rappresentazione e compressione (es. codifica di Huffman). 

Poi passeremo alla *divergenza di Kullback–Leibler (KL)*, che non misura più l’incertezza di *una* distribuzione, ma la distanza informativa tra *due* distribuzioni: “quanta informazione in più paghiamo utilizzando il modello $Q$ quando la realtà segue $P$?” Questa idea — l’*entropia relativa* — è il ponte concettuale tra informazione e *scelta del modello*.

Infine useremo KL per motivare gli strumenti pratici della *valutazione predittiva* in ambito bayesiano: *log-score*, *ELPD*, *LOO-CV* e *WAIC*. Qui il focus si sposta dall’“adattamento ai dati” alla *capacità di generalizzare* a dati nuovi, ovvero alla qualità delle previsioni posteriori: massimizzare l’ELPD equivale (in media) ad avvicinare il nostro modello alla distribuzione che genera i dati. 

Il filo conduttore resta quello del manuale: trattare l’*incertezza* in modo esplicito e *valutare* i modelli con criteri coerenti con le nostre domande sostantive, evitando verdetti dicotomici e privilegiando grandezza degli effetti e capacità predittiva.


