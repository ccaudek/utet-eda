[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Analisi esplorativa dei dati (EDA) in psicologia",
    "section": "",
    "text": "Percorso e obiettivi\nQuesto sito è il modulo sull’analisi esplorativa dei dati (EDA) del progetto UTET a supporto del manuale Metodi bayesiani in psicologia.\nOffre un percorso introduttivo alla preparazione, esplorazione e visualizzazione dei dati psicologici, come primo passo essenziale prima dell’inferenza statistica.\nL’obiettivo è costruire solide abitudini di analisi preliminare, indispensabili per una corretta inferenza.",
    "crumbs": [
      "Percorso e obiettivi"
    ]
  },
  {
    "objectID": "index.html#percorso-e-obiettivi",
    "href": "index.html#percorso-e-obiettivi",
    "title": "Analisi esplorativa dei dati (EDA) in psicologia",
    "section": "",
    "text": "Strutturare un progetto: organizzazione dei dati e file.\n\nPulizia dei dati: gestione dei valori mancanti e dei formati.\n\nDescrizione: esplorare variabili qualitative e quantitative.\n\nVisualizzazione: grafici per pattern, distribuzioni, relazioni.\n\nMisure di tendenza e dispersione: localizzazione e scala.\n\nConcetti propedeutici: distribuzione normale, correlazione, causalità, outlier.",
    "crumbs": [
      "Percorso e obiettivi"
    ]
  },
  {
    "objectID": "index.html#come-usare-il-modulo",
    "href": "index.html#come-usare-il-modulo",
    "title": "Analisi esplorativa dei dati (EDA) in psicologia",
    "section": "Come usare il modulo",
    "text": "Come usare il modulo\n\nProcedi con esempi pratici: i capitoli offrono codice e visualizzazioni riproducibili.\n\nCollega al manuale: l’EDA è la porta d’ingresso per comprendere i modelli e l’inferenza.\n\nRagiona criticamente: non limitarti a “fare grafici”, chiediti sempre cosa raccontano sui dati.",
    "crumbs": [
      "Percorso e obiettivi"
    ]
  },
  {
    "objectID": "index.html#licenza-duso",
    "href": "index.html#licenza-duso",
    "title": "Analisi esplorativa dei dati (EDA) in psicologia",
    "section": "Licenza d’uso",
    "text": "Licenza d’uso\nMateriali distribuiti con licenza\nCC BY-NC-ND 4.0.\nCondivisione consentita con attribuzione, solo per usi non commerciali e senza modifiche.",
    "crumbs": [
      "Percorso e obiettivi"
    ]
  },
  {
    "objectID": "prefazione.html",
    "href": "prefazione.html",
    "title": "1  Prefazione",
    "section": "",
    "text": "1.1 A chi è destinato\nQuesto modulo sull’analisi esplorativa dei dati (EDA) fa parte dell’ecosistema UTET che accompagna il manuale Metodi bayesiani in psicologia.\nIl suo scopo è mostrare come un’analisi accurata e sistematica dei dati costituisca il primo passo verso inferenze affidabili e modelli ben specificati.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Prefazione</span>"
    ]
  },
  {
    "objectID": "prefazione.html#a-chi-è-destinato",
    "href": "prefazione.html#a-chi-è-destinato",
    "title": "1  Prefazione",
    "section": "",
    "text": "A chi si avvicina per la prima volta all’analisi dei dati in psicologia.\n\nA chi desidera rafforzare le proprie competenze nella preparazione e nella visualizzazione dei dati, come base per l’inferenza.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Prefazione</span>"
    ]
  },
  {
    "objectID": "prefazione.html#cosa-troverai",
    "href": "prefazione.html#cosa-troverai",
    "title": "1  Prefazione",
    "section": "1.2 Cosa troverai",
    "text": "1.2 Cosa troverai\n\nConcetti chiave di pulizia, esplorazione e visualizzazione.\n\nTecniche per la descrizione di variabili qualitative e quantitative.\n\nRichiami su distribuzione normale, correlazione, causalità e gestione degli outlier.\n\nCollegamenti al manuale per capire come i dati esplorati diventano input per i modelli statistici.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Prefazione</span>"
    ]
  },
  {
    "objectID": "prefazione.html#come-leggere",
    "href": "prefazione.html#come-leggere",
    "title": "1  Prefazione",
    "section": "1.3 Come leggere",
    "text": "1.3 Come leggere\n\nPuoi seguire il percorso in sequenza, come una guida pratica.\n\nOppure utilizzare i capitoli come riferimento rapido durante il lavoro sui dati.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Prefazione</span>"
    ]
  },
  {
    "objectID": "prefazione.html#ringraziamenti",
    "href": "prefazione.html#ringraziamenti",
    "title": "1  Prefazione",
    "section": "1.4 Ringraziamenti",
    "text": "1.4 Ringraziamenti\nQuesto modulo si inserisce nel progetto UTET per una formazione metodologica integrata, che parte dall’analisi preliminare e si estende fino ai modelli avanzati.\nSuggerimenti e correzioni possono essere inviati tramite il sistema di issue del repository.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Prefazione</span>"
    ]
  },
  {
    "objectID": "prefazione.html#licenza",
    "href": "prefazione.html#licenza",
    "title": "1  Prefazione",
    "section": "1.5 Licenza",
    "text": "1.5 Licenza\nMateriali rilasciati con licenza CC BY-NC-ND 4.0.\nCondivisione consentita con attribuzione; non è consentito l’uso commerciale né la creazione di opere derivate.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Prefazione</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/introduction_key_notions.html",
    "href": "chapters/key_notions/introduction_key_notions.html",
    "title": "Fondamenti",
    "section": "",
    "text": "La data science è un campo che si sviluppa all’intersezione tra la statistica e l’informatica. La statistica fornisce una serie di metodologie per analizzare i dati e ottenere informazioni significative, mentre l’informatica si occupa dello sviluppo di software e strumenti per implementare tali metodologie. In questa sezione della dispensa, approfondiremo alcuni concetti fondamentali della statistica e della misurazione psicologica. Considereremo anche in termini generali quali sono gli obiettivi e i limiti dell’analisi dei dati psicologici.",
    "crumbs": [
      "Fondamenti"
    ]
  },
  {
    "objectID": "chapters/key_notions/01_data_analysis.html",
    "href": "chapters/key_notions/01_data_analysis.html",
    "title": "2  La crisi di replicazione e la riforma metodologica in psicologia",
    "section": "",
    "text": "Introduzione\nNegli ultimi vent’anni, la psicologia ha attraversato una trasformazione metodologica profonda, innescata da una crescente consapevolezza dei propri limiti empirici. Questa trasformazione prende il nome di crisi di replicazione (Baker, 2016; Bishop, 2019). Numerosi tentativi sistematici di replicare effetti pubblicati in studi classici hanno rivelato un tasso sorprendentemente alto di fallimenti. In molti casi, i risultati non solo non si replicano con la stessa ampiezza, ma talvolta non emergono affatto. Questa situazione ha costretto l’intera disciplina a interrogarsi sulla solidità delle proprie basi empiriche.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>La crisi di replicazione e la riforma metodologica in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/01_data_analysis.html#introduzione",
    "href": "chapters/key_notions/01_data_analysis.html#introduzione",
    "title": "2  La crisi di replicazione e la riforma metodologica in psicologia",
    "section": "",
    "text": "Panoramica del capitolo\n\nDistinguere replicazione, riproducibilità, robustezza e generalizzabilità (criteri diversi, complementari).\nCause principali: bassa potenza, flessibilità analitica (forking paths), HARKing, bias di pubblicazione, scarsa affidabilità.\nRiforme: preregistrazione/Registered Reports, open data & code, versionamento (Git/GitHub), report riproducibili (Quarto).\nSvolta bayesiana: prior + prior predictive, MCMC, posterior predictive checks, LOO/ELPD, analisi di sensibilità; modelli gerarchici.\nChecklist pratica: progetto adeguato, misure affidabili, evitare dichotomie p&lt;.05, riportare effetti con incertezza e soglie di rilevanza pratica.\n\n\n\n\n\n\n\nPrerequisiti\n\n\n\n\n\n\nLeggere Statistical Rethinking (McElreath, 2020). Focalizzati sui primi capitoli dove si discute della dicotomia tra “small world” e “big world”.\nLeggere What has happened down here is the winds have changed (Gelman 2016). Un post sul blog di Andrew Gelman che fornisce una panoramica sulla crisi di replicazione e su come le scienze sociali sono cambiate di conseguenza.\nLeggere Productive Explanation: A Framework for Evaluating Explanations in Psychological Science. L’adozione di teorie formali è essenziale per affrontare la crisi di riproducibilità dei risultati nella ricerca psicologica.\nPer chi fosse interessato a un romanzo su questi temi, sorprendentemente avvincente, consiglio Quando abbiamo smesso di capire il mondo di Benjamín Labatut (Labatut, 2021).\n\n\n\n\n\n\n2.0.1 Che cos’è la replicazione\nReplicare un risultato non significa semplicemente ottenere un nuovo p-value inferiore a .05. Significa ripetere un esperimento in condizioni il più possibile simili all’originale e ottenere una stima dell’effetto compatibile con quella iniziale, in termini sia di direzione che di grandezza. La replicabilità è uno dei pilastri fondamentali della scienza empirica: se un risultato rappresenta un fenomeno reale e generalizzabile, dovrebbe emergere anche in campioni indipendenti.\n\n\n2.0.2 I numeri della crisi\nLa portata della crisi è stata messa in evidenza dall’Collaboration (2015), che ha tentato di replicare 100 studi pubblicati su riviste leader nel settore. Solo il 36% delle repliche ha prodotto risultati “significativi” nello stesso senso degli studi originali. Questo valore non va inteso come una misura bayesiana di probabilità, ma come un indicatore allarmante di quanto i risultati pubblicati siano sensibili alle condizioni sperimentali, ai campioni, e alle analisi.\nL’evidenza si è accumulata con studi successivi: Camerer et al. (2018) hanno mostrato una riproducibilità deludente anche in economia comportamentale, mentre Klein et al. (2014) hanno riportato effetti inconsistenti in psicologia sociale. In molti casi, i risultati originali si sono dimostrati fragili, condizionali, o il prodotto di scelte analitiche arbitrarie.\n\n\n2.0.3 Il fallimento della replicazione è un sintomo\nPiù che una patologia in sé, il fallimento della replicazione è un sintomo di un problema più ampio: un’adozione acritica e routinaria del paradigma frequentista, in particolare del Null Hypothesis Significance Testing (NHST). Questo approccio, se non applicato con estrema cautela, incentiva strategie di analisi discutibili. La dipendenza da soglie fisse come p &lt; .05, la flessibilità nel trattamento dei dati, e la pratica di adattare le ipotesi a posteriori (HARKing), contribuiscono ad amplificare l’illusione della scoperta anche in assenza di effetti reali.\nLa statistica frequentista tradizionale, centrata sul concetto di errore di I e II specie, può indurre a interpretazioni errate e a una eccessiva enfasi su esiti binari (significativo/non significativo). Questa mentalità ha contribuito alla diffusione di falsi positivi, alla scarsa trasparenza nelle analisi, e a una generale crisi di credibilità nella letteratura psicologica (Ioannidis, 2005; Meehl, 1967).\n\n\n2.0.4 Conseguenze scientifiche e sociali\nIl risultato è un panorama in cui non è più chiaro quali risultati siano attendibili. Gli effetti di questa incertezza non sono solo accademici: si riflettono nella perdita di fiducia da parte del pubblico, nello spreco di risorse su linee di ricerca inconsistenti, e in una generale difficoltà a costruire teorie cumulative. Tuttavia, questa crisi ha innescato una risposta positiva, nota come Credibility Revolution (Angrist & Pischke, 2010), che mira a riformare alla radice le pratiche di ricerca, ponendo l’accento su rigore metodologico, trasparenza, e apertura.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>La crisi di replicazione e la riforma metodologica in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/01_data_analysis.html#una-via-duscita-la-rivoluzione-bayesiana",
    "href": "chapters/key_notions/01_data_analysis.html#una-via-duscita-la-rivoluzione-bayesiana",
    "title": "2  La crisi di replicazione e la riforma metodologica in psicologia",
    "section": "2.1 Una via d’uscita: la rivoluzione bayesiana",
    "text": "2.1 Una via d’uscita: la rivoluzione bayesiana\nIn questo contesto, l’approccio bayesiano si è imposto come una delle risposte più promettenti. La statistica bayesiana si fonda sull’idea che la conoscenza scientifica non sia binaria (vero/falso), ma debba essere espressa in termini di gradi di credenza che evolvono nel tempo. L’inferenza diventa allora un processo di aggiornamento della conoscenza, in cui le distribuzioni di probabilità posteriori riflettono quanto siamo sicuri di una determinata ipotesi, alla luce dei dati e delle nostre conoscenze pregresse.\nA differenza dell’approccio NHST, che produce una decisione dicotomica, l’inferenza bayesiana restituisce un’intera distribuzione di credibilità sull’effetto. Questo consente di rispondere a domande più naturali e utili per la pratica scientifica, come ad esempio: “quanto è probabile che l’effetto superi una soglia di rilevanza pratica?”, oppure: “quanto si restringe la mia incertezza sull’effetto rispetto alla conoscenza pregressa?”",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>La crisi di replicazione e la riforma metodologica in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/01_data_analysis.html#il-problema-dei-piccoli-campioni-e-leterogeneità",
    "href": "chapters/key_notions/01_data_analysis.html#il-problema-dei-piccoli-campioni-e-leterogeneità",
    "title": "2  La crisi di replicazione e la riforma metodologica in psicologia",
    "section": "2.2 Il problema dei piccoli campioni e l’eterogeneità",
    "text": "2.2 Il problema dei piccoli campioni e l’eterogeneità\nUno dei limiti strutturali della ricerca psicologica riguarda la frequente presenza di campioni piccoli e popolazioni eterogenee. A causa della natura dei fenomeni studiati (ad esempio, patologie rare o condizioni sperimentali complesse), molti studi operano in condizioni di informazione limitata e con forte variabilità interindividuale. Questo porta a stime instabili, a bassa potenza statistica, e a risultati difficilmente replicabili.\nL’approccio bayesiano è particolarmente adatto a questi contesti. Permette di:\n\nintegrare conoscenze pregresse (priors) per aumentare la stabilità delle stime;\nmodellare esplicitamente l’incertezza e l’eterogeneità tra soggetti o studi;\nvalutare la robustezza dei risultati rispetto a diverse ipotesi a priori.\n\nIn altre parole, la statistica bayesiana rende possibile un’inferenza più solida in condizioni dove i metodi frequentisti si rivelano fragili, soprattutto nei casi in cui la variabilità è alta e i dati sono scarsi.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>La crisi di replicazione e la riforma metodologica in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/01_data_analysis.html#verso-una-scienza-cumulativa-e-trasparente",
    "href": "chapters/key_notions/01_data_analysis.html#verso-una-scienza-cumulativa-e-trasparente",
    "title": "2  La crisi di replicazione e la riforma metodologica in psicologia",
    "section": "2.3 Verso una scienza cumulativa e trasparente",
    "text": "2.3 Verso una scienza cumulativa e trasparente\nLa crisi di replicazione ha accelerato la transizione verso pratiche di ricerca più aperte, riproducibili e cumulative. In questo nuovo paradigma, l’approccio bayesiano si integra perfettamente con la Data Science e gli strumenti di Open Science: version control con GitHub, documentazione con Quarto, condivisione di dati e codice, preregistrazione delle ipotesi e confronto tra modelli. Questi strumenti, sempre più adottati, non sono semplici tecnicalità, ma elementi strutturali di un nuovo modello di scienza psicologica.\nInoltre, si sta assistendo a un rinnovato interesse per la modellazione formale, in cui le ipotesi teoriche vengono esplicitate attraverso modelli matematici interpretabili. La statistica bayesiana è il linguaggio naturale di questi modelli, poiché permette di confrontare teorie alternative, incorporare incertezza parametrica, e testare la coerenza predittiva in modo diretto.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>La crisi di replicazione e la riforma metodologica in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/01_data_analysis.html#riflessioni-conclusive",
    "href": "chapters/key_notions/01_data_analysis.html#riflessioni-conclusive",
    "title": "2  La crisi di replicazione e la riforma metodologica in psicologia",
    "section": "Riflessioni conclusive",
    "text": "Riflessioni conclusive\nLa crisi di replicazione ha messo a nudo i limiti di un certo modo di fare scienza: eccessiva fiducia nei risultati significativi, scarsa attenzione all’incertezza, e una concezione rigida dell’inferenza. Il paradigma bayesiano, affiancato da pratiche di ricerca aperta e strumenti di data science, offre un’alternativa concreta e operativa. Questo libro si propone come guida introduttiva a questo approccio, con l’obiettivo di formare ricercatori capaci di pensare in termini di variabilità, incertezza e aggiornamento continuo della conoscenza.\n\n\n\n\n\n\nProblemi\n\n\n\n\n\n\nQuali sono i principali fattori che hanno portato alla “Credibility Revolution” in psicologia e in che modo le nuove metodologie — in particolare l’approccio bayesiano e le buone pratiche di Data Science — mirano a superare i limiti che hanno contribuito alla “Replication Crisis”?\nIn che modo il paradigma bayesiano differisce dall’approccio frequentista nella gestione dell’incertezza e nell’aggiornamento della conoscenza, e quali vantaggi pratici offre quando si lavora con campioni di piccole dimensioni e popolazioni eterogenee?\nQual è il ruolo delle distribuzioni a priori nelle analisi bayesiane e come ci si assicura che l’uso di priors non introduca bias indesiderati, specialmente in un contesto come quello psicologico dove le teorie e i dati pregressi possono essere incompleti o controversi?\nPerché la modellazione formale e le buone pratiche di Data Science (ad es. condivisione di codice, controllo di versione, pipeline riproducibili) risultano fondamentali per una scienza cumulativa e per mitigare gli errori sistematici nella ricerca psicologica?\n\n\n\n\n\n\n\n\n\n\nSoluzioni\n\n\n\n\n\n1. Quali sono i principali fattori che hanno portato alla “Credibility Revolution” in psicologia e in che modo le nuove metodologie — in particolare l’approccio bayesiano e le buone pratiche di Data Science — mirano a superare i limiti che hanno contribuito alla “Replication Crisis”?\nNegli ultimi decenni, la psicologia ha attraversato una “Replication Crisis” a causa di diverse pratiche di ricerca problematiche, tra cui l’utilizzo di campioni di piccole dimensioni, l’uso eccessivo di test di significatività frequentisti con p &lt; .05 come soglia rigida, il fenomeno del p-hacking (cioè l’adattamento delle analisi per ottenere risultati “significativi”), e la carenza di trasparenza e condivisione dei dati. Questi fattori hanno portato alla pubblicazione di molti falsi positivi e a un’erosione della fiducia nelle conclusioni psicologiche.\nLa “Credibility Revolution” nasce dalla presa di coscienza di questi problemi e dall’introduzione di nuove metodologie che offrono maggior rigore e trasparenza. L’approccio bayesiano, in particolare, consente di superare alcuni limiti della statistica frequentista, poiché fornisce distribuzioni posteriori di plausibilità per i parametri e non si affida a soglie arbitrarie di significatività. Inoltre, la Data Science ha promosso la diffusione di pipeline analitiche riproducibili (tramite il controllo di versione, la condivisione del codice e dei dati), contribuendo a ridurre errori, bias e a favorire la replicabilità. Insieme, queste innovazioni mirano a creare una scienza più aperta, solida e cumulativa.\n2. In che modo il paradigma bayesiano differisce dall’approccio frequentista nella gestione dell’incertezza e nell’aggiornamento della conoscenza, e quali vantaggi pratici offre quando si lavora con campioni di piccole dimensioni e popolazioni eterogenee?\nIl paradigma frequentista si basa sull’idea di ripetizione ipotetica degli esperimenti e sull’applicazione di test di significatività, focalizzandosi su p-value e intervalli di confidenza che rispondono a domande “se si ripetesse infinite volte l’esperimento, in media cosa accadrebbe?”. L’inferenza bayesiana, al contrario, concepisce la probabilità come uno stato di conoscenza (o di credenza) e integra le informazioni precedenti (priors) con i dati osservati per produrre una distribuzione posteriore. Ciò consente un aggiornamento continuo e iterativo delle ipotesi alla luce delle nuove evidenze.\nQuesta impostazione risulta particolarmente utile quando i campioni sono ridotti e le popolazioni indagate sono eterogenee. In tali condizioni, il paradigma frequentista rischia di generare stime instabili o intervalli di confidenza molto ampi. L’approccio bayesiano, invece, permette di incorporare informazioni pregresse e di ottenere stime più precise, a patto che le priors siano giustificate e non eccessivamente informative. L’attenzione alla distribuzione posteriore rende inoltre più chiaro il grado di incertezza associato ai parametri di interesse e favorisce la formulazione di inferenze più calibrate.\n3. Qual è il ruolo delle distribuzioni a priori nelle analisi bayesiane e come ci si assicura che l’uso di priors non introduca bias indesiderati, specialmente in un contesto come quello psicologico dove le teorie e i dati pregressi possono essere incompleti o controversi?\nNell’approccio bayesiano, le distribuzioni a priori (priors) rappresentano la conoscenza o le ipotesi iniziali di cui si dispone sui parametri in esame prima di raccogliere i dati. Se ben specificate, contribuiscono a rendere l’analisi più informativa, soprattutto quando il campione è di piccole dimensioni. Tuttavia, un uso improprio delle priors può introdurre bias, poiché priors troppo “forti” (ossia eccessivamente vincolanti) possono spingere i risultati verso determinate conclusioni.\nPer evitare questi rischi, i ricercatori devono adottare priors ben motivate e trasparenti. In psicologia, dove le teorie possono essere ancora in fase di sviluppo e i dati pregressi non sempre affidabili, è spesso utile iniziare con priors non informative o debolmente informative, per ridurre il rischio di forzare troppo il modello. È anche fondamentale effettuare analisi di sensibilità: testare differenti specificazioni di priors per verificare se i risultati sono robusti oppure fortemente dipendenti da particolari assunzioni iniziali. La documentazione delle scelte fatte (e le relative ragioni) è parte integrante di una buona pratica di ricerca trasparente.\n4. Perché la modellazione formale e le buone pratiche di Data Science (ad es. condivisione di codice, controllo di versione, pipeline riproducibili) risultano fondamentali per una scienza cumulativa e per mitigare gli errori sistematici nella ricerca psicologica?\nLa modellazione formale consente di superare la mera descrizione delle relazioni tra variabili (tipica dell’ANOVA o dei modelli lineari tradizionali) e di entrare nel merito dei meccanismi sottostanti i fenomeni psicologici, costruendo teorie più articolate e fondate. Questa prospettiva rende più esplicite le assunzioni e le ipotesi su cui si basa la ricerca, permettendo un confronto chiaro tra diverse spiegazioni concorrenti.\nParallelamente, l’adozione di strumenti e pratiche di Data Science come la condivisione di codice (in repository pubblici), l’uso del controllo di versione (es. Git) e pipeline analitiche riproducibili riducono gli errori e favoriscono la verifica indipendente dei risultati. Ciò aumenta la trasparenza, poiché altri ricercatori possono ispezionare i passaggi compiuti, e consente la replicazione degli studi. In una scienza cumulativa, infatti, la possibilità di riprodurre, criticare e migliorare i risultati di lavori precedenti è essenziale per costruire un corpus di conoscenze solido e affidabile.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>La crisi di replicazione e la riforma metodologica in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/01_data_analysis.html#bibliografia",
    "href": "chapters/key_notions/01_data_analysis.html#bibliografia",
    "title": "2  La crisi di replicazione e la riforma metodologica in psicologia",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nAngrist, J. D., & Pischke, J.-S. (2010). The credibility revolution in empirical economics: How better research design is taking the con out of econometrics. Journal of economic perspectives, 24(2), 3–30.\n\n\nBaker, M. (2016). 1,500 scientists lift the lid on reproducibility. Nature, 533(7604).\n\n\nBishop, D. (2019). The psychology of experimental psychologists: Overcoming cognitive constraints to improve research.\n\n\nCollaboration, O. S. (2015). Estimating the reproducibility of psychological science. Science, 349(6251), aac4716.\n\n\nIoannidis, J. P. (2005). Why most published research findings are false. PLoS medicine, 2(8), e124.\n\n\nLabatut, B. (2021). Quando abbiamo smesso di capire il mondo. Adelphi Edizioni spa.\n\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2nd Edition). CRC Press.\n\n\nMeehl, P. E. (1967). Theory-testing in psychology and physics: A methodological paradox. Philosophy of science, 34(2), 103–115.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>La crisi di replicazione e la riforma metodologica in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/02_key_notions.html",
    "href": "chapters/key_notions/02_key_notions.html",
    "title": "3  Concetti chiave",
    "section": "",
    "text": "Introduzione\nNella ricerca scientifica, la formulazione di risposte a specifiche domande di indagine avviene attraverso l’applicazione di metodologie rigorose e l’esecuzione di osservazioni accurate e controllate. Le informazioni raccolte mediante diverse tecniche di indagine—come ricerche sul campo, indagini campionarie e protocolli sperimentali—vengono definite con il termine tecnico di dati. Questo capitolo introduce i principi fondamentali dell’analisi dei dati, concentrandosi sia sulle caratteristiche dei dati stessi sia sui metodi di raccolta.\nL’analisi dei dati permette di sintetizzare grandi quantità di informazioni e di verificare le previsioni avanzate dalle teorie. Tuttavia, senza una teoria che dia significato ai dati, le osservazioni rimangono mere descrizioni prive di un contesto esplicativo. È attraverso l’integrazione tra dati e teoria che si raggiunge una comprensione profonda dei fenomeni e si favorisce l’avanzamento scientifico [es., Wertheimer (1880–1943), scoperta del movimento-\\(\\phi\\) e nascita del movimento della Gestalt; Steinman et al. (2000)].",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/02_key_notions.html#introduzione",
    "href": "chapters/key_notions/02_key_notions.html#introduzione",
    "title": "3  Concetti chiave",
    "section": "",
    "text": "Statistica\n\n\n\n\n\nIl termine “statistica” può assumere diversi significati a seconda del contesto:\n\nPrimo significato: La statistica è una scienza che si occupa dello studio e dell’applicazione di metodi per la raccolta, organizzazione, analisi, interpretazione e presentazione dei dati.\nSecondo significato: Il termine si riferisce a una misura o valore numerico calcolato a partire da un campione di dati, come la media campionaria, la deviazione standard campionaria o il coefficiente di correlazione campionario.\n\n\n\n\n\n\nPanoramica del capitolo\n\nDefinizione di popolazione e campione.\nDistinzione tra variabili indipendenti e dipendenti.\nLa struttura e l’importanza della matrice dei dati.\nL’effetto delle variabili all’interno delle analisi statistiche.\nI concetti fondamentali di stima e inferenza.\nIl significato e l’applicazione dei modelli psicologici.\n\n\n\n\n\n\n\nPrerequisiti\n\n\n\n\n\n\nLeggere Horoscopes. L’ultimo capitolo di McElreath (2020) discute il contesto scientifico e culturale della statistica.\nLeggere The Effect: An Introduction to Research Design and Causality. Focalizzati sul capitolo 10 Treatment Effects.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/02_key_notions.html#la-spiegazione-scientifica",
    "href": "chapters/key_notions/02_key_notions.html#la-spiegazione-scientifica",
    "title": "3  Concetti chiave",
    "section": "3.1 La spiegazione scientifica",
    "text": "3.1 La spiegazione scientifica\nLa scienza non si limita a descrivere o prevedere i fenomeni: il suo obiettivo principale è spiegare il perché degli eventi, offrendo una comprensione approfondita delle cause e dei meccanismi che li regolano. La spiegazione scientifica è cruciale per costruire teorie capaci non solo di descrivere e prevedere, ma anche di chiarire le dinamiche causali e le connessioni tra i fenomeni, contribuendo a un controllo più consapevole e informato su di essi.\nConsideriamo, ad esempio, il rapporto tra il background familiare e il rendimento scolastico. Numerose ricerche evidenziano una forte correlazione tra il livello di istruzione dei genitori e il successo accademico dei figli. Una prospettiva puramente descrittiva potrebbe limitarsi a constatare che: “Gli studenti provenienti da famiglie con basso livello di istruzione hanno minori probabilità di conseguire un titolo universitario”. Tuttavia, la vera sfida scientifica consiste nell’andare oltre questa previsione, ponendosi domande più profonde:\n\nquali meccanismi causali determinano questa disparità?\n\nquali interventi possono efficacemente ridurre tali disuguaglianze?\n\nPer superare il livello di semplice previsione, la ricerca deve identificare i fattori causali alla base del fenomeno, in modo da comprendere come l’azione su questi fattori possa modificare gli esiti. Nel caso dell’esempio sul rapporto tra background familiare e rendimento scolastico, ciò implica comprendere, ad esempio:\n\nse e in che modo il sostegno finanziario possa favorire il percorso degli studenti svantaggiati;\nquali politiche educative possano produrre effetti positivi sul lungo termine;\ncome i meccanismi sociali e individuali influenzino il processo educativo.\n\nAcquisire una conoscenza approfondita dei meccanismi causali permette di andare oltre la semplice previsione, rendendo possibile la progettazione di interventi mirati e strategici che possano realmente incidere sui fenomeni in modo efficace e duraturo.\n\n3.1.1 Elementi fondamentali della spiegazione scientifica\nLa filosofia della scienza identifica tre componenti fondamentali di una spiegazione scientifica:\n\nExplanandum. È il fenomeno che desideriamo comprendere, ovvero ciò di cui cerchiamo le cause o i meccanismi. Un esempio: “Gli studenti con alti livelli di ansia da prestazione ottengono punteggi più bassi nei test scolastici rispetto ai loro pari.”\nExplanans. È l’insieme dei fattori che spiegano il fenomeno. Nel caso dell’ansia da prestazione, un possibile explanans potrebbe essere: “L’ansia danneggia la concentrazione e la memoria di lavoro, influendo negativamente sulla performance nei test.”\nLegame esplicativo. Comprende i principi o i meccanismi che dimostrano come l’explanans produca l’explanandum. Seguendo l’esempio precedente: “Livelli elevati di ansia innescano il sistema nervoso simpatico, aumentando lo stress fisiologico e riducendo l’efficienza dei processi cognitivi necessari per compiti complessi.”\n\nQuesti tre elementi si combinano all’interno di modelli scientifici, che costituiscono le strutture teoriche e metodologiche impiegate per formulare e verificare spiegazioni. In psicologia, tali modelli includono:\n\nil fenomeno da spiegare (ad es. le prestazioni scolastiche);\ni fattori che lo influenzano (ad es. ansia, regolazione emotiva);\ni meccanismi sottostanti che collegano cause ed effetti (ad es. attivazione fisiologica, memoria di lavoro compromessa).\n\nUn modello psicologico sull’ansia da prestazione, ad esempio, potrebbe considerare la relazione tra livello di ansia percepita, capacità di regolazione emotiva e memoria di lavoro. A differenza di modelli esclusivamente descrittivi o predittivi, i modelli esplicativi rispondono a domande causali: non si limitano ad attestare che ansia e prestazioni sono correlate, ma mostrano come e perché l’ansia riduca il rendimento. Inoltre, tali modelli suggeriscono strategie d’intervento per attenuare l’effetto dell’ansia, come il potenziamento della regolazione emotiva o l’uso di tecniche di gestione dello stress.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/02_key_notions.html#modelli-psicologici",
    "href": "chapters/key_notions/02_key_notions.html#modelli-psicologici",
    "title": "3  Concetti chiave",
    "section": "3.2 Modelli psicologici",
    "text": "3.2 Modelli psicologici\nUn modello è una rappresentazione concettuale – spesso supportata da formalismi matematici – di un fenomeno reale, basata su un insieme di equazioni e ipotesi che descrivono le relazioni tra variabili e la struttura probabilistica sottostante. L’obiettivo è coglierne gli aspetti essenziali senza includere ogni dettaglio superfluo, formulando predizioni quantitative che possano essere verificate empiricamente. Poiché spesso esistono molteplici modelli in grado di spiegare lo stesso fenomeno, il compito della ricerca consiste nel selezionare quello che meglio descrive i dati e soddisfa criteri di validità, accuratezza e parsimonia.\nI modelli psicologici, in particolare, sono strumenti teorici finalizzati a descrivere, spiegare e prevedere il comportamento umano e i processi mentali. Un modello ben costruito dovrebbe possedere le seguenti caratteristiche:\n\nCoerenza descrittiva. Il modello deve fornire una rappresentazione logica e coerente del fenomeno, includendo tutti gli elementi chiave del processo psicologico e organizzando le osservazioni in una struttura interpretativa chiara.\nCapacità predittiva. Deve essere in grado di formulare previsioni verificabili e di produrre ipotesi testabili sulla base dei dati raccolti, permettendo così di valutare la validità del modello.\nSupporto empirico. Le previsioni e le ipotesi del modello vanno confrontate con l’evidenza empirica, ottenuta attraverso ricerche sistematiche e rigorose. I dati devono corroborare le relazioni proposte dal modello.\nFalsificabilità. Il modello deve poter essere sottoposto a verifica empirica e, all’occorrenza, smentito. Se emergono osservazioni in conflitto con le sue previsioni, il modello deve essere revisionato o sostituito.\nParsimonia. La spiegazione deve risultare semplice e lineare, includendo solo gli elementi indispensabili per rendere conto del fenomeno. Assunzioni superflue o ridondanti ne riducono la robustezza.\nGeneralizzabilità. Il modello dovrebbe poter essere esteso a contesti e situazioni diverse, superando i limiti di specifiche condizioni sperimentali o campioni ristretti.\nUtilità pratica. Dovrebbe offrire linee guida concrete per l’applicazione nel mondo reale, ad esempio negli interventi clinici, nei programmi di prevenzione o nelle terapie, così da avere un impatto positivo sugli individui e sulla società.\n\nUno degli ostacoli maggiori nella costruzione di modelli in psicologia è la natura soggettiva, dinamica e variabile dell’esperienza umana. È quindi necessario bilanciare la precisione teorica (spesso supportata da formalizzazioni matematiche o computazionali) con la flessibilità necessaria a catturare l’eterogeneità dei fenomeni psicologici. A questo si aggiungono i vincoli etici della ricerca sull’essere umano e le potenziali ricadute sociali dei risultati.\nL’analisi quantitativa dei dati gioca un ruolo centrale nella validazione dei modelli psicologici: mediante metodologie statistiche e computazionali avanzate, i ricercatori possono verificare se le predizioni di un modello trovano riscontro nei dati empirici e se tali predizioni si mantengono valide in contesti diversi. Questo processo non solo consolida la comprensione del fenomeno, ma consente anche di anticipare e, in alcune circostanze, influenzare il comportamento e i processi mentali. Un modello rigorosamente formulato e testabile diviene quindi un potente strumento per lo sviluppo di interventi efficaci e il progresso teorico.\n\n3.2.1 Rappresentare i fenomeni per ragionare e comunicare\nLa spiegazione scientifica non si limita a far luce sui meccanismi causali, ma fornisce anche un linguaggio formale per analizzare e condividere conoscenze sui fenomeni. In psicologia, i modelli scientifici costituiscono strumenti fondamentali per descrivere i processi attraverso variabili, funzioni e parametri, offrendo una struttura che facilita l’individuazione di relazioni e proprietà essenziali. Un modello efficace semplifica la complessità del fenomeno, agevolando sia la comunicazione tra studiosi sia la comprensione intuitiva.\nInoltre, i modelli non si limitano a organizzare le informazioni esistenti: stimolano anche l’emergere di nuove ipotesi di ricerca, promuovono collegamenti tra concetti apparentemente lontani e consentono di trasferire conoscenze tra discipline, ampliando così l’orizzonte dell’indagine scientifica.\n\n\n3.2.2 Il ruolo dell’analisi dei dati\nL’analisi dei dati è parte integrante del metodo scientifico e, in psicologia, assolve due funzioni primarie:\n\nSemplificare e sintetizzare informazioni complesse\nAttraverso statistiche descrittive, rappresentazioni grafiche e altre tecniche di sintesi, l’analisi dei dati aiuta a individuare schemi, tendenze e anomalie. Questo passaggio è essenziale per comprendere le differenze tra individui o gruppi e per formulare ipotesi di ricerca più mirate.\nValutare le predizioni dei modelli\nConfrontando i dati raccolti con le previsioni teoriche, si misura la validità di un modello. Tale confronto è indispensabile per confermare, raffinare o rivedere le ipotesi di partenza, orientando così il progresso della conoscenza scientifica.\n\nTuttavia, limitarsi alla ricerca di correlazioni o di pattern nei dati, senza un solido quadro teorico, non basta a comprendere pienamente il fenomeno. Risultati empirici privi di spiegazioni causali rimangono frammentari. Per questo motivo, integrare i dati in un modello teorico esplicativo è cruciale: si possono così proporre meccanismi causali, identificare relazioni e avanzare nuove ipotesi di ricerca.\n\n\n3.2.3 Carattere multidisciplinare dell’analisi dei dati\nPer rispondere alle complesse domande poste in psicologia, l’analisi dei dati si fonda sull’integrazione di più discipline: statistica, teoria della probabilità e informatica. Ciascuna offre contributi indispensabili per affrontare la complessità dei processi psicologici:\n\nStatistica\nFornisce tecniche per la raccolta, l’organizzazione e l’interpretazione dei dati, consentendo di riassumere le informazioni, individuare pattern significativi e valutare empiricamente le ipotesi dei modelli psicologici.\nTeoria della probabilità\nCostituisce la base matematica della statistica e della modellazione scientifica, offrendo strumenti per quantificare l’incertezza, descrivere la variabilità delle osservazioni e costruire modelli predittivi rigorosi.\nInformatica\nContribuisce con strumenti per la gestione, l’analisi e la visualizzazione di grandi quantità di dati, nonché per l’implementazione di modelli computazionali sofisticati. Questi modelli si rivelano fondamentali nel simulare e testare dinamiche dei processi psicologici.\n\nLa natura multidisciplinare dell’analisi dei dati rispecchia l’esigenza di competenze diverse per comprendere e modellizzare i fenomeni psicologici in modo rigoroso. L’approccio quantitativo e computazionale ai modelli non si limita a descrivere e interpretare i dati, ma consente di formulare predizioni precise e sottoponibili a verifica, contribuendo così all’avanzamento della psicologia come scienza.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/02_key_notions.html#concetti-chiave-nellanalisi-dei-dati",
    "href": "chapters/key_notions/02_key_notions.html#concetti-chiave-nellanalisi-dei-dati",
    "title": "3  Concetti chiave",
    "section": "3.3 Concetti chiave nell’analisi dei dati",
    "text": "3.3 Concetti chiave nell’analisi dei dati\nPer condurre un’analisi dei dati efficace, è fondamentale comprendere alcuni concetti chiave che guidano il processo di indagine, dall’identificazione del fenomeno alla formulazione di inferenze.\n\n3.3.1 Popolazioni e campioni\nL’analisi dei dati inizia con l’identificazione della popolazione di interesse, che rappresenta l’insieme completo degli individui o delle entità coinvolte nel fenomeno studiato. Poiché studiare un’intera popolazione è spesso impraticabile, si ricorre ai campioni, sottoinsiemi rappresentativi della popolazione. La qualità e la rappresentatività del campione sono cruciali: un campione non rappresentativo può portare a conclusioni errate, limitando la generalizzabilità dei risultati.\n\n\n\n\n\n\nParametri e statistiche\n\n\n\n\n\nUn parametro è una caratteristica numerica della popolazione (es. media \\(\\mu\\), deviazione standard \\(\\sigma\\)). Una statistica è una caratteristica numerica calcolata sul campione (es. media campionaria \\(\\bar{x}\\), deviazione standard campionaria \\(s\\)). L’inferenza statistica si occupa di stimare i parametri della popolazione a partire dalle statistiche campionarie.\n\n\n\n\n\n3.3.2 Bias nella raccolta dei dati\nI bias nella raccolta e interpretazione dei dati possono compromettere l’accuratezza dei risultati. Comprendere chi ha raccolto i dati, come e con quali scopi è essenziale per una corretta interpretazione (Johnson et al., 2022). I dati non sono mai completamente neutri; i metodi e gli obiettivi di raccolta influenzano i risultati. Ad esempio, selezionare partecipanti da una popolazione di studenti universitari potrebbe introdurre un bias sistematico, limitando la generalizzabilità ad altri contesti (Murray & Carr, 2024; Nobles, 2000).\n\n\n3.3.3 Variabili e costanti\nNell’analisi statistica, le variabili rappresentano le caratteristiche osservate che possono assumere diversi valori (numerici o categorici). Le costanti, al contrario, rimangono fisse in un determinato contesto. Le variabili si distinguono in:\n\nvariabili indipendenti (o predittive): influenzano altri fenomeni;\n\nvariabili dipendenti: rappresentano gli esiti di interesse influenzati dalle variabili indipendenti.\n\nAd esempio, in uno studio sugli effetti della terapia cognitivo-comportamentale, la variabile indipendente potrebbe essere la partecipazione alla terapia, mentre la variabile dipendente sarebbe la riduzione dei sintomi di ansia.\n\n\n3.3.4 Studi osservazionali ed esperimenti\nEsistono due principali metodi di raccolta dati.\n\nEsperimenti: I ricercatori manipolano una o più variabili per valutare il loro effetto su altre variabili, controllando per i fattori confondenti. Ad esempio, per valutare l’efficacia di un trattamento, i partecipanti possono essere assegnati casualmente a un gruppo di controllo (placebo) e a un gruppo sperimentale (trattamento attivo). La randomizzazione riduce il rischio di bias sistematici.\nStudi osservazionali: I dati vengono raccolti senza interferire con il fenomeno osservato. Ad esempio, un’indagine su come lo stress influenza la produttività lavorativa potrebbe basarsi su questionari senza manipolare lo stress dei partecipanti. Questi studi forniscono correlazioni tra variabili, ma non dimostrano relazioni causali.\n\n\n\n3.3.5 Effetti\nIn statistica, un effetto rappresenta il cambiamento osservato nella variabile dipendente in relazione a una variabile indipendente. Questo cambiamento può indicare un’associazione tra le due variabili, ma la sua interpretazione come relazione causale dipende strettamente dal disegno sperimentale con cui i dati sono stati raccolti.\nAd esempio, se si osserva una riduzione dei sintomi tra la fase pre-trattamento e quella post-trattamento in un gruppo di pazienti sottoposti a una terapia, è possibile identificare un effetto della terapia. Tuttavia, senza un disegno sperimentale adeguato – come un esperimento controllato randomizzato (RCT) – non è possibile stabilire con certezza che la riduzione dei sintomi sia causata dalla terapia e non da altri fattori, come il decorso naturale della malattia o l’effetto placebo (Huntington-Klein, 2021).\nI modelli statistici, da soli, non possono determinare relazioni causali: possono quantificare l’entità di un effetto e valutare la forza dell’associazione tra variabili, ma la causalità può essere inferita solo se i dati provengono da un disegno sperimentale che isola il meccanismo di interesse, controllando per possibili fattori di confondimento. Pertanto, per trarre conclusioni causali robuste, è essenziale integrare l’analisi statistica con un approccio metodologico rigoroso basato su strategie di manipolazione sperimentale, assegnazione casuale o tecniche avanzate per il controllo dei bias nei dati osservazionali.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/02_key_notions.html#stima-e-inferenza-statistica-dal-campione-alla-popolazione",
    "href": "chapters/key_notions/02_key_notions.html#stima-e-inferenza-statistica-dal-campione-alla-popolazione",
    "title": "3  Concetti chiave",
    "section": "3.4 Stima e inferenza statistica: dal campione alla popolazione",
    "text": "3.4 Stima e inferenza statistica: dal campione alla popolazione\nLa stima e l’inferenza statistica costituiscono i pilastri della metodologia quantitativa, poiché permettono di estendere le conclusioni tratte da un campione – una porzione limitata di individui osservati – all’intera popolazione di interesse. L’uso dei campioni risulta indispensabile a causa dei vincoli di tempo, costi e risorse, che spesso rendono impossibile lo studio dell’intera popolazione.\nTuttavia, il ricorso al campione introduce inevitabilmente un’incertezza intrinseca: le statistiche campionarie (come media o varianza del campione) sono stime dei parametri della popolazione e di norma non coincidono esattamente con i valori “veri” della popolazione. Tale discrepanza è nota come errore di campionamento, la cui entità dipende, tra l’altro, dalla dimensione del campione e dalla strategia di campionamento adottata.\nLa teoria degli stimatori e gli strumenti di inferenza statistica (ad esempio, intervalli di confidenza in un approccio frequentista o intervalli di credibilità in un approccio bayesiano) consentono di quantificare e gestire quest’incertezza, fornendo un quadro che permette di trarre conclusioni credibili sulla popolazione partendo dai dati raccolti.\n\n3.4.1 Stima: inferire le caratteristiche della popolazione\nLa stima è il processo con cui, a partire dai dati di un campione, si inferiscono proprietà della popolazione, come la media o la varianza. Poiché ogni campione rappresenta solo una frazione della popolazione, può fornire stime diverse; questo fenomeno è noto come variabilità campionaria. Proprio tale variabilità costituisce la principale fonte di incertezza nelle inferenze: se un singolo campione non è sufficientemente ampio o rappresentativo, la stima potrebbe discostarsi in misura rilevante dai valori effettivi presenti nella popolazione.\n\n3.4.1.1 Fattori che influenzano l’accuratezza\nTre fattori fondamentali influiscono sull’accuratezza di una stima:\n\nDimensione del campione. Un campione più grande tende a ridurre la variabilità campionaria, aumentando la precisione delle stime.\nRappresentatività. Un campione ben progettato e rappresentativo rispecchia le caratteristiche essenziali della popolazione. Al contrario, un campione distorto (ad esempio, selezionato per convenienza) può condurre a stime fuorvianti.\nVariabilità della popolazione. Se la popolazione è estremamente eterogenea, sono necessari campioni più ampi per produrre stime affidabili.\n\n\n\n3.4.1.2 Gli stimatori: proprietà fondamentali\nGli stimatori sono formule matematiche o procedure statistiche usate per calcolare le stime. La loro qualità si valuta principalmente in base a:\n\nConsistenza. Uno stimatore è consistente se, all’aumentare della dimensione del campione, la stima tende a convergere verso il valore reale del parametro.\nNon distorsione (unbiasedness). Uno stimatore è non distorto se il suo valore atteso corrisponde al parametro della popolazione. In altri termini, in media, lo stimatore coincide con il valore reale.\nEfficienza. Tra stimatori non distorti, è più efficiente quello con varianza minore, poiché fornisce stime più stabili.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/02_key_notions.html#inferenza-statistica",
    "href": "chapters/key_notions/02_key_notions.html#inferenza-statistica",
    "title": "3  Concetti chiave",
    "section": "3.5 Inferenza statistica",
    "text": "3.5 Inferenza statistica\nL’inferenza statistica si basa sulle stime campionarie per trarre conclusioni sull’intera popolazione. In particolare, risponde a tre grandi interrogativi:\n\nStima dei parametri della popolazione. Ottenere valori plausibili per parametri quali media, varianza e proporzioni, quantificando contestualmente l’incertezza (ad esempio, costruendo intervalli di confidenza o di credibilità).\nValutazione di ipotesi. Confrontare ipotesi rivali, come l’esistenza di differenze tra gruppi o di relazioni tra variabili. Attraverso il confronto tra ipotesi e dati, si determina quale ipotesi è meglio supportata.\nPrevisione. Utilizzare i dati esistenti per anticipare risultati futuri, tenendo conto delle fonti di incertezza legate sia alla variabilità intrinseca dei dati sia ai parametri non perfettamente noti.\n\nSia l’approccio frequentista sia quello bayesiano affrontano questi problemi in modo rigoroso, ma differiscono nel modo di concettualizzare l’incertezza e di incorporare l’informazione nei modelli.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/02_key_notions.html#le-sfide-dellinferenza-statistica-in-psicologia",
    "href": "chapters/key_notions/02_key_notions.html#le-sfide-dellinferenza-statistica-in-psicologia",
    "title": "3  Concetti chiave",
    "section": "3.6 Le sfide dell’inferenza statistica in psicologia",
    "text": "3.6 Le sfide dell’inferenza statistica in psicologia\nIn psicologia e, più in generale, nelle scienze sociali, l’inferenza statistica incontra specifiche problematiche spesso connesse alla complessità dei fenomeni oggetto di studio (Gelman et al., 2021). Tra le sfide principali figurano:\n\nLimiti nella generalizzazione dei risultati. In molti studi psicologici, le condizioni sperimentali create in laboratorio o in ambienti altamente controllati non sempre rispecchiano le dinamiche reali in cui i fenomeni si manifestano. L’uso di procedure standardizzate e compiti artificiali può semplificare notevolmente le variabili in gioco, a scapito della validità esterna: i risultati ottenuti potrebbero non essere direttamente trasferibili a contesti naturali o situazioni di vita quotidiana. Inoltre, se i partecipanti vengono selezionati per ragioni pratiche (ad esempio, studenti universitari reclutati su base volontaria), ciò limita ulteriormente la rappresentatività del campione, rendendo più difficile estendere le conclusioni a gruppi più eterogenei o a popolazioni diverse.\nRischio di semplificare eccessivamente i meccanismi causali ipotizzati. L’inferenza causale – implicita o esplicita nella maggior parte delle ricerche in psicologia – mira a comprendere se e come un fattore influisca su un altro. Tuttavia, in contesti così complessi, i modelli causali proposti possono risultare eccessivamente semplificati, trascurando interazioni tra variabili, fattori contestuali o processi multilivello. Quando tali aspetti non vengono adeguatamente considerati, le conclusioni possono rivelarsi poco utili o non sufficientemente applicabili ai contesti reali.\nDistorsioni legate alla misurazione. Molti costrutti di interesse psicologico (es. ansia, autostima, intelligenza) non sono direttamente osservabili, bensì misurati attraverso questionari, test o altre metodologie indirette. Tale approccio introduce possibili errori di misurazione e distorsioni legate allo strumento di valutazione. L’inferenza statistica deve quindi tenere conto di questa complessità, collegando in modo rigoroso le osservazioni empiriche ai costrutti teorici sottostanti.\n\nIn sintesi, la stima e l’inferenza statistica rappresentano strumenti fondamentali per trasformare i dati campionari in conoscenza generalizzabile, soprattutto in un contesto come quello psicologico, caratterizzato da un’elevata variabilità nei comportamenti e nei processi mentali. Da un lato, la metodologia quantitativa offre un quadro consolidato per gestire l’incertezza e testare ipotesi; dall’altro, è cruciale prestare attenzione alla qualità del campione, alla validità degli strumenti di misura e all’intrinseca complessità dei costrutti indagati.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/02_key_notions.html#la-quantificazione-dellincertezza",
    "href": "chapters/key_notions/02_key_notions.html#la-quantificazione-dellincertezza",
    "title": "3  Concetti chiave",
    "section": "3.7 La quantificazione dell’incertezza",
    "text": "3.7 La quantificazione dell’incertezza\nLe considerazioni introduttive di questo capitolo mettono in evidenza come la gestione e la quantificazione dell’incertezza rappresentino un aspetto cruciale della stima e dell’inferenza statistica. Qualunque stima ottenuta da un campione è inevitabilmente soggetta a errore, poiché il campione costituisce soltanto una frazione della popolazione di riferimento. L’inferenza statistica offre gli strumenti necessari per quantificare tale incertezza, ad esempio tramite gli intervalli di confidenza (nell’approccio frequentista) o le distribuzioni a posteriori (nell’approccio bayesiano), consentendo di esprimere in modo rigoroso il grado di fiducia nelle conclusioni raggiunte.\nIn conclusione, la stima e l’inferenza statistica rappresentano strumenti essenziali per trasformare i dati empirici in conoscenza solida e applicabile. È tuttavia indispensabile avvalersene in maniera critica, tenendo sempre presenti le possibili distorsioni insite nel processo di raccolta e analisi dei dati. Ciò significa prestare particolare attenzione alla rappresentatività del campione, alla validità delle misurazioni e all’interpretazione corretta dei risultati, così da evitare generalizzazioni indebite o conclusioni fuorvianti.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/02_key_notions.html#riflessioni-conclusive",
    "href": "chapters/key_notions/02_key_notions.html#riflessioni-conclusive",
    "title": "3  Concetti chiave",
    "section": "Riflessioni conclusive",
    "text": "Riflessioni conclusive\nL’analisi dei dati acquisisce valore solo quando è integrata con una solida teoria scientifica, che fornisce il contesto e il quadro interpretativo necessario per attribuire senso ai risultati. Ad esempio, osservare che un trattamento psicologico riduce i sintomi è un’osservazione empirica che, senza una teoria che chiarisca i meccanismi sottostanti, rimane priva di potere esplicativo. È la teoria che orienta il processo analitico, formulando ipotesi verificabili e offrendo interpretazioni che si inseriscono in un modello più ampio.\nIn definitiva, la relazione tra teoria e analisi dei dati è intrinsecamente circolare e dinamica: le teorie guidano la raccolta, l’analisi e l’interpretazione dei dati, mentre i dati, a loro volta, stimolano il perfezionamento e l’evoluzione delle teorie. Questo dialogo continuo è ciò che permette un progresso costante nella comprensione dei fenomeni psicologici.\n\n\n\n\n\n\nProblemi\n\n\n\n\n\n\nChe cos’è una spiegazione scientifica e in che modo si differenzia da una mera descrizione o previsione di un fenomeno?\nPerché, quando si parla di popolazione e campione, è fondamentale assicurarsi che il campione sia rappresentativo, e quali conseguenze possono derivare da un campione non rappresentativo?\nChe differenza c’è tra un parametro e una statistica, e perché in inferenza statistica si cerca di stimare il parametro sconosciuto a partire dalla statistica campionaria?\nCosa si intende per bias nella raccolta e interpretazione dei dati, e in che modo la consapevolezza dei possibili bias può migliorare la qualità della ricerca?\nPerché in psicologia e nelle scienze sociali risulta essenziale integrare l’analisi dei dati con un quadro teorico solido e coerente?\nQual è la differenza principale tra uno studio osservazionale e un esperimento, e perché la distinzione è importante per comprendere la causalità?\nChe ruolo svolgono i modelli scientifici in psicologia, e quali caratteristiche fondamentali dovrebbero possedere per essere considerati validi e utili?\nIn che modo l’analisi dei dati aiuta a passare dalle semplici correlazioni o tendenze osservate all’elaborazione di ipotesi e spiegazioni più profonde?\nChe differenza c’è tra variabili indipendenti e variabili dipendenti, e perché questa distinzione è cruciale per disegnare uno studio e interpretarne i risultati?\nPerché parlare di incertezza è inevitabile quando si utilizzano i dati di un campione, e come la statistica (frequentista o bayesiana) ci aiuta a gestirla?\n\n\n\n\n\n\n\n\n\n\nSoluzioni\n\n\n\n\n\n1. Che cos’è una spiegazione scientifica e in che modo si differenzia da una mera descrizione o previsione di un fenomeno?\nUna spiegazione scientifica mira a individuare le cause e i meccanismi che generano o influenzano un fenomeno. Non si limita quindi a descrivere cosa accade o a prevedere ciò che potrebbe accadere (come una semplice correlazione o un modello predittivo), ma cerca di chiarire perché il fenomeno si verifica. Ad esempio, dire “i bambini con genitori laureati hanno migliori prestazioni scolastiche” è una descrizione (o previsione) utile; spiegare che ciò avviene a causa di un maggior sostegno nel percorso di studi, di un ambiente più ricco di stimoli culturali, o di un contesto socioeconomico facilitante, fornisce invece una spiegazione che va oltre la pura correlazione statistica.\n2. Perché, quando si parla di popolazione e campione, è fondamentale assicurarsi che il campione sia rappresentativo, e quali conseguenze possono derivare da un campione non rappresentativo?\nIl campione è il sottoinsieme di individui selezionati da una popolazione più ampia. Affinché i risultati di uno studio siano validi e generalizzabili, il campione deve rispecchiare le principali caratteristiche della popolazione (ad esempio in termini di età, genere, livello socioeconomico, ecc.). Se il campione non è rappresentativo (per esempio, se si reclutano solo studenti universitari per uno studio su tutta la popolazione italiana), possono emergere bias di selezione che rendono impossibile estendere correttamente i risultati a gruppi sociali diversi. Conseguenze tipiche di un campione non rappresentativo includono stime distorte dei parametri d’interesse, conclusioni fuorvianti e ridotta validità esterna della ricerca.\n3. Che differenza c’è tra un parametro e una statistica, e perché in inferenza statistica si cerca di stimare il parametro sconosciuto a partire dalla statistica campionaria?\n\nUn parametro è una caratteristica numerica della popolazione (ad esempio la media reale di un determinato tratto o la proporzione di individui con una certa caratteristica).\n\nUna statistica è una misura analoga, ma calcolata sul campione (ad esempio la media o la proporzione campionaria).\n\nPoiché in genere è impossibile o molto costoso misurare l’intera popolazione, si raccoglie un campione più piccolo e gestibile. La statistica del campione (ad es. la media campionaria) è quindi usata per stimare il parametro (ad es. la media della popolazione). L’obiettivo dell’inferenza statistica è fornire, insieme a questa stima, una misura dell’incertezza associata (per esempio un intervallo di confidenza), così da comprendere quanto la statistica campionaria potrebbe “avvicinarsi” al vero valore del parametro.\n4. Cosa si intende per bias nella raccolta e interpretazione dei dati, e in che modo la consapevolezza dei possibili bias può migliorare la qualità della ricerca?\nIl bias è un errore sistematico che altera i risultati di uno studio in una direzione specifica, dovuto a scelte o condizioni nel disegno della ricerca, nella selezione del campione, nella misurazione o nell’interpretazione dei dati. Ad esempio, se reclutiamo solo volontari particolarmente motivati a partecipare a una ricerca, potremmo ottenere risultati che sovrastimano un certo fenomeno e non rispecchiano la popolazione generale.\nEssere consapevoli di come i bias possano nascere aiuta i ricercatori a mitigarli (ad esempio, bilanciando il reclutamento dei partecipanti o rendendo anonima la compilazione di un questionario) e a tenere conto dei loro effetti quando si interpretano i risultati. Così, la ricerca risulta più affidabile e validamente interpretata.\n5. Perché in psicologia e nelle scienze sociali risulta essenziale integrare l’analisi dei dati con un quadro teorico solido e coerente?\nNelle scienze sociali e in psicologia, i fenomeni studiati sono spesso complessi e influenzati da molte variabili. I dati da soli, senza una teoria, forniscono soltanto una descrizione o una misurazione di ciò che accade in un dato momento. La teoria invece permette di:\n\nIdentificare le variabili rilevanti e formulare ipotesi specifiche;\n\nInterpretare i risultati, attribuendo un senso e un contesto alle relazioni osservate;\n\nComprendere i meccanismi causali e sviluppare spiegazioni che vadano oltre la pura descrizione.\n\nSenza un quadro teorico di riferimento, sarebbe difficile capire perché si osservano determinate relazioni e come possano cambiare in contesti diversi o in situazioni sperimentali alternative.\n6. Qual è la differenza principale tra uno studio osservazionale e un esperimento, e perché la distinzione è importante per comprendere la causalità?\n\nStudio osservazionale: Il ricercatore raccoglie i dati senza intervenire né manipolare alcuna variabile. Ad esempio, si misura il livello di stress delle persone e la loro produttività sul lavoro, senza modificare artificialmente il livello di stress. Questi studi mostrano correlazioni, ma è difficile stabilire con certezza relazioni di causa-effetto.\n\nEsperimento: Il ricercatore manipola una o più variabili (variabili indipendenti) e controlla le condizioni, ad esempio assegnando in modo casuale i partecipanti a un gruppo di trattamento e a uno di controllo. Ciò facilita la comprensione di eventuali nessi causali, perché la randomizzazione e il controllo degli altri fattori riducono il rischio che variabili esterne influenzino i risultati.\n\nLa distinzione è cruciale perché, nei fenomeni complessi della psicologia, gli studi osservazionali possono suggerire ipotesi di relazione, ma di solito occorre un disegno sperimentale (quando possibile) per trarre conclusioni più solide sulla causalità.\n7. Che ruolo svolgono i modelli scientifici in psicologia, e quali caratteristiche fondamentali dovrebbero possedere per essere considerati validi e utili?\nI modelli scientifici in psicologia forniscono una struttura concettuale e spesso formale (matematica o simulativa) per rappresentare e spiegare processi mentali e comportamentali. Servono a:\n\nOrganizzare osservazioni ed evidenze in un sistema coerente;\n\nFare previsioni verificabili empiricamente;\n\nGuidare l’interpretazione di nuovi dati e la progettazione di futuri studi.\n\nCaratteristiche di un buon modello sono:\n\nCoerenza descrittiva (rappresenta fedelmente il fenomeno);\n\nCapacità predittiva (prevede correttamente i risultati di situazioni nuove);\n\nSupporto empirico (confermato dai dati raccolti rigorosamente);\n\nFalsificabilità (dev’essere possibile smentirlo con evidenze contrarie);\n\nParsimonia (non dev’essere inutilmente complicato);\n\nGeneralizzabilità (applicabile a diversi contesti e situazioni);\n\nUtilità pratica (fornisce indicazioni utili per interventi o comprensione teorica).\n\n8. In che modo l’analisi dei dati aiuta a passare dalle semplici correlazioni o tendenze osservate all’elaborazione di ipotesi e spiegazioni più profonde?\nL’analisi dei dati non si limita a segnalare che “due variabili sono associate” (correlazioni), ma offre:\n\nStrumenti per isolare l’effetto di una variabile sulle altre (regressioni multiple, modelli a effetti misti, ecc.);\n\nMetodologie per la verifica di ipotesi specifiche sulla direzione e sulla natura delle relazioni (ad es. test statistici o modelli di mediazione-moderazione in psicologia);\n\nIndicatori dell’incertezza e della robustezza dei risultati (intervalli di confidenza, analisi della potenza, analisi bayesiane).\n\nCon questi strumenti, i ricercatori possono integrare i risultati quantitativi con le teorie esistenti, sviluppare nuove ipotesi su meccanismi causali e proporre spiegazioni più articolate su come e perché le variabili si influenzino reciprocamente.\n9. Che differenza c’è tra variabili indipendenti e variabili dipendenti, e perché questa distinzione è cruciale per disegnare uno studio e interpretarne i risultati?\n\nVariabile indipendente (VI): è quella che si sospetta abbia un effetto su un’altra variabile, o che si desidera manipolare in un disegno sperimentale (per esempio, l’introduzione di un nuovo metodo di studio).\n\nVariabile dipendente (VD): è la variabile che si misura per valutare l’eventuale effetto della variabile indipendente (ad esempio, i risultati di un test di apprendimento).\nLa distinzione è basilare perché chiarisce la direzione del rapporto di interesse e permette di formulare ipotesi come “VI → VD” (es. “il nuovo metodo di studio migliora i risultati del test”). Sbagliare a identificare quali sono le variabili indipendenti e dipendenti può portare a disegni di ricerca confusi e interpretazioni errate.\n\n10. Perché parlare di incertezza è inevitabile quando si utilizzano i dati di un campione, e come la statistica (frequentista o bayesiana) ci aiuta a gestirla?\nQuando raccogliamo dati da un campione (necessariamente limitato), non possiamo osservare l’intera popolazione. Questo introduce un margine di incertezza su quanto la misura campionaria (statistica) rispecchi il parametro reale della popolazione. Inoltre, possono sempre esserci fattori non controllati o errori di misurazione.\n\nNell’approccio frequentista, l’incertezza è gestita tramite concetti come gli intervalli di confidenza e i valori p, che quantificano la probabilità di osservare determinati risultati assumendo determinate ipotesi (per es. l’ipotesi nulla).\n\nNell’approccio bayesiano, l’incertezza è modellata tramite distribuzioni di probabilità (posteriori) che incorporano sia i dati osservati sia le informazioni pregresse (priors).\n\nEntrambi gli approcci forniscono metodologie per valutare quanto ci si possa fidare di una data conclusione, riconoscendo il carattere aleatorio e parziale dei dati e rendendo esplicito il grado di incertezza.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/02_key_notions.html#bibliografia",
    "href": "chapters/key_notions/02_key_notions.html#bibliografia",
    "title": "3  Concetti chiave",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nGelman, A., Hill, J., & Vehtari, A. (2021). Regression and other stories. Cambridge University Press.\n\n\nHuntington-Klein, N. (2021). The effect: An introduction to research design and causality. Chapman; Hall/CRC.\n\n\nJohnson, A. A., Ott, M., & Dogucu, M. (2022). Bayes Rules! An Introduction to Bayesian Modeling with R. CRC Press.\n\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2nd Edition). CRC Press.\n\n\nMurray, E. J., & Carr, K. C. (2024). Measuring Racial Sentiment Using Social Media Is Harder Than It Seems. Epidemiology, 35(1), 60–63.\n\n\nNobles, M. (2000). Shades of citizenship: Race and the census in modern politics. Stanford University Press.\n\n\nSteinman, R. M., Pizlo, Z., & Pizlo, F. J. (2000). Phi is not beta, and why Wertheimer’s discovery launched the Gestalt revolution. Vision research, 40(17), 2257–2264.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/03_design.html",
    "href": "chapters/key_notions/03_design.html",
    "title": "4  Campionamento, metodologia sperimentale e studi osservazionali",
    "section": "",
    "text": "4.1 Introduzione\nQuesto capitolo si propone di approfondire i concetti introdotti in precedenza, concentrandosi in particolare sul processo di campionamento e sull’importanza delle diverse metodologie di ricerca in psicologia. Dopo aver compreso il ruolo fondamentale dei dati nella verifica delle teorie, emerge una questione cruciale: come vengono raccolti questi dati?\nLa raccolta dei dati non è un’attività neutra o priva di conseguenze metodologiche. I dati, infatti, non hanno lo stesso valore scientifico a seconda di come vengono ottenuti. Alcune modalità di raccolta generano informazioni preziose e utili per testare le teorie, mentre altre possono produrre risultati fuorvianti, distorti o addirittura dannosi per la validità della ricerca.\nIl metodo scientifico fornisce un quadro di riferimento che delinea le caratteristiche ideali di un processo di raccolta dati in grado di produrre informazioni valide e affidabili. Tuttavia, le prescrizioni del metodo scientifico sono per loro natura generali e astratte. Tradurre questi principi in procedure concrete e applicarli efficacemente in un contesto di ricerca specifico rappresenta una sfida significativa.\nQuesto passaggio, che collega la teoria alla pratica, dipende dalle risorse disponibili, dalla competenza metodologica e dalla creatività del ricercatore. La capacità di ideare e attuare strategie di raccolta dati adeguate al contesto specifico della ricerca è fondamentale per garantire la qualità e la validità dei risultati ottenuti. In questo capitolo, approfondiremo i principi del campionamento e introdurremo i concetti chiave relativi ai disegni di ricerca.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Campionamento, metodologia sperimentale e studi osservazionali</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/03_design.html#introduzione",
    "href": "chapters/key_notions/03_design.html#introduzione",
    "title": "4  Campionamento, metodologia sperimentale e studi osservazionali",
    "section": "",
    "text": "Prerequisiti\n\n\n\n\n\n\nConsultare A discipline-wide investigation of the replicability of Psychology papers over the past two decades (Youyou et al., 2023).",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Campionamento, metodologia sperimentale e studi osservazionali</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/03_design.html#popolazioni-e-campioni",
    "href": "chapters/key_notions/03_design.html#popolazioni-e-campioni",
    "title": "4  Campionamento, metodologia sperimentale e studi osservazionali",
    "section": "4.2 Popolazioni e Campioni",
    "text": "4.2 Popolazioni e Campioni\nNella ricerca scientifica, è essenziale distinguere tra popolazione e campione.\n\nPopolazione: rappresenta l’insieme completo di unità che condividono una o più caratteristiche specifiche oggetto di studio. La dimensione della popolazione è indicata con N.\nCampione: è un sottoinsieme della popolazione, di dimensione n. L’obiettivo del campionamento è ottenere un campione rappresentativo, ovvero un sottoinsieme che rifletta accuratamente le caratteristiche della popolazione di riferimento.\n\n\n4.2.1 Metodi di Campionamento\nEsistono diverse strategie per selezionare un campione rappresentativo da una popolazione. Queste strategie si dividono principalmente in due categorie: campionamento probabilistico e campionamento non probabilistico.\n\n4.2.1.1 Campionamento Probabilistico\nNel campionamento probabilistico, ogni unità della popolazione ha una probabilità nota e non nulla di essere inclusa nel campione. Questo approccio minimizza il rischio di distorsioni sistematiche (bias) e consente di stimare l’errore di campionamento.\n\nCampionamento Casuale Semplice (CCS):\nOgni unità della popolazione ha la stessa probabilità di essere inclusa nel campione. Questo metodo richiede una lista completa di tutte le unità della popolazione, nota come frame di campionamento. La selezione può avvenire con o senza reinserimento. Il CCS senza reinserimento è il più comune nella pratica, ma nelle ricerche psicologiche è raramente utilizzabile a causa della difficoltà di ottenere un frame completo della popolazione.\nCampionamento Stratificato:\nLa popolazione viene divisa in strati (H), ovvero sottogruppi omogenei in base a una o più variabili rilevanti (es. età, genere, regione geografica). Da ogni strato h viene estratto un campione casuale semplice di dimensione nh. Questo metodo può essere:\n\nProporzionale: la dimensione del campione in ogni strato è proporzionale alla dimensione dello strato nella popolazione.\nNon proporzionale: utilizzato per sovra-campionare gruppi minoritari, consentendo analisi dettagliate di sottogruppi altrimenti poco rappresentati.\nNelle ricerche psicologiche, il campionamento stratificato è utile per garantire che variabili come il genere o l’età siano adeguatamente rappresentate, ma può essere complesso da implementare.\n\nCampionamento a Grappolo (Cluster Sampling):\nLa popolazione viene suddivisa in grappoli (cluster), che rappresentano gruppi eterogenei (es. scuole, ospedali, quartieri). Vengono selezionati casualmente alcuni grappoli, includendo tutte le unità al loro interno. Questo metodo è economico e pratico, specialmente in contesti dove accedere all’intera popolazione è difficile. Tuttavia, la precisione può essere ridotta se i grappoli differiscono notevolmente tra loro.\nCampionamento Multistadio:\nCombinazione di campionamento a grappolo e CCS. Vengono selezionati casualmente alcuni grappoli e, successivamente, all’interno di ciascun grappolo, si estrae un campione casuale di unità. Questo metodo bilancia costi e precisione, risultando particolarmente adatto a studi su larga scala, ad esempio a livello nazionale.\n\n\n\n4.2.1.2 Campionamento Non Probabilistico\nNel campionamento non probabilistico, la probabilità di inclusione di ogni unità nel campione non è nota. Questo approccio è spesso adottato per ragioni di praticità o quando non è disponibile un frame di campionamento. Tuttavia, aumenta il rischio di distorsioni e limita la generalizzabilità dei risultati alla popolazione.\n\nCampionamento di Convenienza:\nÈ il metodo più diffuso nella ricerca psicologica. I partecipanti vengono selezionati in base alla loro facile accessibilità (es., studenti universitari, volontari). Questo metodo è rapido ed economico, ma introduce significativi bias di selezione, poiché il campione non è rappresentativo della popolazione generale.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Campionamento, metodologia sperimentale e studi osservazionali</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/03_design.html#il-campionamento-nella-ricerca-psicologica",
    "href": "chapters/key_notions/03_design.html#il-campionamento-nella-ricerca-psicologica",
    "title": "4  Campionamento, metodologia sperimentale e studi osservazionali",
    "section": "4.3 Il Campionamento nella Ricerca Psicologica",
    "text": "4.3 Il Campionamento nella Ricerca Psicologica\nIl tema del campionamento nella ricerca psicologica è cruciale perché influisce in modo diretto sulla validità esterna e sulla generalizzabilità dei risultati. Nella pratica, però, l’ideale metodologico del campionamento probabilistico è spesso difficile da perseguire, a causa di vincoli di tempo, di risorse economiche e di accessibilità ai partecipanti (Henrich et al., 2010a). Per queste ragioni, molti studi in psicologia fanno ricorso al campionamento di convenienza, che prevede la selezione dei partecipanti sulla base della loro facile reperibilità, come studenti universitari o volontari reclutati online.\n\n4.3.1 Perché il Campionamento di Convenienza è così Diffuso?\n\nVincoli di Risorse\nLa ricerca psicologica, specie in ambito accademico, spesso non dispone di finanziamenti sufficienti per condurre campionamenti su larga scala. Reclutare partecipanti rappresentativi di un’intera popolazione richiederebbe budget, logistica e tempo ben superiori a quelli generalmente disponibili (Peterson & Merunka, 2014). Il campionamento di convenienza diventa quindi un compromesso “necessario” per poter portare avanti gli studi.\nAccessibilità ai Partecipanti\nNel contesto universitario, gli studenti rappresentano il bacino più facilmente accessibile e motivato a partecipare a studi psicologici, talvolta in cambio di crediti formativi o rimborsi simbolici (Sears, 1986). In altri contesti, si ricorre a piattaforme online che forniscono volontari in tempi brevi, consentendo di raccogliere dati in modo rapido e a costi contenuti.\nRapidità di Raccolta Dati\nIl vantaggio principale del campionamento di convenienza è la possibilità di raccogliere dati in tempi molto più ridotti rispetto a strategie di campionamento probabilistico. Tale rapidità può risultare essenziale per studi pilota o ricerche che richiedono analisi preliminari di fenomeni ancora poco esplorati.\n\n\n\n4.3.2 Limiti e Implicazioni\nIl ricorso al campionamento di convenienza comporta inevitabilmente dei limiti in termini di rappresentatività del campione. Gli individui che si prestano a partecipare a uno studio potrebbero avere caratteristiche socio-demografiche, cognitive o motivazionali peculiari (ad esempio, essere più giovani, con livelli di istruzione più alti, culturalmente più omogenei), portando ad un fenomeno noto come “campioni WEIRD” [Western, Educated, Industrialized, Rich, Democratic; Henrich et al. (2010b)]. Ciò significa che i risultati ottenuti potrebbero non riflettere adeguatamente l’intera variabilità della popolazione umana.\n\nGeneralizzabilità Ridotta: Uno studio condotto su studenti di psicologia in un’università europea può non essere applicabile a individui di diverse fasce di età, provenienti da altre aree geografiche o con retroterra socio-culturali differenti.\nBias di Selezione: I partecipanti volontari, specialmente online, possono essere attratti dallo studio per ragioni specifiche (ad esempio, curiosità verso la psicologia, tempo libero a disposizione, motivazione a ottenere un compenso), introducendo distorsioni non presenti nella popolazione più ampia.\nLimitazioni nei Risultati: Se i processi psicologici oggetto di indagine sono influenzati da cultura, età o altre variabili, lo studio potrebbe non catturare in modo esaustivo la complessità del fenomeno.\n\n\n\n4.3.3 Perché in Psicologia è (in Parte) Accettabile\nSebbene il campionamento di convenienza costituisca un limite, è bene ricordare che molti fenomeni psicologici presentano elementi di base che sono relativamente generali o universali nell’essere umano [ad esempio, i processi di percezione, l’apprendimento di base, alcune dinamiche emotive e motivazionali; cfr. Tooby & Cosmides (2005)]. Ciò significa che, entro certi confini, studiare un campione di convenienza può comunque fornire indicazioni utili e trasferibili ad altre popolazioni. Inoltre, buona parte delle ricerche in psicologia mira ad approfondire meccanismi e processi interna corporis – cioè aspetti di natura cognitiva, emotiva o sociale che, pur potendo variare in intensità o manifestazione, hanno basi comuni tra gli individui.\nIn altre parole, esiste un trade-off tra la necessità di disporre di campioni rappresentativi per garantire la massima generalizzabilità e la specificità dei fenomeni psicologici, che talvolta risiedono in processi considerati “universali”. Se lo scopo di uno studio è quello di testare meccanismi cognitivi di base (per esempio, l’elaborazione di stimoli visivi o di memoria), un campione di studenti potrebbe comunque fornire dati sufficientemente robusti, purché si riconoscano i limiti del contesto di raccolta.\n\n4.3.3.1 Considerazioni Etiche e Pratiche\nTalvolta, per ragioni etiche, non è semplice reperire partecipanti da particolari fasce di popolazione (ad esempio minori, pazienti clinici, soggetti in contesti istituzionali). In alcune ricerche, poi, l’uso di questionari lunghi o procedure sperimentali intense rende difficile attrarre volontari al di fuori dell’ambiente universitario. Da questo punto di vista, avere un bacino di partecipanti noti (ad es. studenti di psicologia) agevola la raccolta dei dati.\n\nRicompense: Spesso i dipartimenti offrono crediti formativi o piccoli compensi ai partecipanti, innescando un circolo virtuoso di interesse per la ricerca.\nLibertà di rifiuto: Le università dispongono di comitati etici che tutelano i diritti dei partecipanti, garantendo che anche i reclutamenti “di comodo” rispettino i principi etici fondamentali (consenso informato, anonimato, diritto al recesso, ecc.).\n\n\n\n\n4.3.4 Come Mitigare i Limiti del Campionamento di Convenienza\n\nReplicazione Incrociata (Cross-Replication)\nUn metodo efficace per aumentare la validità dei risultati è replicare lo stesso studio su campioni differenti, di età diversa o provenienti da contesti socio-culturali eterogenei. Ripetere la ricerca in più contesti e ottenere risultati simili fornisce evidenza della generalizzabilità del fenomeno.\nCampionamento Diversificato\nAnche se si ricorre a un campionamento di convenienza, si può tentare di diversificare la provenienza dei partecipanti (ad esempio, includendo studenti di diverse facoltà o atenei, oppure reclutando volontari su piattaforme online internazionali). Un campione che rifletta almeno in parte una maggiore varietà di background socio-culturali è comunque preferibile alla selezione di un solo gruppo omogeneo.\nCaratterizzazione Dettagliata del Campione\nÈ fondamentale fornire informazioni precise su età, genere, livello di istruzione, estrazione socio-culturale e altre variabili rilevanti. Questo permette ai lettori di valutare in che misura i risultati dello studio possano essere trasferiti ad altre popolazioni.\nIntegrazione di Metodi di Campionamento Misti\nAlcuni ricercatori scelgono di integrare campionamenti non probabilistici con piccole porzioni di campionamento più stratificato o di selezionare sotto-campioni rappresentativi per determinati sottogruppi. Non si tratta di un vero e proprio campionamento probabilistico, ma può comunque migliorare la robustezza dei risultati rispetto al puro campionamento di convenienza.\n\n\n\n4.3.5 Considerazioni Economiche e Future Prospettive\nAlla luce di questi punti, appare chiaro che il ricorso a metodi di campionamento probabilistici più rigorosi (campionamento casuale semplice, stratificato o a grappolo) richiederebbe aumenti significativi nei finanziamenti e un impegno logistico maggiore. Ciò non è sempre fattibile in contesti di ricerca accademica o quando i progetti sono condotti con budget limitati.\nD’altro canto, i finanziamenti aggiuntivi permetterebbero di:\n\nReclutare campioni più ampi e diversificati, ad esempio attraverso agenzie di rilevazione professionali o sondaggi a livello nazionale.\n\nImplementare studi longitudinali su popolazioni geograficamente distribuite, riducendo il rischio di raccogliere dati esclusivamente da aree limitrofe ai centri di ricerca.\nSostenere progetti multicentrici a livello internazionale, che consentono di testare l’universalità o la specificità culturale dei fenomeni psicologici.\n\nFino a quando queste risorse non saranno disponibili su larga scala, il campionamento di convenienza rimarrà la soluzione più diffusa e “realistica” nella ricerca psicologica. Tuttavia, non bisogna considerarlo esclusivamente un limite: la specificità dei fenomeni psicologici, legati a processi condivisi tra gli individui, talvolta permette di estrarre conclusioni valide anche da campioni meno rappresentativi, purché si utilizzino adeguate cautele nell’interpretazione e si persegua la replicazione come prassi consolidata.\nIn sintesi, il campionamento di convenienza è una strategia inevitabile nell’attuale panorama della ricerca psicologica, caratterizzato da forti limitazioni di risorse, ma in molti casi risulta comunque sufficiente per investigare dinamiche e processi psicologici di base. L’auspicio per il futuro è di poter disporre di finanziamenti e collaborazioni che permettano di ampliare il raggio d’azione e la diversità dei partecipanti, rafforzando la validità e la generalizzabilità della produzione scientifica in psicologia.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Campionamento, metodologia sperimentale e studi osservazionali</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/03_design.html#metodologia-sperimentale",
    "href": "chapters/key_notions/03_design.html#metodologia-sperimentale",
    "title": "4  Campionamento, metodologia sperimentale e studi osservazionali",
    "section": "4.4 Metodologia Sperimentale",
    "text": "4.4 Metodologia Sperimentale\n\n4.4.1 Principi Fondamentali del Disegno Sperimentale\n\nControllo\nL’obiettivo è ridurre l’influenza di variabili confondenti (Z) sulla relazione tra la variabile indipendente (X) e quella dipendente (Y). Utilizzare un gruppo di controllo consente di stabilire un riferimento per valutare l’effetto del trattamento.\nRandomizzazione\nL’assegnazione casuale dei partecipanti ai gruppi sperimentali distribuisce le variabili confondenti in modo equilibrato tra i gruppi, aumentando la credibilità dell’inferenza causale tra X e Y. Questo approccio garantisce che eventuali differenze osservate siano attribuibili al trattamento e non a fattori esterni.\n\n\n\n4.4.2 Strategie di Mitigazione dei Bias\n\nCecità (Blinding)\nStrumento chiave per minimizzare l’influenza di aspettative e pregiudizi, sia nei partecipanti che nei ricercatori. Le principali modalità includono:\n\nCecità singola: I partecipanti non sono consapevoli del trattamento assegnato.\nCecità doppia: Sia i partecipanti che i ricercatori che interagiscono direttamente con loro non conoscono l’assegnazione dei trattamenti.\nCecità tripla: Né i partecipanti, né i ricercatori, né gli analisti dei dati sono a conoscenza dei trattamenti durante la raccolta e l’analisi dei dati.\n\nGruppo di Controllo\nL’introduzione di un gruppo che riceve un trattamento inerte (controllo) consente di isolare gli effetti psicologici legati alle aspettative dei partecipanti, distinguendoli dagli effetti specifici del trattamento.\nStandardizzazione delle Procedure\nGarantire che tutte le condizioni sperimentali, eccetto la variabile manipolata, siano mantenute costanti tra i gruppi. Questo riduce la variabilità non controllata, migliorando la comparabilità dei risultati.\n\n\n\n4.4.3 Nota sulla Replicazione\nLa replicazione degli esperimenti non è una caratteristica intrinseca del metodo sperimentale, ma rappresenta una pratica fondamentale nella scienza per verificare l’affidabilità e la generalizzabilità dei risultati. Si distingue in:\n\nReplicazione diretta: Ripetere lo stesso studio con le stesse condizioni.\nReplicazione concettuale: Ripetere lo studio modificando aspetti specifici per testare la robustezza del risultato.\n\n\n\n4.4.4 Tipologie di Disegni Sperimentali\n\nDisegno a Gruppi Indipendenti (Between-Subjects):\nI partecipanti vengono assegnati a un unico gruppo e sono esposti a una sola condizione sperimentale. Questo disegno è utile quando l’esposizione multipla potrebbe introdurre confondenti, ma richiede un campione più ampio per raggiungere lo stesso livello di precisione.\nDisegno a Misure Ripetute (Within-Subjects):\nGli stessi partecipanti vengono esposti a tutte le condizioni sperimentali. Questo approccio riduce la variabilità tra soggetti, migliorando la precisione delle stime. Tuttavia, richiede attenzione nel controllare gli effetti di ordine, come affaticamento o apprendimento, spesso attraverso il bilanciamento dell’ordine di presentazione dei trattamenti.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Campionamento, metodologia sperimentale e studi osservazionali</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/03_design.html#studi-osservazionali",
    "href": "chapters/key_notions/03_design.html#studi-osservazionali",
    "title": "4  Campionamento, metodologia sperimentale e studi osservazionali",
    "section": "4.5 Studi Osservazionali",
    "text": "4.5 Studi Osservazionali\nGli studi osservazionali possono essere classificati in:\n\nStudi Trasversali (Cross-Sectional)\nI dati vengono raccolti in un singolo momento. Utili per stimare la prevalenza di una condizione, ma non permettono di stabilire relazioni causali.\nStudi di Coorte (Cohort Studies)\nUn gruppo di individui (coorte) viene seguito nel tempo per osservare l’incidenza di un evento. Permettono di studiare la relazione tra esposizione e outcome, ma possono essere costosi e richiedere molto tempo.\nStudi Caso-Controllo (Case-Control Studies)\nVengono confrontati individui con una determinata condizione (casi) con individui senza la condizione (controlli) per identificare possibili fattori di rischio. Utili per studiare malattie rare, ma soggetti a bias di selezione e di ricordo.\n\nLe principali limitazioni degli studi osservazionali sono la presenza di variabili confondenti e la difficoltà di stabilire relazioni causali.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Campionamento, metodologia sperimentale e studi osservazionali</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/03_design.html#riflessioni-conclusive",
    "href": "chapters/key_notions/03_design.html#riflessioni-conclusive",
    "title": "4  Campionamento, metodologia sperimentale e studi osservazionali",
    "section": "4.6 Riflessioni Conclusive",
    "text": "4.6 Riflessioni Conclusive\nNel corso di questo capitolo, abbiamo esplorato l’importanza del campionamento e delle diverse strategie di ricerca in psicologia, mettendo in luce i vincoli metodologici e le implicazioni derivanti da scelte spesso guidate dalle risorse a disposizione. Se da un lato i metodi di campionamento probabilistico garantiscono maggiore controllo sulla rappresentatività del campione, dall’altro, la realtà accademica e il contesto operativo di molti studi psicologici spingono verso soluzioni di comodo, inevitabilmente limitanti ma non per questo prive di valore scientifico.\nLe complessità intrinseche nello studio di fenomeni psicologici – spesso universali e al tempo stesso influenzati da variabili culturali e individuali – ci ricordano quanto sia importante mantenere un equilibrato senso critico. Da un punto di vista metodologico, l’esigenza di replicare gli studi in più contesti rimane la prassi fondamentale per rafforzare la credibilità dei risultati. Altrettanto cruciale è la volontà di caratterizzare in modo trasparente i propri campioni, rendendo chiaro in che misura siano (o non siano) rappresentativi della popolazione d’interesse. In tal modo, la comunità scientifica può valutare con maggiore consapevolezza la trasferibilità delle conclusioni a contesti diversi.\nUn ulteriore spunto di riflessione riguarda la tensione tra la desiderabilità di disegni di ricerca ideali (con campioni estesi e probabilistici) e le inevitabili restrizioni di tempo e budget. Se l’implementazione di studi su larga scala o a livello multicentrico consentirebbe di cogliere le sfumature culturali e socio-demografiche dei fenomeni, è altrettanto vero che, nel panorama attuale, molte ricerche non potrebbero essere realizzate senza ricorrere a partecipanti di più facile accesso, come gli studenti universitari o i volontari online. Tale flessibilità operativa può comunque produrre conoscenza significativa, a condizione che il ricercatore rimanga vigile rispetto ai possibili bias introdotti.\nIn definitiva, l’invito a chiunque conduca ricerche psicologiche è quello di coltivare una mentalità aperta, volta a bilanciare i limiti contingenti (economici, logistici, etici) con la necessità di mantenere standard metodologici solidi. Ciò implica sfruttare le potenzialità dell’integrazione tra disegni sperimentali e osservazionali, prestare attenzione alle forme di bias e, soprattutto, adottare la replicazione come pratica sistematica. Solo attraverso questa sinergia fra rigore scientifico e consapevolezza delle risorse disponibili è possibile far progredire la disciplina su basi empiriche sempre più solide e generalizzabili.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Campionamento, metodologia sperimentale e studi osservazionali</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/03_design.html#esercizi",
    "href": "chapters/key_notions/03_design.html#esercizi",
    "title": "4  Campionamento, metodologia sperimentale e studi osservazionali",
    "section": "Esercizi",
    "text": "Esercizi\n\n\n\n\n\n\nProblemi\n\n\n\n\n\n\nPerché la fase di raccolta dei dati è considerata “non neutrale” e quali conseguenze può avere sull’affidabilità dei risultati di una ricerca psicologica?\nIn che modo i diversi metodi di campionamento (probabilistico vs. non probabilistico) influiscono sulla generalizzabilità delle conclusioni di uno studio e quali situazioni giustificano l’uso dell’uno o dell’altro?\nQuali sono i principali rischi nell’adottare un campionamento di convenienza in ricerche psicologiche, e quali strategie si possono utilizzare per mitigare questi rischi?\nIn che modo la randomizzazione e l’uso di gruppi di controllo supportano l’inferenza causale in un disegno sperimentale, e perché queste caratteristiche sono spesso più difficili da mantenere negli studi osservazionali?\nQuali sono i principali criteri da considerare quando si valuta la validità di uno studio in termini di campionamento, disegno di ricerca e replicabilità?\n\n\n\n\n\n\n\n\n\n\nSoluzioni\n\n\n\n\n\n1. Perché la fase di raccolta dei dati è considerata “non neutrale” e quali conseguenze può avere sull’affidabilità dei risultati di una ricerca psicologica?\nLa raccolta dei dati non è mai un processo completamente neutrale perché comporta scelte metodologiche e pratiche che possono influenzare la qualità e la natura delle informazioni ottenute. Ad esempio:\n\nSelezione del campione: Scegliere chi includere o escludere nella ricerca incide sulla rappresentatività del campione. Un campione non rappresentativo può produrre risultati distorti e difficilmente generalizzabili.\n\nModalità di somministrazione degli strumenti: La maniera in cui vengono somministrati questionari, test o interviste può influire sulle risposte dei partecipanti (per esempio, differenze tra somministrazione online vs. cartacea, questionari anonimi vs. non anonimi, ecc.).\n\nContesto e tempistiche: Il contesto ambientale (es., rumori, distrazioni) o il momento in cui i dati vengono raccolti (es., in prossimità di esami universitari) possono influenzare lo stato emotivo o motivazionale dei partecipanti.\n\nCome conseguenza, tutte queste variabili possono introdurre bias – ossia distorsioni sistematiche – che compromettono l’affidabilità e la validità dei risultati, rendendo l’interpretazione dei dati più complessa e potenzialmente fuorviante. In altre parole, se la fase di raccolta dati non è accuratamente progettata e condotta, la ricerca potrebbe fornire conclusioni scorrette o di limitata utilità scientifica.\n2. In che modo i diversi metodi di campionamento (probabilistico vs. non probabilistico) influiscono sulla generalizzabilità delle conclusioni di uno studio e quali situazioni giustificano l’uso dell’uno o dell’altro?\n\nCampionamento probabilistico:\n\nOgni unità della popolazione ha una probabilità nota e non nulla di essere selezionata.\n\nMetodi come il campionamento casuale semplice, stratificato o a grappolo forniscono un quadro più solido per stimare l’errore di campionamento e minimizzare i bias.\n\nI risultati ottenuti sono, in linea di massima, più facilmente generalizzabili all’intera popolazione di riferimento.\n\nÈ preferibile in studi di larga scala, in cui esiste un buon frame di campionamento (lista esaustiva della popolazione) e le risorse (tempo, fondi) consentono di implementare un disegno rigoroso.\n\nCampionamento non probabilistico:\n\nLa probabilità di inclusione di un’unità non è nota, per cui non si possono calcolare in modo rigoroso le stime di errore.\n\nIl metodo più comune è il campionamento di convenienza, in cui vengono reclutate persone facilmente accessibili (es., studenti universitari, volontari, piattaforme online).\n\nLa generalizzabilità dei risultati è ridotta, poiché il campione potrebbe non rispecchiare le caratteristiche della popolazione di interesse.\n\nÈ spesso utilizzato per studi esplorativi, ricerche a budget limitato o quando non si dispone di un elenco completo della popolazione.\n\n\nIn sintesi, il campionamento probabilistico è preferibile quando si mira a ottenere risultati solidi e generalizzabili a una popolazione più ampia e le condizioni logistiche lo consentono. Il campionamento non probabilistico, invece, è giustificato in studi preliminari, in situazioni in cui la popolazione non è ben definita o difficilmente accessibile, o quando si hanno vincoli di risorse che rendono impraticabile un campionamento probabilistico.\n3. Quali sono i principali rischi nell’adottare un campionamento di convenienza in ricerche psicologiche, e quali strategie si possono utilizzare per mitigare questi rischi?\n\nPrincipali rischi:\n\nBias di selezione: I partecipanti reclutati con metodi di convenienza (ad es. studenti di psicologia) potrebbero non rispecchiare l’eterogeneità della popolazione generale, limitando la generalizzabilità dei risultati.\n\nOmogeneità del campione: Se il campione è molto omogeneo (per età, livello di istruzione, contesto culturale), diventa difficile estendere le conclusioni a gruppi con caratteristiche diverse.\n\nAutoselezione: I volontari che si offrono di partecipare potrebbero differire sistematicamente da coloro che non partecipano (ad esempio, maggiore interesse per il tema della ricerca o per la ricompensa economica offerta).\n\nStrategie di mitigazione:\n\nSovra-campionamento: Includere deliberatamente più partecipanti appartenenti a gruppi minoritari o sottorappresentati, per disporre di sottocampioni più completi.\n\nReplicazione: Ripetere l’esperimento con campioni diversi (per età, contesto geografico, cultura) per verificare se i risultati si mantengono coerenti.\n\nDescrizione dettagliata del campione: Fornire informazioni precise sulle caratteristiche sociodemografiche (età, genere, livello di istruzione, ecc.) in modo che altri ricercatori o lettori possano valutare la trasferibilità dei risultati.\n\nCautela nell’interpretazione: Esplicitare nelle conclusioni i limiti relativi alla natura del campionamento e invitare a considerare possibili fattori confondenti legati alla non rappresentatività del campione.\n\n\n4. In che modo la randomizzazione e l’uso di gruppi di controllo supportano l’inferenza causale in un disegno sperimentale, e perché queste caratteristiche sono spesso più difficili da mantenere negli studi osservazionali?\n\nRandomizzazione:\n\nAssegna i partecipanti ai gruppi sperimentali (condizione sperimentale vs. condizione di controllo) in maniera casuale.\n\nGarantisce che variabili potenzialmente confondenti vengano distribuite equamente tra i gruppi, aumentando la probabilità che eventuali differenze nelle misure di esito (variabile dipendente) siano dovute esclusivamente alla manipolazione sperimentale (variabile indipendente).\n\nQuesto processo riduce l’influenza di fattori esterni non misurati o non conosciuti, favorendo un’inferenza causale più solida.\n\nGruppi di controllo:\n\nConsentono di confrontare i risultati di chi riceve il trattamento/intervento con chi non lo riceve (o riceve un trattamento placebo).\n\nAiutano a isolare l’effetto “vero” del trattamento dalle variazioni dovute a effetti psicologici (ad es. effetto placebo), al passare del tempo o a eventi esterni.\n\nDifficoltà negli studi osservazionali:\n\nNon prevedono la manipolazione diretta di una variabile indipendente né l’assegnazione casuale dei partecipanti: le persone “si assegnano da sole” alle condizioni.\n\nManca il controllo sperimentale: non è sempre possibile includere un gruppo di controllo o applicare procedure di randomizzazione.\n\nLe variabili confondenti possono agire in modo non controllabile e compromettere l’interpretazione causale: anche con analisi statistiche sofisticate, è difficile escludere del tutto la presenza di fattori esterni che influenzano la relazione tra esposizione e outcome.\n\n\nPer questi motivi, gli studi sperimentali (con randomizzazione e controllo) rimangono il metodo privilegiato per stabilire nessi di causalità, mentre gli studi osservazionali servono principalmente a generare ipotesi, descrivere fenomeni, o analizzare relazioni di associazione più che di causalità.\n5. Quali sono i principali criteri da considerare quando si valuta la validità di uno studio in termini di campionamento, disegno di ricerca e replicabilità?\n\nRappresentatività del campione:\n\nIl campione rispecchia realmente le caratteristiche della popolazione di interesse?\n\nÈ stato usato un metodo di campionamento appropriato (probabilistico vs. non probabilistico)?\n\nControllo e randomizzazione (validità interna):\n\nLo studio ha previsto un disegno sperimentale con assegnazione casuale e gruppo di controllo?\n\nQuanto è efficace il controllo delle variabili confondenti (bias di selezione, aspettative, effetto placebo, ecc.)?\n\nGeneralizzabilità o validità esterna:\n\nI risultati dello studio sono applicabili oltre il contesto specifico in cui è stato condotto?\n\nVi sono limitazioni dovute all’uso di un campione di convenienza o di un contesto culturale molto particolare?\n\nStandardizzazione delle procedure:\n\nLe istruzioni, i tempi di raccolta dati, i materiali utilizzati sono stati gestiti in modo uniforme per tutti i partecipanti?\n\nReplicabilità:\n\nÈ possibile ripetere lo studio (replicazione diretta o concettuale) e ottenere risultati simili?\n\nGli autori forniscono informazioni sufficienti (materiali, protocolli, analisi) per consentire la replicazione?\n\nChiarezza nell’esposizione dei limiti:\n\nGli autori discutono apertamente le limitazioni del metodo di campionamento o del disegno di ricerca e suggeriscono possibili miglioramenti per studi futuri?\n\n\nNel complesso, una valutazione critica di uno studio deve integrare tutti questi aspetti (campionamento, disegno, replicabilità) per stabilire in che misura i risultati siano solidi, attendibili e utilmente generalizzabili.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Campionamento, metodologia sperimentale e studi osservazionali</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/03_design.html#bibliografia",
    "href": "chapters/key_notions/03_design.html#bibliografia",
    "title": "4  Campionamento, metodologia sperimentale e studi osservazionali",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nHenrich, J., Heine, S. J., & Norenzayan, A. (2010a). Most people are not WEIRD. Nature, 466(7302), 29–29.\n\n\nHenrich, J., Heine, S. J., & Norenzayan, A. (2010b). The weirdest people in the world? Behavioral and Brain Sciences, 33(2-3), 61–83.\n\n\nPeterson, R. A., & Merunka, D. R. (2014). Convenience samples of college students and research reproducibility. Journal of Business Research, 67(5), 1035–1041.\n\n\nTooby, J., & Cosmides, L. (2005). Evolutionary psychology: Conceptual foundations. In D. M. Buss (A c. Di), The Handbook of Evolutionary Psychology (pp. 5–67). Wiley.\n\n\nYouyou, W., Yang, Y., & Uzzi, B. (2023). A discipline-wide investigation of the replicability of Psychology papers over the past two decades. Proceedings of the National Academy of Sciences, 120(6), e2208863120.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Campionamento, metodologia sperimentale e studi osservazionali</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/04_measurement.html",
    "href": "chapters/key_notions/04_measurement.html",
    "title": "5  La misurazione in psicologia",
    "section": "",
    "text": "5.1 Introduzione\nLa scienza si avvale di modelli per interpretare i dati, ma opera sempre con teorie incomplete e misurazioni soggette a errori. Di conseguenza, è fondamentale riconoscere le incertezze quando si cerca di estrarre informazioni dalle misurazioni utilizzando i nostri modelli. Nessuna misurazione, spiegazione o previsione è perfettamente accurata e precisa, e non possiamo mai conoscere con esattezza l’entità dei loro errori. Questo riconoscimento è alla base della teoria della misurazione, che cerca di quantificare e gestire queste incertezze per migliorare la qualità delle nostre conclusioni scientifiche.\nQuesta incertezza viene catturata in tre equazioni fondamentali. La prima è l’Equazione di Misurazione, che riconosce l’errore osservativo:\n\\[\ny = z + \\varepsilon_y,\n\\]\ndove \\(y\\) rappresenta il valore misurato, \\(z\\) il valore reale e \\(\\varepsilon_y\\) l’errore di misurazione. La seconda è l’Equazione di Modellazione, che esprime la presenza di un diverso tipo di errore:\n\\[\nz = f(x, \\theta) + \\varepsilon_\\text{model},\n\\]\ndove \\(f\\) è il modello, \\(x\\) sono le condizioni ambientali per cui eseguiamo il modello, \\(\\theta\\) sono i valori dei parametri del modello e \\(\\varepsilon_\\text{model}\\) rappresenta l’errore del modello, che sorge perché \\(f\\), \\(x\\) e \\(\\theta\\) saranno tutti in qualche misura imprecisi.\nCombinando queste due equazioni, otteniamo l’Equazione della Scienza:\n\\[\ny = f(x, \\theta) + \\varepsilon_\\text{model} + \\varepsilon_y.\n\\]\nLa scienza è il tentativo di spiegare le osservazioni \\(y\\) utilizzando un modello \\(f\\), cercando di minimizzare l’errore di misurazione \\(\\varepsilon_y\\) e l’errore del modello \\(\\varepsilon_\\text{model}\\), in modo che il modello possa essere utilizzato per fare previsioni sul mondo reale (\\(z\\)). L’approccio bayesiano alla scienza riconosce e quantifica le incertezze su tutti e sei gli elementi dell’Equazione della Scienza: \\(y\\), \\(f\\), \\(x\\), \\(\\theta\\), \\(\\varepsilon_\\text{model}\\) e \\(\\varepsilon_y\\).",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>La misurazione in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/04_measurement.html#introduzione",
    "href": "chapters/key_notions/04_measurement.html#introduzione",
    "title": "5  La misurazione in psicologia",
    "section": "",
    "text": "Panoramica del capitolo\n\nProprietà delle scale di misura di Stevens.\nQuali operazioni aritmetiche possono essere applicate a ciascun livello di scala e perchè.\nDistinguere tra variabili continue e discrete.\nComprendere il concetto di bias.\n\n\n\n\n\n\n\n\nPrerequisiti\n\n\n\n\n\n\nLeggere On the philosophical foundations of psychological measurement (Maul et al., 2016) sui fondamenti filosofici della misurazione psicologica.\nLeggere Psychological Measurement and the Replication Crisis: Four Sacred Cows (Lilienfeld & Strother, 2020). Questo articolo mette in relazione le proprietà delle misure psicologiche con la crisi della replicabilità dei risultati della ricerca.\nLeggere il ?sec-apx-numbers dell’Appendice.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>La misurazione in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/04_measurement.html#la-teoria-della-misurazione",
    "href": "chapters/key_notions/04_measurement.html#la-teoria-della-misurazione",
    "title": "5  La misurazione in psicologia",
    "section": "5.2 La teoria della misurazione",
    "text": "5.2 La teoria della misurazione\nLa teoria della misurazione, oggetto di questo capitolo, si concentra sull’errore di misurazione e sull’equazione fondamentale \\(y = z + \\varepsilon_y\\). Questa equazione può essere esaminata da tre prospettive distinte. La prima concerne l’affidabilità della misura, rappresentata dal termine \\(\\varepsilon_y\\). La psicometria, branca dedicata alla teoria della misurazione psicologica, si occupa di quantificare l’affidabilità delle misure psicologiche attraverso metodi come la Teoria Classica dei Test e la Teoria di Risposta all’Item.\nLa seconda prospettiva riguarda la validità delle misure psicologiche, ovvero quanto adeguatamente la misura \\(y\\) rappresenti il costrutto \\(z\\). Questo aspetto, più complesso dell’affidabilità, non può essere risolto meramente con metodi statistici, ma richiede una profonda comprensione delle teorie psicologiche e della loro capacità di descrivere e prevedere i fenomeni psicologici.\nLa terza prospettiva si concentra sulle procedure di assegnazione dei valori a \\(y\\), esplorando quali metodi (questionari, interviste, esperimenti) siano più appropriati e come valutarne l’adeguatezza.\n\n5.2.1 Costrutti psicologici\nLa teoria della misurazione sottolinea l’importanza di distinguere tra la procedura di misurazione e il costrutto che si intende misurare. Ad esempio, mentre la temperatura è un costrutto, il termometro è lo strumento di misurazione. Analogamente, l’abilità matematica è un costrutto, mentre un test di matematica è la procedura per misurarla.\nNelle scienze psicologiche e sociali, la misurazione presenta sfide uniche rispetto alle scienze fisiche, poiché i costrutti in esame sono spesso astratti e non direttamente osservabili. Ciò richiede una particolare attenzione alla validità e all’affidabilità degli strumenti di misurazione, nonché una costante riflessione sulle limitazioni e le potenziali fonti di errore.\nIl capitolo introduce concetti fondamentali relativi alla misurazione quantitativa delle caratteristiche psicologiche, con un focus sulla teoria delle scale di misura di Stevens (1946). Questa teoria fornisce un quadro concettuale per comprendere i diversi tipi di scale di misurazione e le operazioni matematiche appropriate per ciascuna. Inoltre, vengono esplorate alcune procedure di scaling psicologico, ovvero l’assegnazione di numeri all’intensità di fenomeni psicologici.\n\n\n5.2.2 Scaling psicologico\nLo scaling psicologico si occupa della trasformazione dei dati empirici raccolti durante uno studio psicologico in misure o punteggi che rappresentino accuratamente le caratteristiche psicologiche oggetto di indagine.\nScaling di Guttman. Uno dei metodi di scaling più noti è lo «Scaling di Guttman», che viene utilizzato per rappresentare relazioni ordinate tra gli elementi di una scala. Ad esempio, in un questionario sui sintomi dell’ansia, le domande possono essere disposte in ordine di intensità crescente dei sintomi. Secondo il modello di Guttman, se un partecipante risponde “sì” a una domanda che riflette un sintomo più intenso, ci si aspetta che abbia risposto “sì” anche a tutte le domande precedenti, che rappresentano sintomi di intensità minore. Questo approccio consente di costruire una scala che riflette in modo sistematico e coerente la gravità dei sintomi.\nScaling Thurstoniano. Lo «Scaling Thurstoniano» è un metodo utilizzato per misurare preferenze o giudizi soggettivi. Ad esempio, per valutare la preferenza tra diversi tipi di cibi, i partecipanti confrontano due cibi alla volta ed esprimono una preferenza. Le risposte vengono poi utilizzate per assegnare punteggi che riflettono la preferenza media per ciascun cibo.\nScaling Fechneriano. Lo scaling fechneriano si basa sulla legge di Fechner, secondo cui la percezione di uno stimolo aumenta in modo logaritmico rispetto alla sua intensità fisica. La misura fondamentale è la JND (Just Noticeable Difference), ovvero la minima differenza percepibile tra due stimoli. Secondo Fechner, sommando le JND si ottiene una scala psicologica dell’intensità percepita, utile per studiare grandezze sensoriali come luminosità, peso e suono (per es., Domini & Caudek, 2009).\nQuestionari Likert. I questionari Likert richiedono ai partecipanti di esprimere il loro grado di accordo con una serie di affermazioni su una scala a più livelli, che va da «fortemente in disaccordo» a «fortemente d’accordo». I punteggi ottenuti vengono sommati per rappresentare la posizione complessiva dell’individuo rispetto all’oggetto di studio.\n\n\n5.2.3 Metodi di valutazione delle scale psicologiche\nPer valutare le proprietà delle scale psicologiche, vengono utilizzati vari metodi. Ad esempio, l’affidabilità delle misure può essere analizzata utilizzando il coefficiente alpha di Cronbach o il coefficiente Omega di McDonald, entrambi utilizzati per misurare la coerenza interna delle risposte ai diversi item di un questionario. Inoltre, la validità delle scale può essere esaminata confrontando i risultati ottenuti con misure simili o attraverso analisi statistiche che verificano se la scala cattura accuratamente il costrutto psicologico che si intende misurare. La validità di costrutto è particolarmente cruciale, poiché riguarda la capacità della scala di misurare effettivamente il concetto psicologico che si intende esplorare.\n\n\n5.2.4 Prospettive moderne\nNegli ultimi anni, il dibattito sulla misurazione psicologica si è arricchito di nuove prospettive, grazie all’avvento di tecnologie avanzate e all’integrazione di approcci interdisciplinari. Ecco alcune delle tendenze più rilevanti.\nTeoria della Risposta agli Item. La Teoria della Risposta agli Item (IRT) ha guadagnato popolarità per la sua capacità di fornire stime più precise delle abilità latenti rispetto ai modelli classici. La IRT considera la probabilità che un individuo risponda correttamente a un item in funzione della sua abilità e delle caratteristiche dell’item stesso, offrendo una visione più dettagliata delle proprietà psicometriche degli strumenti di misurazione.\nApprocci Bayesiani. Gli approcci bayesiani stanno rivoluzionando il campo della psicometria, permettendo di incorporare informazioni a priori nelle stime e di aggiornare le credenze sulla base di nuovi dati. Questi metodi sono particolarmente utili per affrontare la complessità e l’incertezza inerenti alla misurazione psicologica.\nAnalisi di Rete. L’analisi di rete è un’altra metodologia emergente che vede i costrutti psicologici non come variabili latenti indipendenti, ma come reti di sintomi interconnessi. Questo approccio può offrire nuove intuizioni sulla struttura delle psicopatologie e sulla dinamica dei sintomi.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>La misurazione in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/04_measurement.html#le-scale-di-misurazione",
    "href": "chapters/key_notions/04_measurement.html#le-scale-di-misurazione",
    "title": "5  La misurazione in psicologia",
    "section": "5.3 Le scale di misurazione",
    "text": "5.3 Le scale di misurazione\nLe scale di misurazione sono strumenti fondamentali per assegnare numeri ai dati osservati, rappresentando le proprietà psicologiche. La teoria delle scale di Stevens (1946) identifica quattro tipi di scale di misurazione: nominali, ordinali, a intervalli e di rapporti. Ognuna di queste scale consente di effettuare operazioni aritmetiche diverse, poiché ciascuna di esse è in grado di “catturare” solo alcune delle proprietà dei fenomeni psicologici che si intende misurare.\n\n\n\nScale di misurazione.\n\n\n\n5.3.1 Scala nominale\nLa scala nominale è il livello di misurazione più semplice e corrisponde ad una tassonomia o classificazione delle categorie che utilizziamo per descrivere i fenomeni psicologici. I simboli o numeri che costituiscono questa scala rappresentano i nomi delle categorie e non hanno alcun valore numerico intrinseco. Con la scala nominale possiamo solo distinguere se una caratteristica psicologica è uguale o diversa da un’altra.\nI dati raccolti con la scala nominale sono suddivisi in categorie qualitative e mutuamente esclusive, in cui ogni dato appartiene ad una sola categoria. In questa scala, esiste solo la relazione di equivalenza tra le misure delle unità di studio: gli elementi del campione appartenenti a classi diverse sono differenti, mentre tutti quelli della stessa classe sono tra loro equivalenti.\nL’unica operazione algebrica consentita dalla scala nominale è quella di contare le unità di studio che appartengono ad ogni categoria e il numero totale di categorie. Di conseguenza, la descrizione dei dati avviene tramite le frequenze assolute e le frequenze relative.\nDalla scala nominale è possibile costruire altre scale nominali equivalenti alla prima, trasformando i valori della scala di partenza in modo tale da cambiare i nomi delle categorie, ma lasciando inalterata la suddivisione delle unità di studio nelle medesime classi di equivalenza. In altre parole, cambiando i nomi delle categorie di una variabile misurata su scala nominale, si ottiene una nuova variabile esattamente equivalente alla prima.\n\n\n5.3.2 Scala ordinale\nLa scala ordinale mantiene la caratteristica della scala nominale di classificare ogni unità di misura all’interno di una singola categoria, ma introduce la relazione di ordinamento tra le categorie. In quanto basata su una relazione di ordine, una scala ordinale descrive solo il rango di ordine tra le categorie e non fornisce informazioni sulla distanza tra di esse. Non ci dice, ad esempio, se la distanza tra le categorie \\(a\\) e \\(b\\) è uguale, maggiore o minore della distanza tra le categorie \\(b\\) e \\(c\\).\nUn esempio classico di scala ordinale è quello della scala Mohs per la determinazione della durezza dei minerali. Per stabilire la durezza dei minerali si usa il criterio empirico della scalfittura. Vengono stabiliti livelli di durezza crescente da 1 a 10 con riferimento a dieci minerali: talco, gesso, calcite, fluorite, apatite, ortoclasio, quarzo, topazio, corindone e diamante. Un minerale appartenente ad uno di questi livelli se scalfisce quello di livello inferiore ed è scalfito da quello di livello superiore.\n\n\n\nLa scala di durezza dei minerali di Mohs. Un oggetto è considerato più duro di X se graffia X. Sono incluse anche misure di durezza relativa utilizzando uno sclerometro, da cui emerge la non linearità della scala di Mohs (Burchard, 2004).\n\n\n\n\n5.3.3 Scala ad intervalli\nLa scala ad intervalli di misurazione include le proprietà della scala nominale e della scala ordinale e permette di misurare le distanze tra le coppie di unità statistiche in termini di un intervallo costante, chiamato “unità di misura”, a cui viene attribuito il valore “1”. L’origine della scala, ovvero il punto zero, è scelta arbitrariamente e non indica l’assenza della proprietà che si sta misurando. Ciò significa che la scala ad intervalli consente anche valori negativi e lo zero non viene attribuito all’unità statistica in cui la proprietà risulta assente.\nLa scala ad intervalli equivalenti consente l’esecuzione di operazioni algebriche basate sulla differenza tra i numeri associati ai diversi punti della scala, operazioni algebriche non possibili con le scale di misura nominale o ordinale. Tuttavia, il limite della scala ad intervalli è che non consente di calcolare il rapporto tra coppie di misure. È possibile affermare la differenza tra \\(a\\) e \\(b\\) come la metà della differenza tra \\(c\\) e \\(d\\) o che le due differenze sono uguali, ma non è possibile affermare che \\(a\\) abbia una proprietà misurata in quantità doppia rispetto a \\(b\\). In altre parole, non è possibile stabilire rapporti diretti tra le misure ottenute. Solo le differenze tra le modalità permettono tutte le operazioni aritmetiche, come la somma, l’elevazione a potenza o la divisione, che sono alla base della statistica inferenziale.\nNelle scale ad intervalli equivalenti, l’unità di misura è arbitraria e può essere cambiata attraverso una dilatazione, ovvero la moltiplicazione di tutti i valori della scala per una costante positiva. Inoltre, la traslazione, ovvero l’aggiunta di una costante a tutti i valori della scala, è ammessa poiché non altera le differenze tra i valori della scala. La scala rimane invariata rispetto a traslazioni e dilatazioni e dunque le uniche trasformazioni ammissibili sono le trasformazioni lineari:\n\\[\ny' = a + by, \\quad b &gt; 0.\n\\]\nInfatti, l’uguaglianza dei rapporti fra gli intervalli rimane invariata a seguito di una trasformazione lineare.\nEsempio di scala ad intervalli è la temperatura misurata in gradi Celsius o Fahrenheit, ma non Kelvin. Come per la scala nominale, è possibile stabilire se due modalità sono uguali o diverse: \\(30^\\circ C \\neq 20^\\circ C\\). Come per la scala ordinale è possibile mettere due modalità in una relazione d’ordine: \\(30^\\circ C &gt; 20^\\circ C\\). In aggiunta ai casi precedenti, però, è possibile definire una unità di misura per cui è possibile dire che tra \\(30^\\circ C\\) e \\(20^\\circ C\\) c’è una differenza di \\(30^\\circ - 20^\\circ = 10^\\circ C\\). I valori di temperatura, oltre a poter essere ordinati secondo l’intensità del fenomeno, godono della proprietà che le differenze tra loro sono direttamente confrontabili e quantificabili.\nIl limite della scala ad intervalli è quello di non consentire il calcolo del rapporto tra coppie di misure. Ad esempio, una temperatura di \\(80^\\circ C\\) non è il doppio di una di \\(40^\\circ C\\). Se infatti esprimiamo le stesse temperature nei termini della scala Fahrenheit, allora i due valori non saranno in rapporto di 1 a 2 tra loro. Infatti, \\(20^\\circ C = 68^\\circ F\\) e \\(40^\\circ C = 104^\\circ F\\). Questo significa che la relazione “il doppio di” che avevamo individuato in precedenza si applicava ai numeri della scala centigrada, ma non alla proprietà misurata (cioè la temperatura). La decisione di che scala usare (Centigrada vs. Fahrenheit) è arbitraria. Ma questa arbitrarietà non deve influenzare le inferenze che traiamo dai dati. Queste inferenze, infatti, devono dirci qualcosa a proposito della realtà empirica e non possono in nessun modo essere condizionate dalle nostre scelte arbitrarie che ci portano a scegliere la scala Centigrada piuttosto che quella Fahrenheit.\nConsideriamo ora l’aspetto invariante di una trasformazione lineare, ovvero l’uguaglianza dei rapporti fra intervalli. Prendiamo in esame, ad esempio, tre temperature: \\(20^\\circ C = 68^\\circ F\\), \\(15^\\circ C = 59^\\circ F\\), \\(10^\\circ C = 50 ^\\circ F\\).\nÈ facile rendersi conto del fatto che i rapporti fra intervalli restano costanti indipendentemente dall’unità di misura che è stata scelta:\n\\[\n  \\frac{20^\\circ C - 10^\\circ C}{20^\\circ C - 15^\\circ C} =\n  \\frac{68^\\circ F - 50^\\circ F}{68^\\circ F-59^\\circ F} = 2.\n\\]\n\n\n5.3.4 Scala di rapporti\nNella scala a rapporti equivalenti, lo zero non è arbitrario e rappresenta l’elemento che ha intensità nulla rispetto alla proprietà misurata. Per costruire questa scala, si associa il numero 0 all’elemento con intensità nulla e si sceglie un’unità di misura \\(u\\). Ad ogni elemento si assegna un numero \\(a\\) definito come \\(a = d / u\\), dove \\(d\\) rappresenta la distanza dall’origine. In questo modo, i numeri assegnati riflettono le differenze e i rapporti tra le intensità della proprietà misurata.\nIn questa scala, è possibile effettuare operazioni aritmetiche non solo sulle differenze tra i valori della scala, ma anche sui valori stessi della scala. L’unica scelta arbitraria è l’unità di misura, ma lo zero deve sempre rappresentare l’intensità nulla della proprietà considerata.\nLe trasformazioni ammissibili in questa scala sono chiamate trasformazioni di similarità e sono del tipo \\(y' = by\\), dove \\(b &gt; 0\\). In questa scala, i rapporti tra i valori rimangono invariati dopo le trasformazioni. In altre parole, se rapportiamo due valori originali e due valori trasformati, il rapporto rimane lo stesso: \\(\\frac{y_i}{y_j} = \\frac{y'_i}{y'_j}\\).",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>La misurazione in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/04_measurement.html#gerarchia-dei-livelli-delle-scale-di-misurazione",
    "href": "chapters/key_notions/04_measurement.html#gerarchia-dei-livelli-delle-scale-di-misurazione",
    "title": "5  La misurazione in psicologia",
    "section": "5.4 Gerarchia dei livelli delle scale di misurazione",
    "text": "5.4 Gerarchia dei livelli delle scale di misurazione\nSecondo Stevens (1946), esiste una gerarchia dei livelli delle scale di misurazione, denominati “livelli di scala”. Questi livelli sono organizzati in modo gerarchico, in cui la scala nominale rappresenta il livello più basso della misurazione, mentre la scala a rapporti equivalenti rappresenta il livello più alto.\n\nScala nominale: Classifica le categorie senza un ordine specifico.\nScala ordinale: Classifica le categorie in un ordine specifico, ma senza una misura precisa delle distanze.\nScala a intervalli: Misura le distanze tra le categorie con un intervallo costante, ma senza un punto zero assoluto.\nScala di rapporti: Misura le distanze con un intervallo costante e un punto zero assoluto.\n\n\n\n\nRelazioni tra i livelli di misurazione.\n\n\nPassando da un livello di misurazione ad uno più alto aumenta il numero di operazioni aritmetiche che possono essere compiute sui valori della scala.\n\n5.4.1 Variabili Discrete e Continue\nLe variabili possono essere classificate come variabili a livello di intervalli o di rapporti e possono essere sia discrete che continue.\n\nVariabili discrete: Assumono valori specifici ma non possono assumere valori intermedi.\nVariabili continue: Possono assumere qualsiasi valore all’interno di un intervallo specificato.\n\n\n\n\nVariabili discrete e continue.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>La misurazione in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/04_measurement.html#linterazione-tra-teoria-e-misurazione-nella-ricerca-scientifica",
    "href": "chapters/key_notions/04_measurement.html#linterazione-tra-teoria-e-misurazione-nella-ricerca-scientifica",
    "title": "5  La misurazione in psicologia",
    "section": "5.5 L’interazione tra teoria e misurazione nella ricerca scientifica",
    "text": "5.5 L’interazione tra teoria e misurazione nella ricerca scientifica\n\n5.5.1 Un caso di studio sul mind-body healing\nPer capire perché la misurazione in psicologia non possa essere ridotta a una semplice attribuzione di numeri, consideriamo un esempio recente. Uno studio sul mind-body healing, pubblicato su Nature Aungle & Langer (2023), riportava un’associazione tra pratiche mente-corpo e miglioramenti della salute fisica. Tuttavia, l’articolo è stato oggetto di severe critiche metodologiche, in particolare da parte dello statistico Andrew Gelman, che ha discusso i limiti del lavoro sul blog Statistical Modeling.\nQuesto caso è istruttivo perché mette in luce due aspetti fondamentali della ricerca scientifica:\n\nil ruolo indispensabile della teoria;\nl’importanza della misurazione accurata.\n\n\n\n5.5.2 Il ruolo della teoria\nGelman evidenzia come il problema principale dello studio sia la mancanza di un quadro teorico robusto. Per esempio:\n\ncosa si intende esattamente con “guarigione mente-corpo”?\nquali meccanismi psicologici o biologici potrebbero spiegarne gli effetti?\ncome si collega questa ipotesi a conoscenze già consolidate in neuroscienze, immunologia o psicologia della salute?\n\nSenza una teoria che fornisca definizioni chiare e collegamenti plausibili, i dati raccolti rischiano di essere solo numeri isolati, privi di reale significato. In questo senso, la teoria non è un “ornamento”, ma una condizione necessaria per formulare ipotesi verificabili e interpretare correttamente i risultati.\n\n\n5.5.3 I problemi della misurazione\nAccanto alle debolezze teoriche, lo studio presenta limiti importanti sul piano della misurazione.\n\nValidità degli strumenti: l’uso di scale non validate e di misure che non tengono conto di fattori come l’effetto placebo compromette la solidità delle conclusioni.\nValidità interna: il disegno sperimentale non garantiva un controllo adeguato delle variabili (assenza di blinding, randomizzazione debole), riducendo la credibilità delle inferenze causali.\nValidità esterna: campioni non rappresentativi rendono difficile generalizzare i risultati.\n\nQuando la misurazione non è accurata, si introduce “rumore” nei dati, che può nascondere relazioni reali o, peggio, generare correlazioni illusorie.\n\n\n5.5.4 Un approccio integrato\nQuesto esempio ci mostra che teoria e misurazione devono procedere insieme. Trascurare la teoria porta a un empirismo ingenuo, che raccoglie dati senza domande chiare. Trascurare la misurazione porta a costruire teorie su basi fragili. La scienza avanza invece grazie a un circolo virtuoso: la teoria guida la raccolta dei dati, e i dati aiutano a raffinare la teoria.\nPer lo psicologo, questo significa sviluppare:\n\nconsapevolezza epistemologica, per distinguere tra spiegazioni plausibili e arbitrarie;\ncompetenze metodologiche, per riconoscere quando una misura è valida, affidabile e adatta al fenomeno studiato.\n\nSolo così possiamo distinguere la ricerca scientifica solida dalla pseudoscienza e contribuire a un progresso realmente cumulativo.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>La misurazione in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/04_measurement.html#riflessioni-conclusive",
    "href": "chapters/key_notions/04_measurement.html#riflessioni-conclusive",
    "title": "5  La misurazione in psicologia",
    "section": "Riflessioni conclusive",
    "text": "Riflessioni conclusive\nLa misurazione in psicologia non è un semplice atto di raccolta di dati, ma un processo fondamentale per garantire che le osservazioni empiriche siano interpretabili alla luce di modelli teorici solidi. Una buona misurazione non si limita a ridurre l’errore, ma consente di attribuire un significato coerente ai punteggi ottenuti, facilitando così il progresso della conoscenza scientifica. Senza strumenti adeguati per la misurazione, il rischio è quello di costruire teorie su basi incerte, compromettendo la validità delle conclusioni tratte.\nDue pilastri sostengono dunque una ricerca psicologica rigorosa: la teoria e la misurazione. La teoria fornisce il quadro concettuale entro cui si interpretano i dati, definendo le ipotesi e orientando le analisi. La misurazione, invece, è il ponte tra i costrutti astratti e le osservazioni empiriche, traducendo concetti complessi in variabili operative affidabili. Nessuna delle due componenti può reggersi senza l’altra: una teoria senza misurazione adeguata rischia di rimanere speculativa, mentre una misurazione priva di un solido fondamento teorico può portare a dati privi di significato.\nNella valutazione di un qualsiasi studio psicologico, un approccio critico richiede quindi di esaminare sia la solidità del quadro teorico sia la qualità degli strumenti di misurazione adottati. Il progresso della ricerca dipende dalla capacità di integrare questi due elementi, attraverso metodologie che riducano l’incertezza e migliorino la precisione delle inferenze. Le moderne tecniche di analisi dei dati, i modelli psicometrici avanzati e le tecnologie digitali stanno ampliando le possibilità di misurazione, offrendo strumenti più sensibili e adattabili alla complessità dei fenomeni psicologici. Tuttavia, la sfida principale rimane la stessa: garantire che la misurazione sia non solo accurata, ma anche teoricamente fondata, affinché le conoscenze acquisite possano davvero contribuire alla comprensione della mente e del comportamento umano.\n\n\n\n\n\n\nProblemi 1\n\n\n\n\n\nEsercizio 1: Identificazione del Livello di Misurazione\nObiettivo: Comprendere i diversi livelli di misurazione applicati alla psicologia.\n\nIdentifica il livello di misurazione (nominale, ordinale, intervalli, rapporti) per ciascuna delle seguenti variabili psicologiche:\n\n\nTipo di terapia psicologica (Cognitivo-comportamentale, Psicodinamica, Umanistica)\n\n\nLivello di ansia auto-riferito su una scala da 1 a 10\n\n\nNumero di episodi depressivi in un anno\n\n\nTempo di reazione in millisecondi in un test cognitivo\n\n\n\nEsercizio 2: Confronto tra Scale\nObiettivo: Comprendere le differenze tra le scale di misurazione.\n\nSpiega la differenza tra una scala ordinale e una scala a intervalli utilizzando l’esempio della soddisfazione lavorativa.\nPerché il punteggio QI è misurato su una scala a intervalli e non su una scala a rapporti?\nIn che modo il punteggio di una scala di autostima su una scala Likert differisce da una misurazione su una scala di rapporti?\n\nEsercizio 3: Operazioni Aritmetiche Consentite\nObiettivo: Comprendere le operazioni matematiche consentite per ciascun livello di misurazione.\n\nQuali operazioni aritmetiche sono ammissibili per una scala nominale?\nPuò avere senso calcolare la media di punteggi su una scala ordinale? Perché?\nSe hai misurato il tempo di reazione in secondi, quali operazioni aritmetiche puoi eseguire?\n\nEsercizio 4: Trasformazioni Ammissibili\nObiettivo: Comprendere le trasformazioni possibili per ogni scala di misurazione.\n\nSe una variabile è misurata su una scala nominale, quale tipo di trasformazione è consentita?\nPer una scala a intervalli, quali trasformazioni matematiche sono permesse senza alterare le proprietà della scala?\nQuale tipo di trasformazione è consentita su una scala di rapporti?\n\nEsercizio 5: Applicazione delle Scale a Dati Psicologici\nObiettivo: Applicare i concetti a contesti psicologici reali.\n\nUna scala di ansia clinica fornisce punteggi compresi tra 0 e 100. Quale livello di misurazione è più appropriato e perché?\nUn esperimento misura la memoria dichiarativa chiedendo ai partecipanti di ricordare un elenco di parole. Come dovrebbe essere misurata la variabile “numero di parole ricordate”?\nIn uno studio sulla personalità, i tratti vengono classificati come “estroverso” e “introverso”. Qual è il livello di misurazione?\n\nEsercizio 6: Valutazione della Scala di Misurazione\nObiettivo: Identificare la corretta scala di misurazione per vari fenomeni psicologici.\n\nIl livello di aggressività misurato su una scala da 1 a 5 è nominale, ordinale, intervalli o rapporti? Giustifica la tua risposta.\nIl numero di attacchi di panico in una settimana può essere considerato su scala ordinale? Perché sì o perché no?\nUn test di intelligenza misura il QI con una media di 100 e una deviazione standard di 15. Qual è il livello di misurazione e quali sono le implicazioni per l’analisi statistica?\n\nEsercizio 7: Costruzione di una Scala Psicologica\nObiettivo: Creare una scala di misurazione per una variabile psicologica.\n\nSe dovessi costruire una scala per misurare la resilienza, quale livello di misurazione sceglieresti e perché?\nCome potresti trasformare una scala nominale di preferenza musicale in una scala ordinale?\nUn questionario sulla qualità della vita chiede ai partecipanti di valutare la loro felicità su una scala da 1 a 10. È una scala a intervalli o ordinale? Giustifica.\n\nEsercizio 8: Interpretazione Statistica dei Dati\nObiettivo: Collegare il livello di misurazione alle tecniche statistiche appropriate.\n\nPerché una mediana è più appropriata della media per dati ordinali?\nQuale test statistico sarebbe più adatto per confrontare due gruppi su una variabile nominale?\nQuali analisi possono essere condotte su dati raccolti su una scala a rapporti?\n\nEsercizio 9: Misurazione e Inferenze Psicologiche\nObiettivo: Riflettere su come il livello di misurazione influisce sulle conclusioni di una ricerca.\n\nSe un test di personalità usa una scala Likert da 1 a 7, quali precauzioni devono essere prese nell’interpretare le differenze tra punteggi?\nUn questionario di benessere assegna punteggi tra 0 e 100, ma non ha uno zero assoluto. Quale scala è questa e quali sono le limitazioni?\nIn uno studio sulla depressione, i sintomi vengono codificati come “assenti”, “moderati” o “gravi”. Che tipo di scala è questa e quali statistiche possono essere usate per analizzarla?\n\nEsercizio 10: Esperimenti Psicologici e Misurazione\nObiettivo: Applicare la teoria della misurazione nella progettazione di esperimenti psicologici.\n\nSe un esperimento misura la memoria a breve termine con un compito di richiamo di parole, quale scala di misurazione utilizzeresti?\nCome la scelta della scala di misurazione può influenzare le inferenze che si possono trarre da un esperimento?\nQuali tipi di analisi statistica sono appropriati per dati misurati su scala ordinale rispetto a scala di rapporti?\n\n\n\n\n\n\n\n\n\n\nSoluzioni 1\n\n\n\n\n\nEsercizio 1: Identificazione del Livello di Misurazione\nObiettivo: Comprendere i diversi livelli di misurazione applicati alla psicologia.\n\nIdentifica il livello di misurazione (nominale, ordinale, intervalli, rapporti) per ciascuna delle seguenti variabili psicologiche:\n\n\nNominale (Tipo di terapia psicologica è una classificazione senza ordine)\n\n\nOrdinale (Scala da 1 a 10, con ordine ma senza distanze uguali)\n\n\nRapporti (Numero di episodi depressivi ha uno zero assoluto e si possono fare rapporti tra valori)\n\n\nRapporti (Tempo di reazione ha uno zero assoluto e permette operazioni di rapporto)\n\n\n\nEsercizio 2: Confronto tra Scale\nObiettivo: Comprendere le differenze tra le scale di misurazione.\n\nLa scala ordinale fornisce un ordine ma non permette di calcolare differenze precise, mentre la scala a intervalli ha differenze costanti tra i valori. Ad esempio, “soddisfazione lavorativa” su una scala da 1 a 5 è ordinale, mentre il punteggio di un test psicologico è a intervalli.\nIl punteggio QI è a intervalli perché la differenza tra punteggi è significativa, ma non ha uno zero assoluto che rappresenta l’assenza di intelligenza.\nUna scala Likert misura il livello di accordo con una dichiarazione, quindi è generalmente considerata ordinale, nonostante sia trattata spesso come una scala a intervalli.\n\nEsercizio 3: Operazioni Aritmetiche Consentite\nObiettivo: Comprendere le operazioni matematiche consentite per ciascun livello di misurazione.\n\nNella scala nominale si può solo contare la frequenza delle categorie (ad es., il numero di partecipanti che usano un tipo di terapia).\nNo, la media su dati ordinali può essere fuorviante perché le distanze tra le categorie non sono necessariamente uguali. Meglio usare la mediana.\nSul tempo di reazione si possono eseguire tutte le operazioni aritmetiche, inclusa la media, la moltiplicazione e i rapporti tra valori.\n\nEsercizio 4: Trasformazioni Ammissibili\nObiettivo: Comprendere le trasformazioni possibili per ogni scala di misurazione.\n\nSulla scala nominale, solo le trasformazioni di ricodifica (ad esempio, cambiare i nomi delle categorie) sono permesse.\nPer una scala a intervalli, si possono effettuare trasformazioni lineari della forma y’ = a + by con b &gt; 0.\nPer una scala di rapporti, sono consentite trasformazioni di similarità della forma y’ = by, dove b &gt; 0.\n\nEsercizio 5: Applicazione delle Scale a Dati Psicologici\nObiettivo: Applicare i concetti a contesti psicologici reali.\n\nScala a intervalli, perché ha differenze costanti tra i punteggi ma nessuno zero assoluto.\nScala di rapporti, perché il numero di parole ricordate ha uno zero assoluto e consente operazioni di rapporto.\nNominale, perché non vi è un ordine gerarchico tra le categorie “estroverso” e “introverso”.\n\nEsercizio 6: Valutazione della Scala di Misurazione\nObiettivo: Identificare la corretta scala di misurazione per vari fenomeni psicologici.\n\nOrdinale, perché il livello di aggressività segue un ordine, ma le differenze tra i livelli non sono necessariamente uguali.\nNo, perché il numero di attacchi di panico è una variabile discreta e misurabile su scala di rapporti.\nIntervalli, perché il punteggio QI ha distanze costanti tra i valori, ma non ha uno zero assoluto.\n\nEsercizio 7: Costruzione di una Scala Psicologica\nObiettivo: Creare una scala di misurazione per una variabile psicologica.\n\nOrdinale o a intervalli, a seconda della precisione della misurazione della resilienza.\nSi potrebbe assegnare un valore numerico crescente alle categorie di preferenza musicale per ottenere una scala ordinale.\nÈ una scala ordinale, perché la differenza tra livelli non è necessariamente costante.\n\nEsercizio 8: Interpretazione Statistica dei Dati\nObiettivo: Collegare il livello di misurazione alle tecniche statistiche appropriate.\n\nPerché la mediana è meno sensibile ai valori estremi rispetto alla media.\nUn test chi-quadrato è adatto per confrontare frequenze di dati nominali tra gruppi.\nSi possono calcolare media, deviazione standard e utilizzare test parametrici come t-test o ANOVA.\n\nEsercizio 9: Misurazione e Inferenze Psicologiche\nObiettivo: Riflettere su come il livello di misurazione influisce sulle conclusioni di una ricerca.\n\nI punteggi Likert sono ordinali, quindi confronti tra differenze di punteggio devono essere interpretati con cautela.\nIntervalli, perché non ha uno zero assoluto, il che limita l’uso di operazioni moltiplicative.\nOrdinale, e si possono usare test non parametrici come il test di Kruskal-Wallis o il test di Mann-Whitney.\n\nEsercizio 10: Esperimenti Psicologici e Misurazione\nObiettivo: Applicare la teoria della misurazione nella progettazione di esperimenti psicologici.\n\nRapporti, perché il numero di parole ricordate è una variabile discreta con uno zero assoluto.\nSe si usa una scala ordinale, bisogna essere cauti nell’uso della media e della deviazione standard.\nScala ordinale → test non parametrici (Mann-Whitney); scala di rapporti → test parametrici (t-test, ANOVA).\n\n\n\n\n\n\n\n\n\n\nProblemi 2\n\n\n\n\n\nEsercizio 1 – Teoria Sostanziale e “Junk Science”\nObiettivo: Riconoscere il ruolo di una teoria sostanziale solida e comprendere come la sua assenza possa compromettere uno studio.\n\nLeggi la sezione in cui Gelman critica l’assenza di una teoria solida nello studio sulle pratiche mente-corpo.\n\nSpiega, in massimo 10 righe, perché secondo Gelman la mancanza di una teoria coerente rende i risultati del suddetto studio “poco significativi” o addirittura “junk science”.\n\nProponi un esempio ipotetico (non correlato al mind-body healing) di uno studio psicologico che, pur presentando dati numerosi e analizzati con metodi statistici sofisticati, risulti privo di una teoria solida. Descrivi sinteticamente perché questo potrebbe rientrare nel concetto di “junk science”.\n\nEsercizio 2 – Problemi di Misurazione\nObiettivo: Identificare le criticità più comuni nella misurazione dei fenomeni psicologici.\n\nElenca almeno tre possibili fattori confondenti che potrebbero influenzare la misurazione dell’efficacia di un intervento psicologico (ad esempio, l’effetto placebo, le aspettative dei partecipanti, ecc.).\n\nSpiega come questi fattori confondenti potrebbero compromettere la validità interna dello studio.\n\nIndica almeno due caratteristiche fondamentali che una buona scala di misurazione (per una variabile psicologica) dovrebbe possedere per essere ritenuta affidabile e valida.\n\nEsercizio 3 – Precisione e Bias\nObiettivo: Chiarire la distinzione tra precisione e distorsione (bias) e come questi aspetti si riflettano nella validità delle conclusioni.\n\nDefinisci, con parole tue, i concetti di precisione e bias in ambito psicometrico.\nFornisci un esempio concreto di uno strumento di misura preciso ma distorto (bias elevato) e di uno strumento poco preciso ma non distorto (bias basso).\n\nSpiega come la combinazione di scarsa precisione e alto bias possa influire sulla possibilità di trarre conclusioni affidabili in uno studio psicologico.\n\nEsercizio 4 – Validità Interna ed Esterna\nObiettivo: Approfondire come le scelte di misurazione influiscano sulla validità interna ed esterna di uno studio.\n\nIn riferimento allo studio sul mind-body healing discusso nel capitolo, identifica due fattori che potrebbero compromettere la validità interna e due fattori che potrebbero limitarne la validità esterna.\n\nDescrivi in 5-8 righe le differenze principali tra validità interna e validità esterna, utilizzando esempi presi sia dal contesto della guarigione mente-corpo sia da altri contesti psicologici (ad esempio, studi sull’apprendimento o sulla motivazione).\n\nProponi una modifica al disegno di ricerca (ipotetico) che potrebbe migliorare la validità interna dello studio originale. Spiega brevemente come questa modifica ne influenzerebbe anche la validità esterna.\n\nEsercizio 5 – Integrare Teoria e Misurazione: Breve Progetto di Ricerca\nObiettivo: Mettere in pratica i concetti di teoria e misurazione attraverso la progettazione di uno studio.\n\nImmagina di voler condurre uno studio su un intervento di “training di rilassamento mentale” finalizzato a ridurre l’ansia negli studenti universitari.\n\nSviluppa una breve traccia di progetto (massimo 15 righe) rispondendo ai seguenti punti:\n\nTeoria di base: Qual è la teoria sostanziale dietro l’efficacia del training di rilassamento? Quali meccanismi psicologici verrebbero attivati?\n\nIpotesi: Quale effetto prevedi sull’ansia degli studenti?\n\nMisurazione: Che tipo di strumento useresti per valutare il livello di ansia e perché (ad esempio, questionari self-report validati, misure fisiologiche come battito cardiaco, ecc.)?\n\nControllo dei confondenti: Quali variabili secondarie possono influire sui risultati e come intendi gestirle?\n\nValidità: Come assicureresti una buona validità interna? Che strategie adotteresti per aumentare la validità esterna?\n\nSpiega brevemente in che modo la combinazione di un solido quadro teorico e di una misurazione accurata permette di evitare che lo studio venga etichettato come “junk science”.\n\n\n\n\n\n\n\n\n\n\nSoluzioni 2\n\n\n\n\n\nEsercizio 1 – Teoria Sostanziale e “Junk Science”\n\nPerché la mancanza di una teoria solida rende i risultati poco significativi?\n\n\nGelman critica lo studio sul mind-body healing perché non vi è un modello teorico convincente che spieghi il meccanismo causale tra pratiche mente-corpo e miglioramenti di salute.\n\nSenza un quadro teorico robusto, i risultati sono interpretati in modo esplorativo e rischiano di essere attribuiti a variabili non controllate (effetto placebo, regressione alla media, ecc.).\n\nUna teoria ben formulata aiuta a delimitare le ipotesi, guidare il disegno di ricerca e interpretare correttamente i dati. In assenza di ciò, i numeri raccolti potrebbero essere viziati da fattori confondenti o da semplici correlazioni spurious.\n\n\n“Junk science” in massimo 10 righe\n\n\nEsempio di testo in 10 righe (circa)\n**Lo studio sul mind-body healing viene talvolta definito “junk science” da Gelman perché, in mancanza di una teoria sostanziale solida, i dati raccolti non forniscono indicazioni chiare sui processi psicologici o fisiologici coinvolti. Una ricerca classificata come “junk science” è priva di rigore metodologico o teorico, e può presentare gravi problemi di replicabilità o di interpretazione dei risultati. In particolare, se non vi è un modello plausibile che colleghi in modo coerente la pratica mente-corpo ai cambiamenti in variabili biologiche e comportamentali, i risultati empirici rischiano di essere semplici coincidenze. L’assenza di un costrutto ben definito e di ipotesi derivanti da una teoria coerente rende difficile capire se i cambiamenti osservati siano reali, casuali o dovuti ad altre cause non considerate (per esempio, l’effetto placebo). Infine, senza un’adeguata cornice teorica, gli studiosi non sanno come interpretare o generalizzare i dati, e la scienza non progredisce realmente.*\n\n\nEsempio di uno studio privo di teoria solida (ipotesi di “junk science”)\n\n\nSituazione ipotetica: Uno studio che raccoglie decine di variabili sulla personalità e sul benessere, poi usa tecniche statistiche sofisticate (analisi di big data, reti neurali, ecc.) per trovare correlazioni fra i tratti di personalità e centinaia di indicatori fisici.\nPerché “junk science”: Se lo studio non definisce a priori quali ipotesi testare e non ha una teoria chiara che spieghi perché certe caratteristiche di personalità dovrebbero correlarsi con determinati parametri fisici, i risultati trovati potrebbero essere frutto di coincidenze casuali. Inoltre, in assenza di un modello teorico solido, anche risultati statisticamente significativi possono essere privi di significato dal punto di vista psicologico.\n\nEsercizio 2 – Problemi di Misurazione\n\nTre possibili fattori confondenti nell’efficacia di un intervento psicologico\n\n\nEffetto placebo: I partecipanti migliorano perché si aspettano di migliorare, non per l’effettiva efficacia dell’intervento.\n\nAspettative dei partecipanti: Se sanno di partecipare a uno studio, potrebbero modificare il proprio comportamento (effetto Hawthorne).\n\nDesiderabilità sociale: I partecipanti forniscono risposte che ritengono socialmente desiderabili, falsando i risultati (ad esempio, sottostimando i livelli di ansia o stress).\n\n\nCome questi fattori confondenti compromettono la validità interna\n\n\nLa validità interna riguarda il grado in cui è possibile concludere che sia effettivamente la variabile indipendente (l’intervento) a causare le modifiche osservate nella variabile dipendente (es. livelli di ansia).\n\nSe subentrano l’effetto placebo, aspettative non controllate o tendenze alla desiderabilità sociale, diventa difficile stabilire un nesso causale chiaro. Esiste sempre il dubbio che altri processi cognitivi o sociali (non l’intervento in sé) abbiano prodotto il risultato.\n\n\nDue caratteristiche fondamentali di una buona scala di misurazione\n\n\nAffidabilità: Capacità dello strumento di fornire misure stabili e coerenti nel tempo (ad esempio, coerenza interna, stabilità test-retest).\n\nValidità: Capacità dello strumento di misurare effettivamente ciò che si propone di misurare (validità di contenuto, di costrutto, di criterio).\n\nEsercizio 3 – Precisione e Bias\n\nDefinizioni di precisione e bias\n\n\nPrecisione: Indica il grado di dispersione (o variabilità) delle misurazioni. Uno strumento preciso produce misure molto simili fra loro se ripetute nelle stesse condizioni (bassa varianza).\n\nBias (distorsione): Indica l’errore sistematico, ossia la tendenza a sovra- o sottostimare sistematicamente il fenomeno in esame. Uno strumento può essere molto coerente nelle misure, ma se è “tarato” male, darà sempre un risultato distorto.\n\n\nEsempio concreto di misura “precisa ma distorta” e “poco precisa ma non distorta”\n\n\nPrecisa ma distorta: Un cronometro che, a causa di un difetto di fabbricazione, parte sempre con 2 secondi di ritardo ma poi misura i tempi con estrema coerenza. Risultato: tutte le misure saranno molto simili (alta precisione), ma sempre sfasate di 2 secondi (alto bias).\n\nPoco precisa ma non distorta: Un termometro vecchio che a volte segna 36,2°C, altre 36,7°C, altre 37,1°C, senza un pattern sistematico. In media potrebbe risultare vicino ai 36,5°C, quindi senza un bias chiaro, ma con un’alta variabilità tra una misurazione e l’altra (bassa precisione).\n\n\nConseguenze di scarsa precisione e alto bias\n\n\nSe uno strumento è poco preciso (alta variabilità) e altamente distorto (bias elevato), i risultati ottenuti non solo oscillano in modo imprevedibile, ma sono costantemente lontani dal valore “vero”.\n\nIn queste condizioni, le conclusioni diventano inaffidabili, poiché è quasi impossibile distinguere l’effetto reale (casuale o causale) dalle deformazioni introdotte dallo strumento e dall’errore di misura.\n\nEsercizio 4 – Validità Interna ed Esterna\n\nDue fattori che compromettono la validità interna e due fattori che compromettono la validità esterna (nell’esempio del mind-body healing)\n\n\nValidità interna:\n\nAssegnazione non casuale ai gruppi: se i partecipanti scelgono autonomamente di aderire alle pratiche mente-corpo, potrebbero essere più motivati o avere caratteristiche iniziali diverse.\n\nMancata o inadeguata gestione dell’effetto placebo: non sapere se l’intervento “mente-corpo” sia stato percepito come particolarmente “speciale” dai partecipanti può introdurre differenze di aspettativa.\n\nValidità esterna:\n\nCampione non rappresentativo: se lo studio è condotto solo su persone che frequentano un determinato tipo di centro di benessere, i risultati potrebbero non essere generalizzabili all’intera popolazione.\n\nContesto specifico: pratiche mente-corpo svolte in un ambiente estremamente controllato (es. un laboratorio o un ritiro speciale) potrebbero non replicarsi nella vita quotidiana di chiunque.\n\n\n\nDifferenze tra validità interna ed esterna (5-8 righe di esempio)\n\n\nLa validità interna si riferisce alla correttezza del disegno di ricerca nel dimostrare un effetto causale. Un alto livello di validità interna implica che i ricercatori siano ragionevolmente sicuri che l’intervento (ad esempio, una tecnica mente-corpo) abbia causato i risultati osservati (miglioramento della salute). La validità esterna, invece, riguarda la possibilità di generalizzare i risultati a contesti, persone e tempi differenti. Se un intervento è stato testato in condizioni molto specifiche, potrebbe funzionare bene solo in quel contesto e con quel particolare campione. Per esempio, un intervento sul mind-body healing con individui altamente motivati potrebbe non dare gli stessi risultati in una popolazione generalizzata. Allo stesso modo, uno studio sull’apprendimento condotto in un laboratorio altamente controllato potrebbe non riflettere le reali dinamiche di un’aula scolastica.\n\n\nModifica al disegno di ricerca per migliorare la validità interna e conseguenze sulla validità esterna\n\n\nProposta: Introdurre un gruppo di controllo con un intervento placebo o un’attività simile ma priva di contenuto “mente-corpo” (ad es. sessioni di lettura rilassante). In questo modo, si può confrontare l’effetto “specífico” dell’intervento.\nCome influenza la validità interna: Con un gruppo di controllo placebo, diventa più semplice escludere che il miglioramento sia dovuto solo alle aspettative dei partecipanti. Questo riduce il rischio di confondenti e aumenta la validità interna.\nCome influenza la validità esterna: Potrebbe rendere il contesto dello studio più artificiale (un gruppo fa “meditazione”, l’altro legge in silenzio), il che potrebbe ridurre la naturalezza della situazione e potenzialmente limitare la generalizzabilità ad ambienti reali (validità esterna).\n\nEsercizio 5 – Integrare Teoria e Misurazione: Breve Progetto di Ricerca\n\nBreve traccia di progetto: “Training di rilassamento mentale per ridurre l’ansia negli studenti universitari”\n\n\nTeoria di base\nIl training di rilassamento mentale si fonda sul presupposto teorico che le tecniche di riduzione dello stress (es. respirazione consapevole, rilassamento muscolare progressivo) possano agire sui livelli di attivazione fisiologica e sui pensieri intrusivi. Riducendo l’iperattivazione del sistema nervoso simpatico e favorendo uno stato di calma, diminuisce l’ansia percepita.\nIpotesi\nGli studenti che seguono il training di rilassamento per 4 settimane mostreranno una riduzione significativa nei punteggi di ansia, rispetto a un gruppo di controllo che non partecipa al training.\nMisurazione\nUtilizzo di una scala validata come lo STAI (State-Trait Anxiety Inventory) per misurare il livello di ansia pre e post intervento. Possibile integrazione con misure fisiologiche (battito cardiaco a riposo) per avere dati oggettivi.\nControllo dei confondenti\n\nRegistrare la storia clinica dei partecipanti (per escludere coloro che assumono farmaci ansiolitici).\n\nRichiedere che i partecipanti non modifichino drasticamente le proprie abitudini di studio o di vita durante l’intervento.\n\nAssicurarsi che i valutatori non sappiano chi fa parte del gruppo di training o del gruppo di controllo (blinding parziale).\n\nValidità\n\nValidità interna: Uso di un gruppo di controllo e assegnazione casuale (randomizzazione) per assicurare che i due gruppi siano comparabili.\n\nValidità esterna: Inclusione di studenti provenienti da diverse facoltà, così da riflettere una maggiore eterogeneità di popolazione.\n\n\n\nCome teoria solida e misurazione accurata evitano la “junk science”\n\n\nUna solida cornice teorica spiega i meccanismi psicologici e fisiologici che legano l’intervento (training di rilassamento) all’esito (riduzione dell’ansia).\n\nUna misurazione accurata e validata (STAI, misure fisiologiche) riduce errori e distorsioni. Se le misure sono ripetute nel tempo (pre e post), si possono confrontare i cambiamenti effettivi.\n\nIntegrando teoria e misurazione, i risultati assumono un significato scientifico più robusto. Non basta osservare un miglioramento: occorre dimostrare come e perché tale miglioramento avvenga, evitando di cadere in semplici correlazioni prive di spiegazione (e quindi potenzialmente “junk science”).\n\n\n\n\n\n\n\n\n\n\nProblemi 3\n\n\n\n\n\nEsercizio 1 – Trasformazioni in Scala Nominale\nSituazione\nUn ricercatore vuole indagare la percezione di appartenenza sociale tra studenti universitari di Psicologia. A ciascuno studente viene chiesto di rispondere alla domanda: “Qual è il gruppo studentesco a cui ritieni di appartenere maggiormente?”, scegliendo una tra le seguenti categorie:\n\n\nGruppo A (focalizzato su ricerca e studio)\n\n\n\nGruppo B (focalizzato su attività ricreative)\n\n\n\nGruppo C (focalizzato su volontariato e progetti sociali)\n\n\nIstruzioni\n\nIdentifica la scala di misurazione utilizzata per classificare gli studenti (nominale, ordinale, a intervalli o di rapporti).\n\nIndica quali trasformazioni sono ammissibili su questa scala e spiega perché non è possibile applicare operazioni di tipo aritmetico (somme, differenze, etc.).\n\nProponi un esempio di nuova scala nominale equivalente, ossia una nuova denominazione delle categorie che rispetti la suddivisione originale. (Esempio: rinominarle in Gruppo X, Gruppo Y, Gruppo Z, oppure usare colori, animali-simbolo, ecc.). Spiega perché questa trasformazione non altera i risultati dell’indagine.\n\nEsercizio 2 – Trasformazioni in Scala Ordinale\nSituazione\nIn un questionario sul benessere psicologico, agli studenti viene chiesto di classificare il loro stato di motivazione allo studio su una scala da 1 (bassa motivazione) a 5 (alta motivazione). Si ottiene così un dato ordinalmente misurato.\nIstruzioni\n\nSpiega perché tale variabile (“livello di motivazione”) rappresenta una scala ordinale. Quali proprietà la rendono diversa da una semplice scala nominale?\n\nDescrivi in che modo è possibile ridenominare i valori della scala (ad esempio, da [1,2,3,4,5] a [“Molto bassa”, “Bassa”, “Media”, “Alta”, “Molto alta”]) senza alterare il rapporto d’ordine tra le categorie.\n\nProponi un esempio di trasformazione non ammissibile: qual è un’operazione aritmetica che non avrebbe senso applicare su una scala ordinale e perché (ad esempio, calcolare “il doppio di motivazione”)?\n\nEsercizio 3 – Trasformazioni in Scala ad Intervalli\nSituazione\nUn gruppo di ricercatori in Psicometria vuole confrontare i punteggi di un test d’intelligenza (misurati secondo la scala tradizionale del QI, con media 100 e deviazione standard 15) con un nuovo test sperimentale. Come ben noto, la scala del QI è considerata, nelle sue approssimazioni psicometriche, una scala ad intervalli.\nIstruzioni\n\nSpiega in cosa consiste la trasformazione lineare ammessa (del tipo \\(y' = a + b y\\), con \\(b &gt; 0\\)) e perché tale trasformazione preserva le differenze tra i punteggi.\n\nFai un esempio concreto di trasformazione lineare: supponi di voler “riscalare” i punteggi del QI in modo che la nuova media sia 50. Definisci i valori di \\(a\\) e \\(b\\) (indicando un’ipotesi di calcolo) e mostra come viene modificato il punteggio di un individuo con QI = 115.\n\nDiscuta perché, nonostante la somiglianza con le scale ordinale e nominale (puoi comunque distinguere punteggi e ordinarli), una scala ad intervalli consente operazioni matematiche più complesse (ad esempio, differenze) che non sarebbero valide negli altri due livelli.\n\nEsercizio 4 – Trasformazioni in Scala di Rapporti\nSituazione\nUn laboratorio di psicofisiologia misura i tempi di reazione (in millisecondi) a uno stimolo luminoso. Poiché il tempo di reazione pari a 0 ms significa realmente assenza di risposta (ovvero, impossibile da misurare in pratica, ma concettualmente corrisponde a intensità nulla del fenomeno “tempo di reazione”), ci troviamo in una scala di rapporti.\nIstruzioni\n\nSpiega perché il tempo di reazione soddisfa i requisiti di una scala di rapporti, inclusa la presenza di uno zero assoluto e la possibilità di confrontare i punteggi con rapporti (ad esempio, “il tempo di reazione del partecipante A è il doppio di quello del partecipante B”).\n\nQuali sono le trasformazioni ammissibili su una scala di rapporti? Fornisci un esempio numerico (per esempio, se moltiplichi tutti i tempi di reazione per 2, che cosa accade al rapporto tra i punteggi di due partecipanti?).\n\nDescrivi il motivo per cui è possibile dire che A ha una latenza doppia di B usando i millisecondi, ma non è sempre possibile fare asserzioni analoghe usando scale ad intervalli. Fai un parallelo, ad esempio, con le temperature in Celsius.\n\nEsercizio 5 – Riconoscere e Applicare le Trasformazioni nei Quattro Livelli di Scala\nSituazione\nUn docente di Psicologia sperimentale ha raccolto quattro serie di dati su vari aspetti:\n\nOrientamento politico (liberale, conservatore, centrista, ecc.).\n\nClassifica di soddisfazione sul tirocinio (1° posto, 2° posto, 3° posto, etc.).\n\nPunteggi di un test di personalità su un fattore (con media = 100, deviazione standard = 10) trattato come scala ad intervalli.\n\nFrequenza cardiaca a riposo misurata in battiti al minuto (bpm).\n\nIstruzioni\n\nIdentifica per ciascuno dei quattro insiemi di dati il livello di scala (nominale, ordinale, intervalli, rapporti).\n\nPer ognuno dei quattro livelli di scala elenca almeno una trasformazione ammessa (ad es. ridenominazione delle categorie per la nominale, traslazione e dilatazione per l’intervalli, ecc.) e una non ammessa (esempio: non puoi sommare categorie nominali, non puoi calcolare la radice quadrata di un rango ordinale dandogli significato, ecc.).\n\nRifletti in breve (2-3 righe) su come queste differenze nelle trasformazioni ammissibili incidano sull’interpretazione dei dati e sulle analisi statistiche che il docente potrà validamente utilizzare (ad esempio, test non parametrici per variabili ordinarie, test parametrici per scale ad intervalli/rapporti).\n\n\n\n\n\n\n\n\n\n\nSoluzioni 3\n\n\n\n\n\nEsercizio 1 – Trasformazioni in Scala Nominale\n\nIdentificazione della scala La classificazione degli studenti in “Gruppo A/B/C” è scala nominale. Non esiste alcun ordine intrinseco tra le categorie; si tratta semplicemente di etichette qualitative.\nTrasformazioni ammissibili\n\n\nTrasformazioni ammissibili: ridenominare o rinominare le categorie senza modificare la partizione del campione (esempio: A → “Studio”, B → “Ricreazione”, C → “Volontariato”).\n\nL’unica operazione aritmetica consentita è il conteggio delle frequenze nelle varie categorie.\n\n\nOperazioni non consentite: non è possibile sommare o sottrarre etichette, né confrontare categorie in termini di “più/meno grande” o “rapporto”.\n\n\nEsempio di nuova scala nominale equivalente\n\n\nPotresti chiamare i gruppi: “Alpha, Beta, Gamma” (oppure con colori: “Rosso, Blu, Verde”).\n\nQuesta trasformazione non altera la classificazione in sé: tutti gli studenti del Gruppo A rimangono nel “nuovo” gruppo Alpha, e così via.\n\nNon cambia la struttura dei dati e di conseguenza non altera i risultati della ricerca (restano invariate le frequenze e la suddivisione nelle categorie).\n\nEsercizio 2 – Trasformazioni in Scala Ordinale\n\nPerché è una scala ordinale? La variabile “livello di motivazione” da 1 (bassa) a 5 (alta) indica:\n\n\nClassificazione in categorie (come in una scala nominale).\n\nRelazione d’ordine chiara (1 &lt; 2 &lt; 3 &lt; 4 &lt; 5).\n\nNon fornisce alcuna informazione sulle distanze reali tra i punti (non è detto che la differenza tra 1 e 2 sia uguale a quella tra 3 e 4).\nÈ quindi una scala ordinale e non semplicemente nominale.\n\n\nRidenominazione dei valori mantenendo l’ordine\n\n\nPuoi sostituire i numeri con etichette testuali rispettando lo stesso ordine:\n1 → “Molto bassa”\n2 → “Bassa”\n3 → “Media”\n4 → “Alta”\n5 → “Molto alta”\n\nL’ordine rimane lo stesso: “Molto bassa” &lt; “Bassa” &lt; … &lt; “Molto alta”.\n\n\nEsempio di trasformazione non ammissibile\n\n\nCalcolare “il doppio di motivazione”: dire che la categoria 4 è “il doppio” della categoria 2 non ha senso, perché non c’è un’unità di misura fissa che quantifichi la differenza tra i livelli. Le categorie ordinali servono solo a ordinare, non a quantificare in modo assoluto.\n\nEsercizio 3 – Trasformazioni in Scala ad Intervalli\n\nTrasformazione lineare ammessa\n\n\nForma generale: \\(y' = a + b y\\), con \\(b &gt; 0\\).\n\nPreserva le differenze tra i valori (ad esempio, \\((y_2 - y_1) = (y'_2 - y'_1) / b\\)), perché la traslazione aggiunge una costante a tutti i punteggi e la dilatazione (moltiplicazione per \\(b\\)) mantiene le proporzioni fra gli intervalli.\n\n\nEsempio concreto\n\n\nScala QI: media = 100, deviazione standard = 15.\n\nVuoi che la nuova media sia 50.\n\nPer semplificare, supponiamo di voler “spostare” ogni valore verso una nuova scala centrata a 50, mantenendo una deviazione standard proporzionale.\n\nUna possibile trasformazione lineare:\n\\[\n  y' = (y - 100) + 50 = y - 50.\n\\]\nIn questo caso, \\(a = -50\\), \\(b = 1\\).\n\nSe un individuo ha QI = 115, allora \\(y' = 115 - 50 = 65\\).\n\n\nSe invece volessi anche cambiare la deviazione standard, potresti usare un fattore \\(b \\neq 1\\). Ad esempio, se desideri una deviazione standard = 10, potresti usare \\(b = \\frac{10}{15} \\approx 0.67\\).\n\n\nDifferenze rispetto alle scale nominali/ordinali\n\n\nCon una scala ad intervalli puoi:\n\nOrdinare i punteggi.\n\nStabilire differenze (es. un individuo A ha 15 punti in più di B).\n\n\nNon puoi invece stabilire rapporti (es. “A ha il doppio di X rispetto a B” non è lecito), perché lo zero è arbitrario e la distanza “0” non rappresenta l’assenza del fenomeno (come invece avviene nella scala di rapporti).\n\nEsercizio 4 – Trasformazioni in Scala di Rapporti\n\nPerché il tempo di reazione è in una scala di rapporti?\n\n\nZero assoluto: un tempo di reazione (teoricamente) pari a 0 ms significherebbe nessun tempo trascorso → totale assenza del fenomeno misurato (impossibile nella pratica, ma concettualmente definisce uno zero non arbitrario).\n\nPuoi confrontare i punteggi con rapporti: “il tempo di reazione di A è il doppio di quello di B” (200 ms vs. 100 ms).\n\n\nTrasformazioni ammissibili\n\n\nTrasformazione di similarità: \\(y' = b y\\) con \\(b &gt; 0\\).\n\nSe hai due tempi di reazione \\(y_1\\) e \\(y_2\\), il rapporto \\(\\frac{y_1}{y_2}\\) rimane invariato anche dopo la trasformazione:\n\\[\n  \\frac{y'_1}{y'_2} = \\frac{b y_1}{b y_2} = \\frac{y_1}{y_2}.\n\\]\nEsempio numerico: se i tempi di reazione di due partecipanti sono 100 ms e 200 ms, il rapporto è 2. Se moltiplichi entrambi per 2, ottieni 200 ms e 400 ms, e il rapporto rimane 2.\n\n\nConfronto con scala ad intervalli (esempio delle temperature)\n\n\nIn una scala di rapporti puoi dire “A ha una latenza doppia di B” perché lo zero non è arbitrario.\n\nCon la temperatura (scala ad intervalli) lo zero (es. 0°C) non rappresenta l’assenza di calore, quindi non ha senso dire che 80°C è “il doppio” di 40°C. Cambiando la scala (ad es. Fahrenheit) il rapporto cambia.\n\nEsercizio 5 – Riconoscere e Applicare le Trasformazioni nei Quattro Livelli di Scala\n\nIdentificazione del livello di scala\n\n\nOrientamento politico: scala nominale (categorie qualitative prive di ordine).\n\nClassifica di soddisfazione (1°, 2°, 3°, …): scala ordinale (c’è un ordine, ma non si conosce la “distanza” fra i posti).\n\nPunteggi di un test di personalità (con media=100, dev.st=10), considerati approssimazione di una scala ad intervalli (si assumono le differenze significative, lo zero è arbitrario).\n\nFrequenza cardiaca a riposo (bpm): scala di rapporti (zero assoluto e rapporti confrontabili).\n\n\nTrasformazioni ammesse e non ammesse\n\n\nNominale:\n\nAmmessa: cambiare etichette (A → “Liberale”, B → “Conservatore” ecc.).\n\nNon ammessa: sommare categorie, ordinare, calcolare media delle categorie.\n\n\nOrdinale:\n\nAmmessa: rietichettare i ranghi (1° → “Migliore”, 2° → “Secondo posto”…).\n\nNon ammessa: calcolare rapporti (il 2° posto non è “il doppio” del 1°), sommare posizioni in modo significativo.\n\n\nA intervalli:\n\nAmmessa: trasformazione lineare (traslazione + dilatazione).\n\nNon ammessa: dire che un punteggio è “tre volte” un altro; lo zero è arbitrario.\n\n\nA rapporti:\n\nAmmessa: trasformazione di similarità (\\(y' = b y\\)), in cui i rapporti rimangono invariati.\n\nNon ammessa: aggiunta di una costante a tutti i valori (questa sposterebbe lo zero, rendendolo arbitrario e trasformando la scala in una scala ad intervalli).\n\n\n\nImplicazioni per l’interpretazione e le analisi\n\n\nUna variabile nominale consente solo frequenze e test non parametrici basati su conteggi (es. Chi-quadrato).\n\nUna variabile ordinale permette test di ordinamento (es. test di rank, come il Wilcoxon), ma non calcoli di media con significato forte.\n\nUna scala ad intervalli permette di usare statistiche parametriche (calcolo di media, varianza, test come t-test, ANOVA), assumendo che l’interpretazione delle differenze sia coerente.\n\nUna scala di rapporti permette, in più, il confronto di rapporti (ad esempio, si possono applicare modelli parametrici che includano il concetto di proporzioni o slope logico su dati che abbiano senso a zero assoluto).",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>La misurazione in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/04_measurement.html#bibliografia",
    "href": "chapters/key_notions/04_measurement.html#bibliografia",
    "title": "5  La misurazione in psicologia",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nAungle, P., & Langer, E. (2023). Physical healing as a function of perceived time. Scientific Reports, 13(1), 22432.\n\n\nDomini, F., & Caudek, C. (2009). The intrinsic constraint model and Fechnerian sensory scaling. Journal of Vision, 9(2), 25–25.\n\n\nLilienfeld, S. O., & Strother, A. N. (2020). Psychological measurement and the replication crisis: Four sacred cows. Canadian Psychology/Psychologie Canadienne, 61(4), 281–288.\n\n\nMaul, A., Irribarra, D. T., & Wilson, M. (2016). On the philosophical foundations of psychological measurement. Measurement, 79, 311–320.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>La misurazione in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/05_cognitive_models.html",
    "href": "chapters/key_notions/05_cognitive_models.html",
    "title": "6  Dalla descrizione alla spiegazione",
    "section": "",
    "text": "6.1 Introduzione\nPer comprendere e spiegare i processi mentali in modo più rigoroso, è necessario adottare modelli che vadano oltre la semplice descrizione. Questo capitolo introduce i modelli meccanicistici e computazionali, mostrando come possano rafforzare la spiegazione psicologica. Negli ultimi anni, la psicologia ha attraversato una crisi profonda legata alla riproducibilità dei risultati sperimentali. Molti effetti classici non riescono a replicarsi in studi successivi, sollevando interrogativi sulla solidità delle teorie psicologiche. In questo contesto, è sempre più chiaro che il tipo di modelli utilizzati per spiegare i fenomeni psicologici ha un impatto cruciale sulla credibilità e robustezza della ricerca scientifica. Una distinzione centrale a questo riguardo è quella tra modelli fenomenologici e modelli meccanicistici.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Dalla descrizione alla spiegazione</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/05_cognitive_models.html#introduzione",
    "href": "chapters/key_notions/05_cognitive_models.html#introduzione",
    "title": "6  Dalla descrizione alla spiegazione",
    "section": "",
    "text": "Panoramica del capitolo\n\nIl ruolo dei modelli computazionali nella psicologia scientifica.\nLa struttura e la logica di due modelli fondamentali — il modello di Rescorla-Wagner (per l’apprendimento associativo) e il Drift Diffusion Model (per le decisioni sotto incertezza).\n\n\n\n\n\n\n\nPrerequisiti\n\n\n\n\n\n\nConsulta Why is the Rescorla-Wagner model so influential? (Soto et al., 2023).\n\n\n\n\n\n\n\n\n\n\nPreparazione del Notebook\n\n\n\n\n\n\nhere::here(\"code\", \"_common.R\") |&gt; \n  source()\n\n# Load packages\nif (!requireNamespace(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(rtdists)\n\n\n\n\n\n6.1.1 Modelli fenomenologici: descrivere senza spiegare\nI modelli fenomenologici si limitano a descrivere relazioni osservabili tra variabili psicologiche, spesso attraverso formule matematiche o rappresentazioni statistiche. Un esempio classico è una legge psicofisica che descrive la relazione tra stimolazione sensoriale e risposta percepita. Sebbene questi modelli possano essere estremamente predittivi, non forniscono informazioni sul “come” e “perché” un certo fenomeno si verifica. Non specificano, cioè, le entità e le attività organizzate che lo generano (es. meccanismi cognitivi, neuroni, moduli funzionali).\nCome sottolineato da Povich (2025), modelli fenomenologici come questi possono essere accurati, compatti, persino predittivi — ma non necessariamente esplicativi. In effetti, possono mancare della capacità di rispondere a domande controfattuali del tipo “che cosa succederebbe se…?” e non permettono un controllo diretto sul fenomeno. Questa limitazione si rivela particolarmente problematica in un’epoca in cui la replicabilità richiede non solo constatare un effetto, ma anche comprenderne le condizioni causali e contestuali.\n\n6.1.2 Dai modelli meccanicistici alla modellazione computazionale\nUn modello meccanicistico cerca di rappresentare le componenti causali di un fenomeno. Secondo una definizione ampiamente condivisa, un meccanismo è “una collezione organizzata di entità e attività che produce o mantiene un certo fenomeno” (Bechtel, 2009). I modelli meccanicistici hanno l’obiettivo di descrivere questi meccanismi, specificando in che modo le componenti interagiscono per generare il comportamento osservato.\nNel contesto della psicologia, i modelli meccanicistici vanno oltre la descrizione di correlazioni osservabili. Cercano di identificare strutture cognitive, processi neurali o dinamiche corpo-ambiente o interazioni tra livelli (funzionale, computazionale, implementazionale). Un esempio ben noto è il modello della long-term potentiation (LTP) nella memoria, che spiega come variazioni nei recettori NMDA e AMPA e nella concentrazione di ioni calcio e magnesio determinano il rafforzamento sinaptico — un chiaro caso di spiegazione meccanicistica.\nOggi, molti modelli meccanicistici in psicologia sono implementati come modelli computazionali, ovvero rappresentazioni formali che simulano i processi interni ipotizzati. Attraverso la simulazione e la stima dei parametri, questi modelli permettono di inferire il funzionamento dei meccanismi sottostanti a partire dal comportamento osservabile. I modelli computazionali soddisfano i criteri della spiegazione meccanicistica quando forniscono informazioni su entità ipotetiche (come credenze, soglie decisionali, accumulo di evidenza) e sulle loro interazioni causali.\n\n6.1.3 La differenza epistemica: come distinguere spiegazione da predizione\nUn punto chiave nella distinzione tra spiegazioni fenomenologiche e meccanicistiche è che solo le seconde soddisfano i criteri di potere esplicativo propriamente detto. Come chiarisce Povich (2025), un modello esplicativo deve permettere di:\n\nrispondere a domande controfattuali (“che cosa succederebbe se una componente fosse diversa?”);\nfornire la base per manipolare o controllare il fenomeno.\n\nQuesti criteri sono cruciali per superare la crisi della replicabilità: sapere che un effetto si verifica in certe condizioni è poco utile se non si capisce perché avviene e quali sono i meccanismi sottostanti che lo rendono stabile o instabile rispetto a cambiamenti contestuali.\n\n6.1.4 Oltre le metafore meccaniche: che cosa rende meccanicistico un modello?\nUna fonte comune di confusione riguarda l’idea che un modello, per essere meccanicistico, debba avere necessariamente la forma di una macchina, con entità concrete (es. neuroni, aree cerebrali) e connessioni visibili tra di esse. Ma questa è una semplificazione fuorviante. Ciò che rende un modello meccanicistico non è la sua forma visiva o metaforica, ma la sua capacità di rappresentare l’organizzazione causale del processo che genera un certo comportamento. Un modello può essere espresso con equazioni matematiche, algoritmi, reti neurali, simulazioni, eppure contribuire in modo decisivo a una spiegazione meccanicistica se specifica in che modo le componenti del sistema interagiscono per produrre l’effetto osservato.\nPer chiarire questa idea, possiamo richiamare i tre livelli di spiegazione proposti da David Marr (1982), uno dei riferimenti fondamentali nella psicologia cognitiva computazionale:\n\n\nLivello computazionale: Cosa fa il sistema e perché (qual è il problema che risolve?).\n\nLivello algoritmico: Come lo fa? Quali rappresentazioni interne e quali trasformazioni (regole di calcolo) sono coinvolte.\n\nLivello implementativo: Con quali mezzi fisici è realizzato (per esempio, circuiti neurali).\n\nQuesta distinzione aiuta a chiarire che un modello può essere meccanicistico anche se non rappresenta direttamente substrati biologici, purché descriva in modo formale come un sistema risolve un problema e quali regole seguono le sue componenti.\nNel contesto della psicologia, i modelli computazionali che operano al livello algoritmico o computazionale — come il modello di Rescorla-Wagner o il Drift Diffusion Model — sono perfettamente coerenti con un approccio meccanicistico, anche se non rappresentano esplicitamente l’implementazione biologica.\nQuesti modelli sono “meccanicistici” nel senso che:\n\n\ndescrivono entità funzionali (es. valore atteso, evidenza accumulata),\n\nspecificano regole di interazione tra queste entità (es. aggiornamento, accumulo, soglie),\n\nproducono il comportamento osservabile come risultato di queste interazioni.\n\nDunque, ciò che conta non è la forma del modello, ma la funzione esplicativa che svolge all’interno della teoria psicologica. Modelli formulati come sistemi dinamici, modelli bayesiani gerarchici, modelli di reti neurali artificiali o modelli simbolici possono tutti contribuire a spiegazioni meccanicistiche, se mostrano come un certo comportamento emerga da un’organizzazione di componenti in interazione.\n\n6.1.5 Perché i modelli meccanicistici rafforzano la riproducibilità\nLa crisi della riproducibilità può essere vista come il sintomo di un’eccessiva fiducia in modelli fenomenologici che mancano di profondità esplicativa. I modelli meccanicistici, al contrario:\n\nesplicitano gli assunti causali e strutturali;\npermettono verifiche controfattuali e manipolazioni;\nchiariscono quando e perché un effetto dovrebbe ripetersi o variare;\nsono meno vulnerabili al cherry picking e agli effetti di contesto non dichiarati;\ni modelli computazionali meccanicistici, come il DDM e il modello di Rescorla-Wagner, consentono di simulare e verificare quantitativamente ipotesi sui meccanismi interni, rendendo più trasparente e replicabile l’inferenza psicologica.\n\nIn breve, un modello meccanicistico non si limita a dire che “A predice B”, ma mostra come A produce B, e in quali condizioni questo accade.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Dalla descrizione alla spiegazione</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/05_cognitive_models.html#riflessioni-conclusive",
    "href": "chapters/key_notions/05_cognitive_models.html#riflessioni-conclusive",
    "title": "6  Dalla descrizione alla spiegazione",
    "section": "Riflessioni conclusive",
    "text": "Riflessioni conclusive\nLa crisi di replicabilità che caratterizza la psicologia contemporanea impone un ripensamento metodologico profondo, orientato verso un superamento dei modelli puramente descrittivi a favore di paradigmi meccanicistici e computazionali. A differenza dei primi, questi ultimi non si limitano a prevedere esiti comportamentali, ma mirano a identificare i processi algoritmici e i meccanismi latenti che li generano, trasformando così domande di ricerca generiche in ipotesi formalizzate e rigorosamente verificabili.\nL’adozione di un approccio computazionale in psicologia cognitiva consente di ovviare ai limiti intrinseci dell’analisi descrittiva, offrendo un quadro matematico solido per valutare ipotesi sui processi mentali. L’integrazione di modelli di apprendimento e modelli decisionali, in particolare, permette di costruire rappresentazioni unificate e più profonde dei sistemi cognitivi che sottendono il comportamento umano, con ricadute significative sia nella ricerca di base che in ambito applicativo e clinico.\nTale prospettiva si colloca organicamente nel quadro emergente della psichiatria computazionale e della modellazione bayesiana della cognizione, il cui scopo ultimo non è meramente descrittivo, ma inferenziale: individuare quali processi interni risultino più plausibili alla luce dei dati osservati. In questo senso, la formalizzazione computazionale non migliora soltanto il potere esplicativo dei modelli, ma potenzia anche la capacità di inferenza e generalizzazione, due componenti fondamentali per lo sviluppo di una scienza psicologica cumulativa, robusta e realmente replicabile.\n\n\n\n\n\n\n\n\n\n\nProblemi\n\n\n\n\n\n\nChe cosa descrive il modello di Rescorla-Wagner?\nQual è il ruolo del parametro α nel modello di Rescorla-Wagner?\nQuale funzione matematica viene utilizzata per modellare il bilanciamento tra esplorazione ed esploitazione nel modello di Rescorla-Wagner?\nQuali sono i principali parametri del Drift Diffusion Model (DDM)?\nIn che modo il DDM spiega il compromesso tra velocità e accuratezza nelle decisioni?\n\n\n\n\n\n\n\n\n\n\nSoluzioni\n\n\n\n\n\n\nIl modello di Rescorla-Wagner descrive come gli individui apprendano le associazioni tra stimoli e risposte in base all’errore di previsione. L’aspettativa di ricompensa viene aggiornata attraverso l’esperienza, con un processo regolato dal tasso di apprendimento (\\(\\alpha\\)).\nIl parametro \\(\\alpha\\) (tasso di apprendimento) determina quanto velocemente un individuo aggiorna le proprie aspettative in base all’errore di previsione. Se \\(\\alpha\\) è alto, l’apprendimento è rapido; se è basso, l’individuo si basa maggiormente sulle esperienze passate.\nLa funzione Softmax viene utilizzata per modellare il bilanciamento tra esplorazione e sfruttamento. Essa regola la probabilità di scegliere un’opzione in base al valore atteso e alla temperatura della scelta (\\(\\beta\\)).\n\nI principali parametri del DDM sono:\n\n\ntasso di drift (\\(v\\)): velocità con cui viene accumulata l’evidenza;\n\nseparazione delle soglie (\\(a\\)): distanza tra le soglie decisionali;\n\n\ntempo di non-decisione (\\(t_0\\)): tempo impiegato per processi indipendenti dall’accumulo dell’evidenza;\n\n\nbias iniziale (\\(z\\)): punto di partenza dell’accumulo dell’evidenza.\n\n\nIl DDM spiega il compromesso tra velocità e accuratezza attraverso la separazione delle soglie decisionali (\\(a\\)). Se le soglie sono più vicine, le decisioni sono più rapide ma meno accurate; se sono più distanti, le decisioni sono più lente ma più precise.\n\n\n\n\n\n\n\n\n\n\nInformazioni sull’ambiente di sviluppo\n\n\n\n\n\n\nsessionInfo()\n#&gt; R version 4.5.1 (2025-06-13)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.6.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] rtdists_0.11-5        pillar_1.11.0         tinytable_0.13.0     \n#&gt;  [4] patchwork_1.3.2       ggdist_3.3.3          tidybayes_3.0.7      \n#&gt;  [7] bayesplot_1.14.0      ggplot2_3.5.2         reliabilitydiag_0.2.1\n#&gt; [10] priorsense_1.1.1      posterior_1.6.1       loo_2.8.0            \n#&gt; [13] rstan_2.32.7          StanHeaders_2.32.10   brms_2.22.0          \n#&gt; [16] Rcpp_1.1.0            sessioninfo_1.2.3     conflicted_1.2.0     \n#&gt; [19] janitor_2.2.1         matrixStats_1.5.0     modelr_0.1.11        \n#&gt; [22] tibble_3.3.0          dplyr_1.1.4           tidyr_1.3.1          \n#&gt; [25] rio_1.2.3             here_1.0.1           \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;  [1] gridExtra_2.3         inline_0.3.21         sandwich_3.1-1       \n#&gt;  [4] rlang_1.1.6           magrittr_2.0.3        multcomp_1.4-28      \n#&gt;  [7] snakecase_0.11.1      compiler_4.5.1        systemfonts_1.2.3    \n#&gt; [10] vctrs_0.6.5           gsl_2.1-8             stringr_1.5.1        \n#&gt; [13] pkgconfig_2.0.3       arrayhelpers_1.1-0    fastmap_1.2.0        \n#&gt; [16] backports_1.5.0       rmarkdown_2.29        ragg_1.5.0           \n#&gt; [19] purrr_1.1.0           xfun_0.53             cachem_1.1.0         \n#&gt; [22] jsonlite_2.0.0        broom_1.0.9           parallel_4.5.1       \n#&gt; [25] R6_2.6.1              stringi_1.8.7         RColorBrewer_1.1-3   \n#&gt; [28] lubridate_1.9.4       estimability_1.5.1    knitr_1.50           \n#&gt; [31] zoo_1.8-14            pacman_0.5.1          Matrix_1.7-4         \n#&gt; [34] splines_4.5.1         timechange_0.3.0      tidyselect_1.2.1     \n#&gt; [37] abind_1.4-8           codetools_0.2-20      curl_7.0.0           \n#&gt; [40] pkgbuild_1.4.8        lattice_0.22-7        withr_3.0.2          \n#&gt; [43] bridgesampling_1.1-2  coda_0.19-4.1         evaluate_1.0.5       \n#&gt; [46] survival_3.8-3        RcppParallel_5.1.11-1 tensorA_0.36.2.1     \n#&gt; [49] checkmate_2.3.3       stats4_4.5.1          distributional_0.5.0 \n#&gt; [52] generics_0.1.4        rprojroot_2.1.1       rstantools_2.5.0     \n#&gt; [55] scales_1.4.0          xtable_1.8-4          glue_1.8.0           \n#&gt; [58] emmeans_1.11.2-8      tools_4.5.1           mvtnorm_1.3-3        \n#&gt; [61] grid_4.5.1            QuickJSR_1.8.0        colorspace_2.1-1     \n#&gt; [64] nlme_3.1-168          cli_3.6.5             evd_2.3-7.1          \n#&gt; [67] textshaping_1.0.3     expm_1.0-0            svUnit_1.0.8         \n#&gt; [70] Brobdingnag_1.2-9     V8_7.0.0              gtable_0.3.6         \n#&gt; [73] digest_0.6.37         msm_1.8.2             TH.data_1.1-4        \n#&gt; [76] htmlwidgets_1.6.4     farver_2.1.2          memoise_2.0.1        \n#&gt; [79] htmltools_0.5.8.1     lifecycle_1.0.4       MASS_7.3-65",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Dalla descrizione alla spiegazione</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/05_cognitive_models.html#bibliografia",
    "href": "chapters/key_notions/05_cognitive_models.html#bibliografia",
    "title": "6  Dalla descrizione alla spiegazione",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nBechtel, W. (2009). Looking down, around, and up: Mechanistic explanation in psychology. Philosophical Psychology, 22(5), 543–564.\n\n\nPovich, M. (2025). Mechanistic Explanation in Psychology. In H. Stam & H. Looren de Jong (A c. Di), The SAGE Handbook of Theoretical Psychology. SAGE Publications.\n\n\nSoto, F. A., Vogel, E. H., Uribe-Bahamonde, Y. E., & Perez, O. D. (2023). Why is the Rescorla-Wagner model so influential? Neurobiology of Learning and Memory, 204, 107794.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Dalla descrizione alla spiegazione</span>"
    ]
  },
  {
    "objectID": "chapters/R/introduction_r_lang.html",
    "href": "chapters/R/introduction_r_lang.html",
    "title": "R",
    "section": "",
    "text": "Scrivere Codice\nLa programmazione si fonda su un approccio strutturato che combina logica computazionale e strumenti tecnici, articolandosi su due piani complementari: il livello algoritmico e il livello sintattico.",
    "crumbs": [
      "R"
    ]
  },
  {
    "objectID": "chapters/R/introduction_r_lang.html#scrivere-codice",
    "href": "chapters/R/introduction_r_lang.html#scrivere-codice",
    "title": "R",
    "section": "",
    "text": "Livello algoritmico: l’astrazione del problema\nIn questa fase, si definisce la soluzione concettuale indipendentemente dal linguaggio, attraverso:\n\nanalisi degli input;\nspecifica dell’output;\n\nprogettazione dell’algoritmo.\n\nPer esempio, l’input può essere costituito da un insieme di valori numerici; l’output può corrispondere alla media aritmetica; l’algorimo può essere formalizzato come:\n\\[\n\\text{media} = \\frac{\\sum_{i=1}^{n} x_i}{n} .\n\\]\nQuesto processo richiede capacità di problem solving e modellizzazione astratta, competenze trasversali a qualsiasi linguaggio.\n\n\nLivello sintattico: l’implementazione pratica\nLa soluzione algoritmica viene poi tradotta in codice seguendo le regole specifiche del linguaggio scelto:\nEsempio in R\nmedia &lt;- sum(x) / length(x)\nEsempio in Python\nmedia = sum(x) / len(x)\nPur mantenendo la stessa logica, le differenze sintattiche evidenziano come l’implementazione sia vincolata allo strumento utilizzato.",
    "crumbs": [
      "R"
    ]
  },
  {
    "objectID": "chapters/R/introduction_r_lang.html#priorità-formative-nellera-dellia",
    "href": "chapters/R/introduction_r_lang.html#priorità-formative-nellera-dellia",
    "title": "R",
    "section": "Priorità formative nell’era dell’IA",
    "text": "Priorità formative nell’era dell’IA\nNell’attuale contesto tecnologico dominato dall’intelligenza artificiale, la formazione nella programmazione richiede una ridefinizione delle priorità. Abbiamo visto come sia necessario distinguere tra due dimensioni: da un lato, la capacità di pensare algoritmicamente, ossia l’abilità di scomporre problemi complessi in passaggi logici e astratti; dall’altro, la padronanza della sintassi, ovvero delle regole specifiche dei linguaggi di programmazione.\nIl pensiero algoritmico rappresenta il cuore creativo e critico della programmazione. È ciò che permette di trasformare un problema in una sequenza ordinata di operazioni risolutive. Questa competenza, radicata nella logica e nell’astrazione, rimane un dominio squisitamente umano: per quanto avanzate, le IA non possono sostituire la capacità di formulare domande pertinenti, riconoscere pattern originali o immaginare soluzioni innovative. Senza questa base concettuale, ogni tentativo di risolvere problemi computazionali sarebbe destinato a fallire, anche con gli strumenti più potenti a disposizione.\nLa sintassi computazionale, sebbene necessaria, assume oggi un ruolo diverso. Strumenti di code generation, stanno democratizzando l’accesso alla scrittura del codice: piattaforme intelligenti possono suggerire implementazioni, correggere errori e persino tradurre algoritmi tra linguaggi diversi (Cooper et al., 2024). Gli errori sintattici – un tempo ostacoli insormontabili per i principianti – diventano sempre più correggibili attraverso l’esperienza o l’automazione.\nQuesta gerarchia di competenze riecheggia il framework teorico di Marr, sviluppato nel campo della visione artificiale. Marr distingue tre livelli di analisi: il “perché” del sistema (l’obiettivo computazionale), il “come” logico (la progettazione algoritmica) e il “con cosa” concreto (l’implementazione fisica). Nell’educazione alla programmazione, questo si traduce in una scelta precisa: privilegiare la progettazione consapevole di algoritmi rispetto alla mera esecuzione tecnica.\nLa priorità formativa diventa quindi chiara. Coltivare il pensiero algoritmico significa allenare quella mentalità progettuale che permette di dialogare in modo critico con l’IA: formulare prompt efficaci richiede prima di tutto di comprendere a fondo la struttura del problema; valutare soluzioni proposte dall’intelligenza artificiale presuppone la capacità di riconoscere logiche difettose o approcci subottimali. Allo stesso tempo, questa competenza agisce come un “sesto senso tecnologico”, permettendo di adattarsi a linguaggi e strumenti in continua evoluzione.\nLa sintassi non viene certo abbandonata, ma contestualizzata. L’automazione non sostituisce l’apprendimento, ma lo rende più strategico: invece di memorizzare comandi, si impara a selezionarli e combinarli in modo funzionale agli obiettivi algoritmici.",
    "crumbs": [
      "R"
    ]
  },
  {
    "objectID": "chapters/R/introduction_r_lang.html#r-uno-strumento-per-lanalisi-dei-dati",
    "href": "chapters/R/introduction_r_lang.html#r-uno-strumento-per-lanalisi-dei-dati",
    "title": "R",
    "section": "R: Uno Strumento per l’Analisi dei Dati",
    "text": "R: Uno Strumento per l’Analisi dei Dati\nPer trovare la soluzione concreta a un problema di analisi dei dati, è necessario implementare l’algoritmo desiderato in un linguaggio di programmazione. In questo insegnamento, utilizzeremo R, uno dei linguaggi più utilizzati per l’analisi dei dati, apprezzato per la sua flessibilità, potenza e il supporto offerto da una vasta comunità di utenti e sviluppatori.\n\nPerché R?\n\nNato per l’analisi statistica: R è stato concepito specificamente per rispondere alle esigenze di analisi statistica e visualizzazione grafica, diventando rapidamente uno strumento essenziale nel panorama accademico e scientifico.\nGestione dei dati: R offre strumenti avanzati per gestire, manipolare e analizzare grandi quantità di dati, coprendo un’ampia gamma di tecniche statistiche, dalla modellazione lineare all’analisi delle serie temporali.\nVisualizzazione grafica: Con pacchetti come ggplot2 e plotly, R permette di creare grafici e visualizzazioni di alta qualità, fondamentali per comunicare risultati in modo efficace.\nComunità e pacchetti: L’ecosistema di R è arricchito da una vasta libreria di pacchetti, che estendono le capacità del linguaggio per soddisfare necessità specifiche e settoriali.\n\n\n\nR in Psicologia e nelle Scienze Sociali\nNato come linguaggio dedicato alla statistica, R si è evoluto fino a diventare un punto di riferimento per psicologi, ricercatori e professionisti impegnati nella valutazione psicometrica, nell’analisi del comportamento e nella modellizzazione di dati complessi. La sua flessibilità, unita alla vastissima collezione di pacchetti specifici, lo rende adatto a molteplici applicazioni in psicologia, dalla costruzione e validazione di test alla gestione di dati provenienti da studi sperimentali, longitudinali ed Ecological Momentary Assessment (EMA).",
    "crumbs": [
      "R"
    ]
  },
  {
    "objectID": "chapters/R/introduction_r_lang.html#riflessioni-conclusive",
    "href": "chapters/R/introduction_r_lang.html#riflessioni-conclusive",
    "title": "R",
    "section": "Riflessioni Conclusive",
    "text": "Riflessioni Conclusive\nImparare ad usare R non significa solo acquisire competenze tecniche, ma anche aprire le porte a nuove possibilità di analisi e ricerca. Tuttavia, è fondamentale ricordare che la vera sfida nella programmazione non è padroneggiare la sintassi di un linguaggio specifico, ma comprendere la logica algoritmica che sta alla base della soluzione di un problema. L’IA può aiutarci a trovare la sintassi corretta, ma spetta a noi decidere quale algoritmo implementare. Pertanto, i nostri sforzi devono essere rivolti a capire la logica del problema, piuttosto che concentrarci esclusivamente sull’implementazione sintattica.",
    "crumbs": [
      "R"
    ]
  },
  {
    "objectID": "chapters/R/introduction_r_lang.html#bibliografia",
    "href": "chapters/R/introduction_r_lang.html#bibliografia",
    "title": "R",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nCooper, N., Clark, A. T., Lecomte, N., Qiao, H., & Ellison, A. M. (2024). Harnessing large language models for coding, teaching and inclusion to empower research in ecology and evolution. Methods in Ecology and Evolution.",
    "crumbs": [
      "R"
    ]
  },
  {
    "objectID": "chapters/R/01_r_syntax.html",
    "href": "chapters/R/01_r_syntax.html",
    "title": "7  Un approccio moderno all’analisi dei dati",
    "section": "",
    "text": "Introduzione\nNell’analisi dei dati psicologici, R non è solo uno strumento statistico avanzato, ma un vero e proprio linguaggio per organizzare il pensiero scientifico. La sua sintassi trasforma procedure complesse in passaggi chiari, verificabili e ripetibili, rispondendo alla crisi della replicabilità che ha coinvolto la psicologia negli ultimi anni (Obels et al., 2020). Imparare R significa quindi acquisire un metodo di lavoro rigoroso e trasparente.",
    "crumbs": [
      "R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Un approccio moderno all'analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/R/01_r_syntax.html#introduzione",
    "href": "chapters/R/01_r_syntax.html#introduzione",
    "title": "7  Un approccio moderno all’analisi dei dati",
    "section": "",
    "text": "Panoramica del capitolo\n\nInstallare R e RStudio.\nCreare e gestire progetti in RStudio.\nManipolare oggetti e vettori in R.\nUtilizzare funzioni e lavorare con dati mancanti.\nEstrarre e gestire sottoinsiemi di dati.\nApprezzare l’importanza di rendere riproducibile l’analisi dei dati, condividendone ogni passaggio.\n\n\n\n\n\n\n\nPrerequisiti\n\n\n\n\n\n\nLeggere attentamene l’?sec-apx-sums.\nLeggere il capitolo Getting Started with Data in R di Statistical Inference via Data Science: A ModernDive into R and the Tidyverse (Second Edition), capitoli 1.1-1.3.\nConsultare Introduction to Data Science: Data Wrangling and Visualization with R (Irizarry, 2024)\n\nConsultare R for Data Science (2e) (Wickham et al., 2023).\nConsultare R Programming for Data Science, capitoli 3-4.\n\n\n\n\n\n\n\n\n\n\nPreparazione del Notebook\n\n\n\n\n\n\nhere::here(\"code\", \"_common.R\") |&gt; \n  source()\n\n# Load packages\nif (!requireNamespace(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyr)",
    "crumbs": [
      "R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Un approccio moderno all'analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/R/01_r_syntax.html#la-sintassi-di-r-come-garanzia-di-trasparenza",
    "href": "chapters/R/01_r_syntax.html#la-sintassi-di-r-come-garanzia-di-trasparenza",
    "title": "7  Un approccio moderno all’analisi dei dati",
    "section": "\n7.1 La Sintassi di R come garanzia di trasparenza",
    "text": "7.1 La Sintassi di R come garanzia di trasparenza\nA differenza dei software a menu grafici (come Excel o SPSS), dove le operazioni restano “nascoste” dietro click del mouse, R richiede di descrivere esplicitamente ogni passaggio. Prendiamo questo esempio base:\ndati &lt;- read.csv(\"esperimento1.csv\")\nmodello &lt;- lm(risposta ~ trattamento, data = dati)\nQuesto semplice script realizza tre cose fondamentali:\n\n\nDocumentazione automatica: Ogni operazione resta tracciata nel codice.\n\nVerifica immediata: È possibile ispezionare ogni passaggio (Cosa contiene dati? Come è definito modello?).\n\nModifiche controllate: Cambiare un parametro (es. il file di input) non richiede di rifare tutta l’analisi manualmente.\n\n\n7.1.1 Perché R favorisce la replicabilità\nTre caratteristiche di R facilitano direttamente la replicabilità:\n\n\nStruttura basata su script: Scrivere codice in file .R o .qmd crea una traccia completa e ordinata dell’analisi, integrando:\n\nistruzioni eseguibili,\nannotazioni metodologiche,\nvisualizzazione dei risultati.\n\n\nGestione esplicita dei pacchetti:\nComandi come library(lme4) o install.packages(\"brms\") rendono esplicite tutte le risorse usate, evitando il classico “Sul mio computer funzionava!”.\n\nLiterate programming tramite R Markdown:\nLa combinazione di codice, testo narrativo e risultati dinamici (Knuth, 1984) in documenti Quarto (o R Markdown) consente di generare report che combinano:\n\ntesto esplicativo,\nanalisi eseguibile,\nrisultati dinamici (grafici, tabelle).\n\n\n\n7.1.2 Buone abitudini da adottare subito\n\n\nNomi descrittivi\nUtilizzare sempre nomi chiari per oggetti e dati:\n# Da evitare\nx &lt;- read.csv(\"file1.csv\")  \n\n# Preferibile\ndemographics_data &lt;- read.csv(\"demographic_questionnaire.csv\")  \n\nSalvataggio progressivo delle modifiche (versionamento)\nR permette di salvare e tenere traccia delle modifiche ai file con sistemi come Git. Non è necessario impararlo subito, ma è utile sapere che strumenti come GitHub consentono facilmente di archiviare versioni successive del proprio lavoro, facilitando il recupero di versioni precedenti in caso di necessità.\n\nChecklist pre-invio\nPrima di condividere un’analisi, è buona norma verificare:\n\neseguire lo script integralmente (Ctrl+Shift+Enter su RStudio);\ncontrollare che i percorsi dei file siano corretti (es.: “Il file dati.csv si trova nella cartella giusta?”);\naggiornare tutti i pacchetti installati con update.packages(ask = FALSE).\n\n\n\nQueste pratiche rendono il codice più robusto, l’analisi più affidabile e i risultati più facilmente verificabili.\n\n7.1.3 Perché queste regole contano nella ricerca psicologica\nL’apprendimento di R va oltre l’acquisizione di competenze tecniche. Ogni scelta sintattica riflette un principio scientifico:\n\n\nElemento del codice\nPrincipio metodologico\n\n\n\nset.seed(123)\nControllo delle fonti di casualità\n\n\ndplyr::filter()\nTracciabilità delle esclusioni\n\n\nAPA_style()\nStandardizzazione della reportistica\n\n\n\nIn un contesto dove il 50% degli studi psicologici mostra difficoltà di replicazione (Collaboration, 2015), R offre un framework per costruire ricerche solide fin dalla fase di progettazione.\n\n7.1.4 Prossimi passi\n\nScarica e installa R.\nVai al sito ufficiale di CRAN (https://cran.r-project.org/), scegli la versione per il tuo sistema operativo (Windows, Mac o Linux) e segui le istruzioni di installazione.\nScarica e installa RStudio.\nDopo aver installato R, scarica RStudio dal sito ufficiale (https://posit.co/download/rstudio-desktop/). Scegli la versione gratuita “RStudio Desktop” e segui le istruzioni per il tuo sistema operativo.\n\nUna spiegazione dettagliata del processo di installazione di R e RStudio è disponibile in Okoye & Hosseini (2024).",
    "crumbs": [
      "R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Un approccio moderno all'analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/R/01_r_syntax.html#panoramica-sullinterfaccia-di-rstudio",
    "href": "chapters/R/01_r_syntax.html#panoramica-sullinterfaccia-di-rstudio",
    "title": "7  Un approccio moderno all’analisi dei dati",
    "section": "\n7.2 Panoramica sull’interfaccia di RStudio",
    "text": "7.2 Panoramica sull’interfaccia di RStudio\nRStudio rende l’uso di R più intuitivo grazie alla sua interfaccia divisa in quattro pannelli principali:\n\n\nPannello degli script: Qui puoi scrivere e modificare i tuoi script, cioè sequenze di comandi salvabili per analisi ripetibili e organizzate.\n\nConsole: Esegue i comandi scritti direttamente o lanciati dagli script, mostrando risultati, messaggi e errori.\n\nPannello dell’ambiente: Mostra i dataset, le variabili e gli oggetti caricati nella sessione di lavoro, permettendoti di gestire facilmente i dati.\n\nPannello grafici/aiuto/file: Visualizza grafici, fornisce accesso alla documentazione di R e consente di navigare tra file e cartelle sul tuo sistema.",
    "crumbs": [
      "R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Un approccio moderno all'analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/R/01_r_syntax.html#creare-un-nuovo-progetto-in-rstudio",
    "href": "chapters/R/01_r_syntax.html#creare-un-nuovo-progetto-in-rstudio",
    "title": "7  Un approccio moderno all’analisi dei dati",
    "section": "\n7.3 Creare un nuovo progetto in RStudio",
    "text": "7.3 Creare un nuovo progetto in RStudio\nAvviare un nuovo progetto\nDal menu di RStudio, seleziona File &gt; New Project… per creare un nuovo progetto. I progetti in RStudio sono uno strumento efficace per organizzare il lavoro relativo a una specifica analisi o domanda di ricerca. All’interno di un progetto puoi raccogliere script, file di dati e output, mantenendo tutto ben strutturato.\nScegliere la posizione del progetto\nPuoi creare una nuova directory dedicata al progetto oppure associare il progetto a una directory esistente. Organizzare i progetti in cartelle dedicate aiuta a mantenere i file in ordine e a utilizzare percorsi relativi, rendendo il tuo lavoro più facile da condividere con collaboratori e più portabile tra diversi sistemi.\nQuesta organizzazione è particolarmente utile per evitare confusione e assicurarsi che tutti i file necessari siano facilmente accessibili e collegati al progetto corretto.",
    "crumbs": [
      "R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Un approccio moderno all'analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/R/01_r_syntax.html#concetti-di-base-nella-programmazione-in-r",
    "href": "chapters/R/01_r_syntax.html#concetti-di-base-nella-programmazione-in-r",
    "title": "7  Un approccio moderno all’analisi dei dati",
    "section": "\n7.4 Concetti di base nella programmazione in R",
    "text": "7.4 Concetti di base nella programmazione in R\nIniziare a usare R, soprattutto per chi si avvicina per la prima volta a questo linguaggio nel contesto della psicologia, significa comprendere i concetti fondamentali che ne costituiscono la base. Questo capitolo introduce i principi essenziali della programmazione in R, tra cui:\n\nLa comprensione della sintassi di R.\nLa familiarizzazione con i principali tipi di dati e strutture.\nL’acquisizione delle operazioni di base.\n\nQuesti concetti sono fondamentali per manipolare efficacemente i dati e condurre analisi statistiche, rappresentando il punto di partenza per sfruttare al meglio le potenzialità di R.",
    "crumbs": [
      "R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Un approccio moderno all'analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/R/01_r_syntax.html#oggetti-in-r",
    "href": "chapters/R/01_r_syntax.html#oggetti-in-r",
    "title": "7  Un approccio moderno all’analisi dei dati",
    "section": "\n7.5 Oggetti in R",
    "text": "7.5 Oggetti in R\nIn R, tutto è un oggetto: dai numeri e stringhe di testo più semplici fino a strutture più complesse come vettori, data frame, funzioni, modelli statistici o persino grafici. Un oggetto in R è semplicemente un contenitore che memorizza un valore o una serie di valori, permettendoti di manipolarli e riutilizzarli nel codice.\n\n7.5.1 Creare oggetti\nPer creare un oggetto, è necessario assegnargli un nome e un valore utilizzando l’operatore di assegnazione &lt;- (consigliato) o = (meno utilizzato):\n\nmy_obj &lt;- 48\n\nIn questo esempio, abbiamo creato un oggetto chiamato my_obj e gli abbiamo assegnato il valore 48. Ora questo numero è memorizzato con quel nome e può essere richiamato facilmente.\nPer visualizzare il valore di un oggetto, basta scriverne il nome e premere Invio:\n\nmy_obj\n#&gt; [1] 48\n\n\n7.5.1.1 Dove vengono salvati gli oggetti?\nGli oggetti creati vengono memorizzati nell’ambiente di lavoro (workspace) e restano disponibili finché non vengono rimossi o finché la sessione di R non viene chiusa. Se stai usando RStudio, puoi vedere tutti gli oggetti attualmente presenti nella scheda Environment, dove vengono mostrati con dettagli come tipo, lunghezza e valore.\n\n7.5.1.2 Perché gli oggetti sono importanti?\nLavorare con oggetti in R permette di:\n\n\nRiutilizzare dati e risultati senza doverli digitare nuovamente.\n\n\nOrganizzare il codice in modo chiaro e leggibile, rendendo le analisi più strutturate.\n\n\nManipolare facilmente i dati, combinando, trasformando e analizzando gli oggetti in base alle esigenze.\n\n7.5.1.3 Stringhe\nÈ possibile assegnare a un oggetto anche una stringa di testo, racchiudendola tra virgolette:\n\nmy_obj2 &lt;- \"R è fantastico\"\nmy_obj2\n#&gt; [1] \"R è fantastico\"\n\nSe dimentichi le virgolette, R mostrerà un errore.\n\n7.5.1.4 Modificare oggetti\nPer modificare il valore di un oggetto esistente, basta riassegnarlo:\n\nmy_obj2 &lt;- 1024\n\nOra il tipo di my_obj2 è cambiato da carattere a numerico. È anche possibile usare oggetti per crearne di nuovi:\n\nmy_obj3 &lt;- my_obj + my_obj2\nmy_obj3\n#&gt; [1] 1072\n\n\n7.5.1.5 Manipolare oggetti\nSe provi a sommare oggetti di tipo diverso, R restituirà un errore:\nchar_obj &lt;- \"ciao\"\nchar_obj2 &lt;- \"mondo\"\nchar_obj3 &lt;- char_obj + char_obj2\n#&gt; Error in char_obj + char_obj2 : non-numeric argument to binary operator\nQuando incontri errori come questo, chiedi a AI la spiegazione del messaggio, per esempio: “non-numeric argument to binary operator error + r”. Un errore comune è anche:\nmy_obj &lt;- 48\nmy_obj4 &lt;- my_obj + no_obj\n#&gt; Error: object 'no_obj' not found\nR segnala che no_obj non è stato definito e, di conseguenza, l’oggetto my_obj4 non è stato creato.",
    "crumbs": [
      "R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Un approccio moderno all'analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/R/01_r_syntax.html#nomi-degli-oggetti",
    "href": "chapters/R/01_r_syntax.html#nomi-degli-oggetti",
    "title": "7  Un approccio moderno all’analisi dei dati",
    "section": "\n7.6 Nomi degli oggetti",
    "text": "7.6 Nomi degli oggetti\nAttribuire nomi agli oggetti potrebbe sembrare un dettaglio secondario, ma è fondamentale scegliere nomi brevi e informativi. Un buon nome migliora la leggibilità del codice e ne facilita la manutenzione. È importante adottare uno stile coerente, come uno dei seguenti:\n\n\nSnake case: output_summary\n\n\nDot case: output.summary\n\n\nCamel case: outputSummary\n\n\nIn questo corso useremo lo stile più diffuso, Snake Case, che separa le parole con il carattere di sottolineatura _.\nCi sono alcune regole fondamentali da rispettare nella scelta dei nomi:\n\nNon possono iniziare con un numero (ad esempio, 2my_variable non è valido).\n\nNon possono contenere caratteri speciali come &, ^, /, ecc.\n\nEvita di usare parole riservate (ad esempio, TRUE, NA) o nomi di funzioni esistenti (ad esempio, data).\n\nEsempio di cosa non fare:\ndata &lt;- read.table(\"mydatafile\", header = TRUE) # `data` è già una funzione!",
    "crumbs": [
      "R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Un approccio moderno all'analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/R/01_r_syntax.html#commenti",
    "href": "chapters/R/01_r_syntax.html#commenti",
    "title": "7  Un approccio moderno all’analisi dei dati",
    "section": "\n7.7 Commenti",
    "text": "7.7 Commenti\nI commenti sono uno strumento essenziale per rendere il codice più chiaro e comprensibile, sia per te stesso sia per altri. Nel linguaggio R, i commenti iniziano con il simbolo #, e tutto ciò che lo segue sulla stessa riga viene ignorato dall’interprete durante l’esecuzione.\n\n7.7.1 Perché commentare?\nI commenti servono a spiegare perché il codice è scritto in un certo modo, non solo come funziona (questo è evidente leggendo il codice). Una buona pratica consiste nel commentare le decisioni o i passaggi che non risultano immediatamente evidenti.\nAd esempio, invece di scrivere un commento ridondante come:\n\n# Assegno 42 alla variabile x\nx &lt;- 42\n\nè più utile fornire un contesto:\n\n# Valore iniziale scelto per semplificare i calcoli successivi\nx &lt;- 42\n\n\n7.7.2 Vantaggi\nCommentare in modo appropriato aiuta a:\n\n\nRidurre il tempo necessario per comprendere o modificare il codice, anche mesi o anni dopo averlo scritto.\n\nFacilitare la collaborazione con altri, rendendo il codice leggibile e accessibile.\n\nMigliorare la manutenibilità e il riutilizzo del codice.\n\nUn codice ben commentato non è solo più facile da leggere, ma anche più professionale e robusto nel lungo termine.",
    "crumbs": [
      "R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Un approccio moderno all'analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/R/01_r_syntax.html#usare-r-come-calcolatore",
    "href": "chapters/R/01_r_syntax.html#usare-r-come-calcolatore",
    "title": "7  Un approccio moderno all’analisi dei dati",
    "section": "\n7.8 Usare R come calcolatore",
    "text": "7.8 Usare R come calcolatore\nR può essere utilizzato come un semplice calcolatore digitando direttamente nella console numeri e operatori aritmetici per eseguire operazioni come somma, sottrazione, moltiplicazione e divisione (+, -, *, /). Questo lo rende uno strumento immediato e versatile per calcoli di base e avanzati.\n\nEsempio 7.1 La Satisfaction With Life Scale (SWLS) contiene 5 item, ciascuno valutato con una scala Likert a 7 punti, dove:\n1 = “completamente in disaccordo” e 7 = “completamente d’accordo”.\nGli item sono:\n\nPer la maggior parte, la mia vita si avvicina al mio ideale.\n\nLe mie condizioni di vita sono eccellenti.\n\nSono soddisfatto della mia vita.\n\nFino ad ora, ho ottenuto le cose importanti che voglio nella vita.\n\nSe potessi vivere la mia vita di nuovo, non cambierei quasi nulla.\n\nSupponiamo che un individuo risponda nel seguente modo:\n\nItem 1: 5\n\nItem 2: 3\n\nItem 3: 4\n\nItem 4: 2\n\nItem 5: 2\n\nIl punteggio totale sulla SWLS si calcola sommando i punteggi di ciascun item:\n\nsogg1 &lt;- 5 + 3 + 4 + 2 + 2 \nsogg1\n#&gt; [1] 16\n\n\n\nEsempio 7.2 Il Body Mass Index (BMI) si calcola dividendo il peso, in chilogrammi, per il quadrato dell’altezza, in metri.\nLa formula è:\n\\[\n\\text{BMI} = \\frac{\\text{Peso (kg)}}{\\text{Altezza (m)}^2} .\n\\]\nSupponiamo che un individuo pesi 79000 grammi (79 kg) e sia alto 176 cm. Il calcolo in R sarà:\n\nbmi &lt;- (79000 / 1000) / (176 / 100)^2\nbmi\n#&gt; [1] 25.5\n\n\nNota. L’uso di parentesi è fondamentale per garantire che le operazioni vengano eseguite nell’ordine corretto. In R, come in matematica, le operazioni racchiuse tra parentesi hanno la precedenza rispetto ad altre operazioni. Ad esempio, nel calcolo del BMI, abbiamo usato le parentesi per calcolare prima la conversione dei valori nell’unità di misura appropriata.",
    "crumbs": [
      "R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Un approccio moderno all'analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/R/01_r_syntax.html#ordine-di-precedenza-degli-operatori",
    "href": "chapters/R/01_r_syntax.html#ordine-di-precedenza-degli-operatori",
    "title": "7  Un approccio moderno all’analisi dei dati",
    "section": "\n7.9 Ordine di precedenza degli operatori",
    "text": "7.9 Ordine di precedenza degli operatori\nLe operazioni algebriche vengono eseguite in una particolare sequenza in R, nota come ordine di precedenza degli operatori. Questo ordine determina quali operazioni vengono eseguite per prime quando un’espressione include più operatori. In assenza di parentesi, l’ordine di precedenza è il seguente (dal più alto al più basso):\n\n\nParentesi: Le operazioni racchiuse tra parentesi () vengono eseguite per prime. Questo permette di sovrascrivere l’ordine naturale delle operazioni.\n\nresult &lt;- (2 + 3) * 4  # Risultato: 20\n\n\n\nEsponenziazione: L’operatore ^ viene eseguito dopo le parentesi.\n\nresult &lt;- 2^3  # Risultato: 8\n\n\n\nSegni unari: Il segno meno - o più + applicato a un singolo valore.\n\nresult &lt;- -3 + 5  # Risultato: 2\n\n\n\nMoltiplicazione, divisione e modulo: Gli operatori *, /, %/% (divisione intera) e %% (resto) hanno la stessa precedenza e vengono eseguiti da sinistra a destra.\n\nresult &lt;- 10 / 2 * 3  # Risultato: 15\nresult &lt;- 10 %% 3     # Risultato: 1\n\n\n\nAddizione e sottrazione: Gli operatori + e - vengono eseguiti dopo quelli di moltiplicazione/divisione.\n\nresult &lt;- 5 + 3 - 2  # Risultato: 6\n\n\n\nOperatori di assegnazione: Gli operatori &lt;-, -&gt;, =, che assegnano valori a variabili, vengono valutati per ultimi.\n\nx &lt;- 2 + 3 * 4  # Risultato: 14\n\n\n\nNote importanti:\n\n\nAssociazione a sinistra: La maggior parte degli operatori in R viene valutata da sinistra a destra (ad esempio, +, *, /).\n\nUso delle parentesi: Quando l’ordine di precedenza non è immediatamente chiaro o si vuole assicurare un ordine specifico, è sempre buona pratica usare le parentesi.\n\nCapire l’ordine di precedenza è fondamentale per evitare errori logici e garantire che il codice funzioni come previsto.",
    "crumbs": [
      "R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Un approccio moderno all'analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/R/01_r_syntax.html#funzioni",
    "href": "chapters/R/01_r_syntax.html#funzioni",
    "title": "7  Un approccio moderno all’analisi dei dati",
    "section": "\n7.10 Funzioni",
    "text": "7.10 Funzioni\nFino ad ora abbiamo creato oggetti semplici assegnando loro direttamente un valore. Con l’aumento dell’esperienza in R, potresti voler creare oggetti più complessi. Per aiutarti, R offre numerose funzioni già disponibili nella sua installazione di base, e altre possono essere aggiunte installando pacchetti. Una funzione è un insieme di istruzioni che eseguono un compito specifico. Inoltre, è possibile creare funzioni personalizzate.\n\n7.10.1 La funzione c() per creare vettori\nLa prima funzione utile da imparare è c(), che serve a concatenare valori in un vettore. Ad esempio:\n\nmy_vec &lt;- c(2, 3, 1, 6, 4, 3, 3, 7)\n\nQuesto codice crea un oggetto chiamato my_vec che contiene una sequenza di numeri. Alcuni concetti fondamentali sulle funzioni in R:\n\n\nNome e parentesi: Le funzioni in R sono sempre seguite da parentesi tonde ().\n\nArgomenti: Gli elementi passati alla funzione (tra le parentesi) ne personalizzano il comportamento e sono separati da virgole.\n\nPer vedere il contenuto del vettore:\n\nmy_vec\n#&gt; [1] 2 3 1 6 4 3 3 7\n\n\n7.10.2 Funzioni per analizzare vettori\nPuoi utilizzare altre funzioni per calcolare statistiche sul vettore:\n\nmean(my_vec)    # Media\n#&gt; [1] 3.62\n\n\nvar(my_vec)     # Varianza\n#&gt; [1] 3.98\n\n\nsd(my_vec)      # Deviazione standard\n#&gt; [1] 2\n\n\nlength(my_vec)  # Numero di elementi\n#&gt; [1] 8\n\nPuoi anche salvare i risultati in nuovi oggetti per riutilizzarli:\n\nvec_mean &lt;- mean(my_vec)\nvec_mean\n#&gt; [1] 3.62\n\n\n\n\n\n\n\nConcetto Chiave\n\n\n\n\n\nLa varianza e la deviazione standard sono misure statistiche descrittive che sintetizzano in un unico valore numerico la variabilità di un insieme di dati. Questi indici, che verranno approfonditi nel Capitolo 19, forniscono informazioni su quanto i valori di un dataset siano simili o diversi tra loro.\nIn particolare:\n\nla varianza e la deviazione standard sono pari a 0 quando tutti i valori nel dataset sono identici, indicando assenza di variabilità;\nassumono valori più elevati all’aumentare delle differenze tra i dati, segnalando una maggiore dispersione.\n\nPer i nostri scopi attuali, è sufficiente comprendere che queste misure descrivono il grado di diversità o omogeneità dei dati.\n\n\n\n\n7.10.3 Creare sequenze regolari\nPer creare sequenze di numeri in passi regolari, puoi usare i seguenti comandi.\nSimbolo : per sequenze semplici:\n\nmy_seq &lt;- 1:10\nmy_seq\n#&gt;  [1]  1  2  3  4  5  6  7  8  9 10\n\nFunzione seq() per maggiore controllo:\n\nmy_seq2 &lt;- seq(from = 1, to = 5, by = 0.5)\nmy_seq2\n#&gt; [1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\n\n\n7.10.4 Ripetere valori\nPuoi ripetere valori o sequenze con la funzione rep().\nRipetere un valore:\n\nmy_seq3 &lt;- rep(2, times = 10)\nmy_seq3\n#&gt;  [1] 2 2 2 2 2 2 2 2 2 2\n\nRipetere una sequenza:\n\nmy_seq5 &lt;- rep(1:5, times = 3)\n\nRipetere ogni elemento di una sequenza:\n\nmy_seq6 &lt;- rep(1:5, each = 3)\nmy_seq6\n#&gt;  [1] 1 1 1 2 2 2 3 3 3 4 4 4 5 5 5\n\n\n7.10.5 Annidare funzioni\nÈ possibile combinare funzioni per creare comandi più complessi, come nell’esempio:\n\nmy_seq7 &lt;- rep(c(3, 1, 10, 7), each = 3)\nmy_seq7\n#&gt;  [1]  3  3  3  1  1  1 10 10 10  7  7  7\n\nPer maggiore leggibilità, puoi separare i passaggi:\n\nin_vec &lt;- c(3, 1, 10, 7)\nmy_seq7 &lt;- rep(in_vec, each = 3)\nmy_seq7\n#&gt;  [1]  3  3  3  1  1  1 10 10 10  7  7  7\n\nQuesta pratica facilita la comprensione del codice e lo rende più chiaro.",
    "crumbs": [
      "R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Un approccio moderno all'analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/R/01_r_syntax.html#lavorare-con-i-vettori-in-r",
    "href": "chapters/R/01_r_syntax.html#lavorare-con-i-vettori-in-r",
    "title": "7  Un approccio moderno all’analisi dei dati",
    "section": "\n7.11 Lavorare con i Vettori in R",
    "text": "7.11 Lavorare con i Vettori in R\nIn R, i vettori sono uno degli elementi fondamentali per manipolare, riassumere e ordinare i dati. Qui trovi una panoramica su come estrarre, sostituire, ordinare, lavorare con dati mancanti e sfruttare la vettorizzazione dei vettori.\n\n7.11.1 Estrarre elementi da un vettore\nPuoi estrarre uno o più elementi da un vettore usando le parentesi quadre [ ].\nPer posizione: Specifica la posizione degli elementi.\n\nmy_vec &lt;- c(2, 3, 1, 6, 4, 3, 3, 7)\nmy_vec[3]  # Terzo elemento\n#&gt; [1] 1\n\n\nmy_vec[c(1, 5, 6)]  # Elementi 1°, 5° e 6°\n#&gt; [1] 2 4 3\n\n\nmy_vec[3:8]  # Da 3° a 8°\n#&gt; [1] 1 6 4 3 3 7\n\nCon condizioni logiche: Usa espressioni logiche per selezionare elementi.\n\nmy_vec[my_vec &gt; 4]  # Elementi &gt; 4\n#&gt; [1] 6 7\n\n\nmy_vec[my_vec &lt;= 4]  # Elementi ≤ 4\n#&gt; [1] 2 3 1 4 3 3\n\n\nmy_vec[my_vec != 4]  # Elementi diversi da 4\n#&gt; [1] 2 3 1 6 3 3 7\n\nOperatori logici: Combina condizioni con & (AND) e | (OR).\n\nmy_vec[my_vec &gt; 2 & my_vec &lt; 6]  # Tra 2 e 6\n#&gt; [1] 3 4 3 3\n\n\n7.11.2 Sostituire elementi in un vettore\nPuoi modificare i valori di un vettore usando [ ] e l’operatore &lt;-.\nUn singolo elemento:\n\nmy_vec[4] &lt;- 500  # Cambia il 4° elemento\nmy_vec\n#&gt; [1]   2   3   1 500   4   3   3   7\n\nPiù elementi:\n\nmy_vec[c(6, 7)] &lt;- 100  # Cambia il 6° e 7° elemento\nmy_vec\n#&gt; [1]   2   3   1 500   4 100 100   7\n\nCon condizioni logiche:\n\nmy_vec[my_vec &lt;= 4] &lt;- 1000  # Cambia valori ≤ 4\nmy_vec\n#&gt; [1] 1000 1000 1000  500 1000  100  100    7\n\n\n7.11.3 Ordinare un vettore\nDal più piccolo al più grande:\n\nvec_sort &lt;- sort(my_vec)\nvec_sort\n#&gt; [1]    7  100  100  500 1000 1000 1000 1000\n\nDal più grande al più piccolo:\n\nvec_sort2 &lt;- sort(my_vec, decreasing = TRUE)\nvec_sort2\n#&gt; [1] 1000 1000 1000 1000  500  100  100    7\n\nOrdinare un vettore in base a un altro:\n\nheight &lt;- c(180, 155, 160, 167, 181)\np.names &lt;- c(\"Joanna\", \"Charlotte\", \"Helen\", \"Karen\", \"Amy\")\nheight_ord &lt;- order(height)\nnames_ord &lt;- p.names[height_ord]\nnames_ord\n#&gt; [1] \"Charlotte\" \"Helen\"     \"Karen\"     \"Joanna\"    \"Amy\"",
    "crumbs": [
      "R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Un approccio moderno all'analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/R/01_r_syntax.html#operazioni-vettoriali-e-vettorizzazione-in-r",
    "href": "chapters/R/01_r_syntax.html#operazioni-vettoriali-e-vettorizzazione-in-r",
    "title": "7  Un approccio moderno all’analisi dei dati",
    "section": "\n7.12 Operazioni vettoriali e vettorizzazione in R",
    "text": "7.12 Operazioni vettoriali e vettorizzazione in R\nLa vettorializzazione è una delle caratteristiche più potenti di R, che consente di applicare operazioni o funzioni direttamente a tutti gli elementi di un vettore in modo simultaneo, senza dover ricorrere a cicli espliciti. Questo approccio rende il codice più conciso, leggibile ed efficiente, sfruttando al meglio le capacità intrinseche del linguaggio.\n\n7.12.1 Operazioni aritmetiche su vettori\nLe operazioni algebriche in R, come addizione, sottrazione, moltiplicazione e divisione, sono vettorizzate. Questo significa che ogni operazione viene applicata “elemento per elemento” al vettore.\nConsideriamo ad esempio il seguente vettore:\n\nmy_vec &lt;- c(3, 5, 7, 1, 9, 20)\n\nSe vogliamo moltiplicare ciascun elemento di my_vec per 5, possiamo scrivere:\n\nmy_vec * 5\n#&gt; [1]  15  25  35   5  45 100\n\nAnalogamente, possiamo effettuare altre operazioni algebriche, come divisione o elevamento a potenza:\n\nmy_vec / 2\n#&gt; [1]  1.5  2.5  3.5  0.5  4.5 10.0\n\n\nmy_vec^2\n#&gt; [1]   9  25  49   1  81 400\n\nQueste operazioni vengono applicate automaticamente a ciascun elemento del vettore, senza dover iterare su di essi.\n\n7.12.2 Operazioni elemento per elemento tra due vettori\nLa vettorializzazione consente anche di eseguire operazioni tra due vettori, applicandole elemento per elemento. Supponiamo di avere un secondo vettore:\n\nmy_vec2 &lt;- c(17, 15, 13, 19, 11, 0)\n\nSe vogliamo sommare i due vettori, possiamo scrivere:\n\nmy_vec + my_vec2\n#&gt; [1] 20 20 20 20 20 20\n\nIn questo caso, il primo elemento di my_vec viene sommato al primo elemento di my_vec2, il secondo elemento al secondo, e così via.\n\nEsempio 7.3 Di seguito mostriamo come calcolare i punteggi totali per 10 individui che hanno risposto ai 5 item della Satisfaction With Life Scale (SWLS), utilizzando le formule e l’aritmetica vettorializzata di R.\nStep 1: Definiamo i punteggi per ciascun item. Ogni vettore contiene i punteggi dati dai 10 individui a uno specifico item della scala:\n\n# Punteggi dei 10 individui per ciascun item\nitem1 &lt;- c(5, 4, 6, 7, 3, 2, 5, 6, 4, 7)\nitem2 &lt;- c(3, 2, 4, 6, 2, 1, 4, 5, 3, 6)\nitem3 &lt;- c(4, 5, 6, 5, 3, 2, 5, 7, 4, 5)\nitem4 &lt;- c(2, 3, 4, 3, 2, 1, 3, 4, 2, 5)\nitem5 &lt;- c(2, 2, 3, 4, 1, 1, 3, 3, 2, 4)\n\nI valori 5, 3, 4, 2, 2 sono i punteggi del primo individuo sui 5 item; i punteggio 4, 2, 5, 3, 2 sono i punteggi del secondo individuo sui 5 item, e così via.\nStep 2: Sommiamo i punteggi per calcolare il totale. Il punteggio totale di ciascun individuo è la somma dei punteggi relativi ai 5 item. Formalmente, per l’individuo \\(i\\) (\\(i = 1, 2, \\ldots, 10\\)), il punteggio totale è calcolato come:\n\\[\n\\text{PunteggioTotale}_i = \\text{item1}_i + \\text{item2}_i + \\text{item3}_i + \\text{item4}_i + \\text{item5}_i .\n\\]\nIn R, possiamo sommare i vettori direttamente grazie all’aritmetica vettorializzata:\n\n# Calcolo dei punteggi totali per ciascun individuo\ntotal_scores &lt;- item1 + item2 + item3 + item4 + item5\ntotal_scores\n#&gt;  [1] 16 16 23 25 11  7 20 25 15 27\n\nIl risultato è un vettore con i punteggi totali per ciascun individuo.\nStep 3: Mostriamo i risultati. Per organizzare meglio i dati, creiamo una tabella che associa i punteggi totali agli individui:\n\n# Creiamo una tabella con i punteggi totali\nindividui &lt;- paste(\"Individuo\", 1:10)\nrisultati_swls &lt;- data.frame(Individuo = individui, PunteggioTotale = total_scores)\nprint(risultati_swls)\n#&gt;       Individuo PunteggioTotale\n#&gt; 1   Individuo 1              16\n#&gt; 2   Individuo 2              16\n#&gt; 3   Individuo 3              23\n#&gt; 4   Individuo 4              25\n#&gt; 5   Individuo 5              11\n#&gt; 6   Individuo 6               7\n#&gt; 7   Individuo 7              20\n#&gt; 8   Individuo 8              25\n#&gt; 9   Individuo 9              15\n#&gt; 10 Individuo 10              27\n\nSpiegazione delle operazioni:\n\nOgni vettore contiene i punteggi di 10 individui per un dato item. Ad esempio, il vettore item1 contiene i punteggi relativi al primo item, e così via.\n\nGrazie all’aritmetica vettorializzata, quando sommiamo i vettori \\(\\text{item1}\\), \\(\\text{item2}\\), \\(\\text{item3}\\), \\(\\text{item4}\\), \\(\\text{item5}\\), R somma elemento per elemento:\n\\[\n\\text{total\\_scores}_i = \\text{item1}_i + \\text{item2}_i + \\text{item3}_i + \\text{item4}_i + \\text{item5}_i\n\\]\n\nQuesta tecnica consente di calcolare rapidamente i punteggi totali per tutti gli individui senza dover scrivere un ciclo esplicito, rendendo il codice più semplice e leggibile.\n\nQuesto esempio illustra come R semplifichi operazioni complesse grazie al calcolo vettorializzato, migliorando l’efficienza e la chiarezza del codice.\n\n\n7.12.3 Attenzione al riciclo dei vettori\nSe i due vettori hanno lunghezze diverse, R applicherà il meccanismo di riciclo: gli elementi del vettore più corto verranno ripetuti ciclicamente per abbinarsi alla lunghezza del vettore più lungo. Questo comportamento, sebbene utile, richiede attenzione per evitare risultati inattesi.\nAd esempio:\n\nshort_vec &lt;- c(1, 2)\nmy_vec + short_vec\n#&gt; [1]  4  7  8  3 10 22\n\nIn questo caso, gli elementi di short_vec vengono riciclati per abbinarsi alla lunghezza di my_vec. Il risultato è:\n(3+1, 5+2, 7+1, 1+2, 9+1, 20+2)\n\n7.12.4 Applicazione di funzioni su vettori\nLa vettorializzazione non si limita alle operazioni algebriche, ma si estende anche all’uso di funzioni. Supponiamo di voler calcolare il logaritmo naturale di ciascun elemento di un vettore:\n\nlog(my_vec)\n#&gt; [1] 1.10 1.61 1.95 0.00 2.20 3.00\n\nLa funzione log() viene applicata automaticamente a ogni elemento del vettore. Analogamente, possiamo utilizzare altre funzioni predefinite di R, come:\n\nsqrt(my_vec)  # Calcola la radice quadrata di ciascun elemento\n#&gt; [1] 1.73 2.24 2.65 1.00 3.00 4.47\nexp(my_vec)   # Eleva e alla potenza specificata da ciascun elemento\n#&gt; [1]        20.09       148.41      1096.63         2.72      8103.08\n#&gt; [6] 485165195.41\n\nIn conclusione, la vettorializzazione in R rappresenta un approccio elegante ed efficiente per gestire calcoli su vettori. Che si tratti di operazioni algebriche, operazioni tra vettori o applicazione di funzioni, la possibilità di evitare cicli espliciti migliora la leggibilità e la velocità del codice. Tuttavia, è importante prestare attenzione al riciclo dei vettori per evitare errori non intenzionali.",
    "crumbs": [
      "R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Un approccio moderno all'analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/R/01_r_syntax.html#gestire-dati-mancanti-na",
    "href": "chapters/R/01_r_syntax.html#gestire-dati-mancanti-na",
    "title": "7  Un approccio moderno all’analisi dei dati",
    "section": "\n7.13 Gestire dati mancanti (NA)",
    "text": "7.13 Gestire dati mancanti (NA)\nR rappresenta i dati mancanti con NA. La gestione dei dati mancanti dipende dalla funzione utilizzata.\nCalcolo con dati mancanti:\n\ntemp &lt;- c(7.2, NA, 7.1, 6.9, 6.5, 5.8, 5.8, 5.5, NA, 5.5)\nmean(temp)  # Restituisce NA\n#&gt; [1] NA\n\n\nmean(temp, na.rm = TRUE)  # Ignora i valori mancanti\n#&gt; [1] 6.29\n\nNota: na.rm = TRUE è un argomento comune per ignorare i NA, ma non tutte le funzioni lo supportano. Consulta la documentazione della funzione per verificare come gestisce i dati mancanti.\nIn conclusione, manipolare vettori è un’abilità essenziale in R. Dalla selezione e modifica degli elementi all’ordinamento e gestione di dati mancanti, queste tecniche sono alla base dell’analisi dei dati in R.",
    "crumbs": [
      "R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Un approccio moderno all'analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/R/01_r_syntax.html#i-dati-in-r",
    "href": "chapters/R/01_r_syntax.html#i-dati-in-r",
    "title": "7  Un approccio moderno all’analisi dei dati",
    "section": "\n7.14 I dati in R",
    "text": "7.14 I dati in R\nIn R, i dati possono essere rappresentati in diversi tipi e strutture. Comprendere come gestirli è fondamentale per manipolare, analizzare e riassumere i dataset più complessi.\n\n7.14.1 Tipi di dati in R\nR supporta diversi tipi di dati:\n\n\nNumeric: Numeri decimali (es. 2.5).\n\nInteger: Numeri interi (es. 3).\n\nLogical: Valori booleani (TRUE o FALSE) e NA per dati mancanti.\n\nCharacter: Stringhe di testo (es. \"hello\").\n\nFactor: Variabili categoriche (es. livelli come \"low\", \"medium\", \"high\").\n\nPuoi verificare il tipo di un oggetto con class() e controllare se appartiene a un tipo specifico con funzioni come is.numeric(). È anche possibile convertire un tipo in un altro con funzioni come as.character().\n\n7.14.2 Strutture di dati in R\nVettori: Contengono dati dello stesso tipo (es. numeri, stringhe o logici).\n\nmy_vec &lt;- c(1, 2, 3)\nmy_vec\n#&gt; [1] 1 2 3\n\nMatrici e array: Strutture bidimensionali (matrici) o multidimensionali (array) con dati dello stesso tipo.\nCreare una matrice:\n\nmy_mat &lt;- matrix(1:12, nrow = 3, byrow = TRUE)\nmy_mat\n#&gt;      [,1] [,2] [,3] [,4]\n#&gt; [1,]    1    2    3    4\n#&gt; [2,]    5    6    7    8\n#&gt; [3,]    9   10   11   12\n\nOperazioni utili:\n\n\nTrasposizione: t(my_mat)\n\n\nDiagonale: diag(my_mat)\n\n\nMoltiplicazione matriciale: mat1 %*% mat2\n\n\nListe: Possono contenere elementi di tipi diversi, inclusi vettori, matrici o altre liste.\n\nmy_list &lt;- list(\n  numbers = c(1, 2), \n  text = \"hello\", \n  mat = matrix(1:4, nrow = 2)\n)\nmy_list$numbers  # Accedi agli elementi con il nome\n#&gt; [1] 1 2\n\nData frame: Strutture bidimensionali che possono contenere colonne di tipi diversi. Ideale per dataset strutturati.\nCreare un data frame:\n\nheight &lt;- c(180, 155, 160)\nweight &lt;- c(65, 50, 52)\nnames &lt;- c(\"Joanna\", \"Charlotte\", \"Helen\")\n\ndataf &lt;- data.frame(height = height, weight = weight, names = names)\nstr(dataf)  # Mostra la struttura del data frame\n#&gt; 'data.frame':    3 obs. of  3 variables:\n#&gt;  $ height: num  180 155 160\n#&gt;  $ weight: num  65 50 52\n#&gt;  $ names : chr  \"Joanna\" \"Charlotte\" \"Helen\"\n\nPer convertire le stringhe in fattori durante la creazione:\n\ndataf &lt;- data.frame(\n  height = height, \n  weight = weight, \n  names = names, \n  stringsAsFactors = TRUE\n)\n\ndataf\n#&gt;   height weight     names\n#&gt; 1    180     65    Joanna\n#&gt; 2    155     50 Charlotte\n#&gt; 3    160     52     Helen\n\n\n7.14.3 Operazioni utili sui data frame\n\n\nVerificare dimensioni: dim(dataf)\n\n\nVisualizzare struttura: str(dataf)\n\n\nAccedere a colonne: dataf$height",
    "crumbs": [
      "R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Un approccio moderno all'analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/R/01_r_syntax.html#operazioni-di-base-in-r",
    "href": "chapters/R/01_r_syntax.html#operazioni-di-base-in-r",
    "title": "7  Un approccio moderno all’analisi dei dati",
    "section": "\n7.15 Operazioni di base in R",
    "text": "7.15 Operazioni di base in R\n\n7.15.1 Operazioni aritmetiche\nCome abbiamo visto in precedenza, R supporta le classiche operazioni aritmetiche come somma (+), sottrazione (-), moltiplicazione (*), divisione (/) ed esponenziazione (^).\n\n7.15.2 Operazioni logiche\nLe operazioni logiche in R includono:\n\n\n&: “and” logico\n\n\n|: “or” logico\n\n\n!: “not” logico\n\n\n&gt;: maggiore di\n\n\n&lt;: minore di\n\n\n==: uguale a\n\n\n!=: diverso da\n\nPer esempio:\n\n# Maggiore di\n3 &gt; 2\n#&gt; [1] TRUE\n# Uguale a\n3 == 2\n#&gt; [1] FALSE\n\n\nEsempio 7.4 Consideriamo l’esempio precedente, in cui abbiamo calcolato i punteggi totali dei 10 individui sulla Satisfaction With Life Scale (SWLS). Ora vogliamo determinare la proporzione di individui nel campione che ha ottenuto un punteggio totale maggiore di 15.\nI punteggi totali dei 10 individui sono memorizzati nella colonna PunteggioTotale del data frame risultati_swls:\n\nrisultati_swls$PunteggioTotale\n#&gt;  [1] 16 16 23 25 11  7 20 25 15 27\n\nPossiamo creare un vettore logico che indica, per ciascun individuo, se il suo punteggio totale supera 15. In R, l’operatore di confronto &gt; restituisce un valore TRUE se la condizione è soddisfatta e FALSE altrimenti:\n\nrisultati_swls$PunteggioTotale &gt; 15\n#&gt;  [1]  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE  TRUE\n\nIn R, i valori TRUE e FALSE possono essere trattati come numeri: TRUE equivale a 1 e FALSE equivale a 0. Questo ci permette di sommare i valori logici per contare quante volte la condizione è soddisfatta. Per calcolare la proporzione, dividiamo questa somma per il numero totale di individui:\n\nsum(risultati_swls$PunteggioTotale &gt; 15) / length(risultati_swls$PunteggioTotale)\n#&gt; [1] 0.7\n\nLa proporzione è calcolata come:\n\\[\n\\text{Proporzione} = \\frac{\\sum_{i=1}^{n} I(\\text{PunteggioTotale}_i &gt; 15)}{n} ,\n\\]\ndove:\n\n\n\\(n\\) è il numero totale di individui (in questo caso, 10),\n\n\\(I(\\text{PunteggioTotale}_i &gt; 15)\\) è una funzione indicatrice che vale 1 se il punteggio dell’individuo \\(i\\) è maggiore di 15, e 0 altrimenti.\n\nNel nostro esempio, la proporzione degli individui con punteggio totale maggiore di 15 è 0.7, cioè il 70% del campione soddisfa questa condizione.",
    "crumbs": [
      "R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Un approccio moderno all'analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/R/01_r_syntax.html#estrazione-di-sottoinsiemi-di-oggetti-in-r",
    "href": "chapters/R/01_r_syntax.html#estrazione-di-sottoinsiemi-di-oggetti-in-r",
    "title": "7  Un approccio moderno all’analisi dei dati",
    "section": "\n7.16 Estrazione di sottoinsiemi di oggetti in R",
    "text": "7.16 Estrazione di sottoinsiemi di oggetti in R\nIn R esistono tre operatori principali per estrarre sottoinsiemi di oggetti:\n\nOperatore [ ]\nQuesto operatore restituisce sempre un oggetto della stessa classe dell’originale. È utile per selezionare più elementi da un oggetto. È importante chiudere l’estrazione con ].\nOperatore [[ ]]\nQuesto operatore viene utilizzato per estrarre elementi da liste o data frame. A differenza di [ ], permette di estrarre un solo elemento alla volta e la classe dell’oggetto restituito non sarà necessariamente una lista o un data frame. L’estrazione va chiusa con ]].\nOperatore $\nCome visto in precedenza, questo operatore serve per estrarre elementi da una lista o un data frame utilizzando il loro nome letterale. Il comportamento semantico è simile a quello di [[ ]].\n\n\n7.16.1 Gli indici di un data frame in R\nIn R, gli indici di un data frame sono utilizzati per selezionare righe e colonne. La sintassi generale è:\ndf[i, j]\ndove:\n\n\ni rappresenta l’indice o gli indici delle righe,\n\n\nj rappresenta l’indice o gli indici delle colonne.\n\nSe uno degli indici viene omesso, si considerano tutte le righe o tutte le colonne, a seconda della dimensione omessa.\n\n7.16.1.1 Esempi pratici\n\n\nSelezione di righe specifiche su tutte le colonne\nSe vogliamo estrarre solo alcune righe, possiamo specificare gli indici delle righe nel primo argomento e lasciare vuoto il secondo. Ad esempio:\ndf[c(2, 3, 5), ]\nQuesto seleziona la seconda, terza e quinta riga del data frame df, includendo tutte le colonne.\n\n\nSelezione di colonne specifiche su tutte le righe\nPer selezionare solo alcune colonne, specifichiamo i loro indici nel secondo argomento e lasciamo vuoto il primo. Ad esempio:\ndf[, c(2, 3, 5)]\nQuesto seleziona la seconda, terza e quinta colonna del data frame df, includendo tutte le righe.\n\n\nSelezione di righe e colonne specifiche\nPossiamo combinare gli indici per selezionare una sotto-matrice specifica. Ad esempio:\ndf[c(2, 4), c(1, 3)]\nQuesto seleziona le righe 2 e 4 e le colonne 1 e 3.\n\n\n7.16.1.2 Ulteriori dettagli\n\n\nSelezione singola di riga o colonna\nSe vogliamo estrarre una singola riga o colonna, possiamo specificare un solo valore per i o j. Ad esempio:\ndf[1, ]  # Prima riga, tutte le colonne\ndf[, 2]  # Seconda colonna, tutte le righe\n\n\nUso di nomi invece di indici\nSe il data frame ha nomi per righe o colonne, possiamo utilizzarli per la selezione. Ad esempio:\ndf[\"nome_riga\", ]        # Seleziona la riga con nome \"nome_riga\"\ndf[, \"nome_colonna\"]     # Seleziona la colonna con nome \"nome_colonna\"\n\n\nSelezione logica\nPossiamo utilizzare un vettore logico per selezionare righe o colonne. Ad esempio, per selezionare le righe dove il valore nella prima colonna è maggiore di 10:\ndf[df[, 1] &gt; 10, ]\n\n\n7.16.1.3 Sintesi visiva\n\n\n\n\n\n\nSintassi\nDescrizione\n\n\n\ndf[i, ]\nSeleziona la riga i con tutte le colonne.\n\n\ndf[, j]\nSeleziona la colonna j con tutte le righe.\n\n\ndf[c(i1, i2), c(j1, j2)]\nSeleziona righe e colonne specifiche.\n\n\ndf[i, j]\nSeleziona l’intersezione di righe e colonne.\n\n\ndf[ , ]\nRestituisce l’intero data frame.\n\n\n\nQuesta flessibilità rende l’indicizzazione dei data frame in R potente ed efficace per manipolare e analizzare i dati.\n\nEsempio 7.5 Consideriamo il data frame iris incluso di default in base R.\n\niris |&gt; head()\n#&gt;   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#&gt; 1          5.1         3.5          1.4         0.2  setosa\n#&gt; 2          4.9         3.0          1.4         0.2  setosa\n#&gt; 3          4.7         3.2          1.3         0.2  setosa\n#&gt; 4          4.6         3.1          1.5         0.2  setosa\n#&gt; 5          5.0         3.6          1.4         0.2  setosa\n#&gt; 6          5.4         3.9          1.7         0.4  setosa\n\nUsiamo la funzione head() per stampare le prime 6 righe del data frame.\nL’istruzione seguente restituisce le prime tre colonne del dataset iris.\n\niris[, 1:3] |&gt; head()\n#&gt;   Sepal.Length Sepal.Width Petal.Length\n#&gt; 1          5.1         3.5          1.4\n#&gt; 2          4.9         3.0          1.4\n#&gt; 3          4.7         3.2          1.3\n#&gt; 4          4.6         3.1          1.5\n#&gt; 5          5.0         3.6          1.4\n#&gt; 6          5.4         3.9          1.7\n\nSelezione di colonne specifiche per nome:\n\niris[, c('Sepal.Length', 'Petal.Length')] |&gt; \n  head()  \n#&gt;   Sepal.Length Petal.Length\n#&gt; 1          5.1          1.4\n#&gt; 2          4.9          1.4\n#&gt; 3          4.7          1.3\n#&gt; 4          4.6          1.5\n#&gt; 5          5.0          1.4\n#&gt; 6          5.4          1.7\n\nSelezione di una singola colonna:\n\niris[, 'Petal.Length'] \n#&gt;   [1] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 1.5 1.6 1.4 1.1 1.2 1.5 1.3 1.4\n#&gt;  [19] 1.7 1.5 1.7 1.5 1.0 1.7 1.9 1.6 1.6 1.5 1.4 1.6 1.6 1.5 1.5 1.4 1.5 1.2\n#&gt;  [37] 1.3 1.4 1.3 1.5 1.3 1.3 1.3 1.6 1.9 1.4 1.6 1.4 1.5 1.4 4.7 4.5 4.9 4.0\n#&gt;  [55] 4.6 4.5 4.7 3.3 4.6 3.9 3.5 4.2 4.0 4.7 3.6 4.4 4.5 4.1 4.5 3.9 4.8 4.0\n#&gt;  [73] 4.9 4.7 4.3 4.4 4.8 5.0 4.5 3.5 3.8 3.7 3.9 5.1 4.5 4.5 4.7 4.4 4.1 4.0\n#&gt;  [91] 4.4 4.6 4.0 3.3 4.2 4.2 4.2 4.3 3.0 4.1 6.0 5.1 5.9 5.6 5.8 6.6 4.5 6.3\n#&gt; [109] 5.8 6.1 5.1 5.3 5.5 5.0 5.1 5.3 5.5 6.7 6.9 5.0 5.7 4.9 6.7 4.9 5.7 6.0\n#&gt; [127] 4.8 4.9 5.6 5.8 6.1 6.4 5.6 5.1 5.6 6.1 5.6 5.5 4.8 5.4 5.6 5.1 5.1 5.9\n#&gt; [145] 5.7 5.2 5.0 5.2 5.4 5.1\n\noppure\n\niris$Petal.Length\n#&gt;   [1] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 1.5 1.6 1.4 1.1 1.2 1.5 1.3 1.4\n#&gt;  [19] 1.7 1.5 1.7 1.5 1.0 1.7 1.9 1.6 1.6 1.5 1.4 1.6 1.6 1.5 1.5 1.4 1.5 1.2\n#&gt;  [37] 1.3 1.4 1.3 1.5 1.3 1.3 1.3 1.6 1.9 1.4 1.6 1.4 1.5 1.4 4.7 4.5 4.9 4.0\n#&gt;  [55] 4.6 4.5 4.7 3.3 4.6 3.9 3.5 4.2 4.0 4.7 3.6 4.4 4.5 4.1 4.5 3.9 4.8 4.0\n#&gt;  [73] 4.9 4.7 4.3 4.4 4.8 5.0 4.5 3.5 3.8 3.7 3.9 5.1 4.5 4.5 4.7 4.4 4.1 4.0\n#&gt;  [91] 4.4 4.6 4.0 3.3 4.2 4.2 4.2 4.3 3.0 4.1 6.0 5.1 5.9 5.6 5.8 6.6 4.5 6.3\n#&gt; [109] 5.8 6.1 5.1 5.3 5.5 5.0 5.1 5.3 5.5 6.7 6.9 5.0 5.7 4.9 6.7 4.9 5.7 6.0\n#&gt; [127] 4.8 4.9 5.6 5.8 6.1 6.4 5.6 5.1 5.6 6.1 5.6 5.5 4.8 5.4 5.6 5.1 5.1 5.9\n#&gt; [145] 5.7 5.2 5.0 5.2 5.4 5.1\n\nPer selezionare righe specifiche, definiamo gli indici corrispondenti. Per esempio, l’istruzione seguente restituisce le righe 1 e 3.\n\niris[c(1, 3), ]\n#&gt;   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#&gt; 1          5.1         3.5          1.4         0.2  setosa\n#&gt; 3          4.7         3.2          1.3         0.2  setosa\n\nFiltraggio logico di righe:\n\niris[iris$Species == 'versicolor', ] |&gt; head()\n#&gt;    Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n#&gt; 51          7.0         3.2          4.7         1.4 versicolor\n#&gt; 52          6.4         3.2          4.5         1.5 versicolor\n#&gt; 53          6.9         3.1          4.9         1.5 versicolor\n#&gt; 54          5.5         2.3          4.0         1.3 versicolor\n#&gt; 55          6.5         2.8          4.6         1.5 versicolor\n#&gt; 56          5.7         2.8          4.5         1.3 versicolor\n\nRestituisce le righe con Species uguale a “versicolor”. Numero di righe e colonne del sottoinsieme:\n\ndim(iris[iris$Species == 'versicolor', ])\n#&gt; [1] 50  5\n\n\n\n7.16.2 Filtraggio avanzato con operatori logici\nGli operatori logici & (AND), | (OR) e ! (NOT) permettono un filtraggio più sofisticato.\nEsempio: Filtrare le osservazioni di specie “versicolor” con lunghezza del sepalo non superiore a 5.0:\n\niris[(iris$Species == 'versicolor') & (iris$Sepal.Length &lt;= 5.0), ]\n#&gt;    Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n#&gt; 58          4.9         2.4          3.3           1 versicolor\n#&gt; 61          5.0         2.0          3.5           1 versicolor\n#&gt; 94          5.0         2.3          3.3           1 versicolor\n\nNumero di osservazioni trovate:\n\ndim(iris[(iris$Species == 'versicolor') & (iris$Sepal.Length &lt;= 5.0), ])\n#&gt; [1] 3 5",
    "crumbs": [
      "R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Un approccio moderno all'analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/R/01_r_syntax.html#riflessioni-conclusive",
    "href": "chapters/R/01_r_syntax.html#riflessioni-conclusive",
    "title": "7  Un approccio moderno all’analisi dei dati",
    "section": "Riflessioni conclusive",
    "text": "Riflessioni conclusive\nR non è soltanto un linguaggio di programmazione per la statistica, ma rappresenta una filosofia che si fonda su tre principi chiave: apertura, collaborazione e avanzamento della conoscenza scientifica.\nPer chi si avvicina a R, sia nel campo della comunicazione sia in altri ambiti, cogliere questa filosofia è essenziale per apprezzarne appieno il valore. R promuove non solo competenze tecniche, ma anche un impegno verso pratiche di ricerca trasparente e riproducibile, che costituiscono un pilastro fondamentale per una scienza rigorosa e affidabile.\nOpen Source\nR è un software open source, liberamente accessibile a tutti. Questo significa che chiunque può visualizzarne, modificarne e distribuirne il codice sorgente, promuovendo un ambiente trasparente e collaborativo. Essendo gratuito, R garantisce accessibilità a ricercatori di tutto il mondo, indipendentemente dal budget o dal supporto istituzionale. Inoltre, grazie alla sua natura aperta, R beneficia del contributo collettivo di una comunità globale eterogenea.\nContributi della Comunità\nLa comunità di R è uno dei suoi punti di forza principali. Statistici, ricercatori e data scientist di diverse discipline arricchiscono continuamente R sviluppando pacchetti: raccolte di funzioni, dati e codice che ampliano le sue funzionalità. Questa collaborazione ha portato alla creazione di migliaia di pacchetti che coprono tecniche statistiche, metodi grafici e strumenti per la manipolazione dei dati, rendendo R uno strumento sempre più versatile e adatto a un’ampia gamma di esigenze di ricerca.\nRicerca Riproducibile\nLa ricerca riproducibile consiste nel condurre studi in modo tale che altri possano replicarne i risultati utilizzando gli stessi dati e seguendo la stessa metodologia. Questo approccio è cruciale per la validazione delle scoperte scientifiche, permettendo la verifica dei risultati e la costruzione di nuove conoscenze su basi solide.\nR facilita la ricerca riproducibile grazie a:\n\nUn ecosistema completo di pacchetti per l’analisi dei dati e la generazione di report dinamici.\n\nStrumenti come R Markdown e Quarto, che permettono di integrare testo descrittivo e codice R in un unico documento. Questa integrazione consente di documentare ogni fase del processo di ricerca—dalla pulizia dei dati all’analisi e alla presentazione dei risultati—garantendo trasparenza e replicabilità.\n\nIn conclusione, comprendere la filosofia open source di R e il suo ruolo nella promozione della ricerca riproducibile fornisce un quadro chiaro del motivo per cui R è diventato uno strumento essenziale per ricercatori e statistici di diverse discipline. Per chi opera in psicologia, sfruttare le potenzialità di R significa produrre risultati di ricerca più trasparenti, replicabili e credibili, contribuendo alla robustezza e affidabilità della conoscenza scientifica nel settore.\n\n\n\n\n\n\nProblemi 1\n\n\n\n\n\nSvolgere gli esercizi da 1 a 38, sia in modo manuale che utilizzando R. Gli esercizi sono disponibili al seguente link:Esercizi su Summation Notation.\n\n\n\n\n\n\n\n\n\nProblemi 2\n\n\n\n\n\nIn questo esercizio, lavorerai con i dati della Satisfaction With Life Scale (SWLS) raccolti da ciascuno degli studenti del gruppo TPV di appartenenza.\nIstruzioni SWLS: Di seguito sono riportate alcune affermazioni con cui puoi descrivere la tua soddisfazione rispetto alla tua vita. Indica quanto sei d’accordo con ciascuna affermazione utilizzando la scala di risposta fornita.\n\nIl più delle volte la mia vita è vicina al mio ideale di vita.\nLe condizioni della mia vita sono eccellenti.\nSono soddisfatto/a della mia vita.\nFinora ho ottenuto le cose importanti che voglio dalla vita.\nSe io potessi rivivere la mia vita, non cambierei quasi nulla.\n\nLa SWLS utilizza una scala Likert a 7 punti, con i seguenti ancoraggi:\n\nFortemente in disaccordo\nDisaccordo\nLeggermente in disaccordo\nNé d’accordo né in disaccordo\nLeggermente d’accordo\nD’accordo\nFortemente d’accordo\n\nIl tuo compito sarà analizzare i dati raccolti sia manualmente su carta che utilizzando R.\nParte 1: Calcolo Manuale\n\n\nCalcolo del punteggio totale\n\nLa SWLS è composta da 5 item, ciascuno valutato su una scala Likert da 1 a 7.\nSomma i punteggi dei 5 item per ciascun partecipante per ottenere il punteggio totale.\n\nRegistra i punteggi totali su carta.\n\n\n\nDeterminazione della media del campione\n\nCalcola la media aritmetica dei punteggi totali dei 10 studenti.\nScrivi il calcolo e il risultato.\n\n\n\nCalcolo della deviazione standard\n\n\nCalcola la deviazione standard dei punteggi totali manualmente utilizzando la formula:\n\\[\ns^2 = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n-1}}.\n\\]\n\nRegistra il risultato.\n\n\n\nParte 2: Analisi con R\n\n\nCreazione del dataset in R\n\nInserisci i dati in R come un vettore chiamato swls_scores.\n\n\n\nCalcolo della media e della deviazione standard in R\n\nUsa le funzioni mean() e sd() per ottenere la media e la deviazione standard dei punteggi totali.\n\n\n\nVisualizzazione dei dati\n\nCrea un istogramma per visualizzare la distribuzione dei punteggi totali utilizzando hist(). Se non conosci l’istogramma, fai una ricerca su web; commenta il risultato ottenuto.\n\n\n\nIdentificazione dei punteggi superiori a 20\n\nUtilizza un’operazione logica per contare quanti partecipanti hanno un punteggio totale maggiore di 20.\n\n\n\nFiltraggio dei dati\n\nEstrai e visualizza solo i punteggi superiori alla media del campione.\n\n\n\nEsportazione dei risultati\n\nSalva i punteggi totali in un file CSV utilizzando la funzione write.csv().\n\n\n\nConsegna\n\nScrivi le risposte della Parte 1 su carta.\nScrivi il codice e i risultati della Parte 2 in un file .R e invialo come consegna.\n\n\n\n\n\n\n\n\n\n\nSoluzioni 2\n\n\n\n\n\nParte 1: Calcolo Manuale\n\n\nCalcolo del punteggio totale\n\n\nSupponiamo che i punteggi per 10 studenti siano:\n\n\nStudente\nItem 1\nItem 2\nItem 3\nItem 4\nItem 5\nTotale\n\n\n\n1\n5\n4\n6\n3\n2\n20\n\n\n2\n4\n2\n5\n3\n2\n16\n\n\n3\n6\n4\n6\n4\n3\n23\n\n\n4\n7\n6\n5\n3\n4\n25\n\n\n5\n3\n2\n3\n2\n1\n11\n\n\n6\n2\n1\n2\n1\n1\n7\n\n\n7\n5\n4\n5\n3\n3\n20\n\n\n8\n6\n5\n7\n4\n3\n25\n\n\n9\n4\n3\n4\n2\n2\n15\n\n\n10\n7\n6\n5\n5\n4\n27\n\n\n\n\n\n\n\nDeterminazione della media\n\n\nMedia:\n\\[\n\\bar{x} = \\frac{20+16+23+25+11+7+20+25+15+27}{10} = 18.9\n\\]\n\n\n\n\nCalcolo della deviazione standard\n\n\nLa deviazione standard è:\n\\[\ns = \\sqrt{\\frac{1}{9} \\sum (x_i - 18.9)^2} \\approx 6.56\n\\]\n\n\n\n\nParte 2: Analisi con R\n\n\nCreazione del dataset in R\nswls_scores &lt;- c(20, 16, 23, 25, 11, 7, 20, 25, 15, 27)\n\n\nCalcolo della media e della deviazione standard\nmean(swls_scores)  # Media\nsd(swls_scores)    # Deviazione standard\n\n\nVisualizzazione dei dati\nhist(swls_scores, main=\"Distribuzione SWLS\", xlab=\"Punteggi\", col=\"lightblue\", border=\"black\")\n\n\nIdentificazione dei punteggi superiori a 20\nsum(swls_scores &gt; 20)  # Numero di studenti con punteggio &gt; 20\n\n\nFiltraggio dei dati\nswls_scores[swls_scores &gt; mean(swls_scores)]\n\n\nEsportazione dei risultati\nwrite.csv(data.frame(Student=1:10, Score=swls_scores), \"swls_results.csv\", row.names=FALSE)\n\n\nConclusione Questi esercizi hanno permesso di confrontare il calcolo manuale con l’automatizzazione tramite R, facilitando l’analisi statistica della SWLS.\n\n\n\n\n\n\n\n\n\nInformazioni sull’ambiente di sviluppo\n\n\n\n\n\n\nsessionInfo()\n#&gt; R version 4.5.1 (2025-06-13)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.6.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] pillar_1.11.0         tinytable_0.13.0      patchwork_1.3.2      \n#&gt;  [4] ggdist_3.3.3          tidybayes_3.0.7       bayesplot_1.14.0     \n#&gt;  [7] ggplot2_3.5.2         reliabilitydiag_0.2.1 priorsense_1.1.1     \n#&gt; [10] posterior_1.6.1       loo_2.8.0             rstan_2.32.7         \n#&gt; [13] StanHeaders_2.32.10   brms_2.22.0           Rcpp_1.1.0           \n#&gt; [16] sessioninfo_1.2.3     conflicted_1.2.0      janitor_2.2.1        \n#&gt; [19] matrixStats_1.5.0     modelr_0.1.11         tibble_3.3.0         \n#&gt; [22] dplyr_1.1.4           tidyr_1.3.1           rio_1.2.3            \n#&gt; [25] here_1.0.1           \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;  [1] svUnit_1.0.8          tidyselect_1.2.1      farver_2.1.2         \n#&gt;  [4] fastmap_1.2.0         TH.data_1.1-4         tensorA_0.36.2.1     \n#&gt;  [7] pacman_0.5.1          digest_0.6.37         timechange_0.3.0     \n#&gt; [10] estimability_1.5.1    lifecycle_1.0.4       survival_3.8-3       \n#&gt; [13] magrittr_2.0.3        compiler_4.5.1        rlang_1.1.6          \n#&gt; [16] tools_4.5.1           knitr_1.50            bridgesampling_1.1-2 \n#&gt; [19] htmlwidgets_1.6.4     curl_7.0.0            pkgbuild_1.4.8       \n#&gt; [22] RColorBrewer_1.1-3    abind_1.4-8           multcomp_1.4-28      \n#&gt; [25] withr_3.0.2           purrr_1.1.0           grid_4.5.1           \n#&gt; [28] stats4_4.5.1          colorspace_2.1-1      xtable_1.8-4         \n#&gt; [31] inline_0.3.21         emmeans_1.11.2-8      scales_1.4.0         \n#&gt; [34] MASS_7.3-65           cli_3.6.5             mvtnorm_1.3-3        \n#&gt; [37] rmarkdown_2.29        ragg_1.5.0            generics_0.1.4       \n#&gt; [40] RcppParallel_5.1.11-1 cachem_1.1.0          stringr_1.5.1        \n#&gt; [43] splines_4.5.1         parallel_4.5.1        vctrs_0.6.5          \n#&gt; [46] V8_7.0.0              Matrix_1.7-4          sandwich_3.1-1       \n#&gt; [49] jsonlite_2.0.0        arrayhelpers_1.1-0    systemfonts_1.2.3    \n#&gt; [52] glue_1.8.0            codetools_0.2-20      distributional_0.5.0 \n#&gt; [55] lubridate_1.9.4       stringi_1.8.7         gtable_0.3.6         \n#&gt; [58] QuickJSR_1.8.0        htmltools_0.5.8.1     Brobdingnag_1.2-9    \n#&gt; [61] R6_2.6.1              textshaping_1.0.3     rprojroot_2.1.1      \n#&gt; [64] evaluate_1.0.5        lattice_0.22-7        backports_1.5.0      \n#&gt; [67] memoise_2.0.1         broom_1.0.9           snakecase_0.11.1     \n#&gt; [70] rstantools_2.5.0      coda_0.19-4.1         gridExtra_2.3        \n#&gt; [73] nlme_3.1-168          checkmate_2.3.3       xfun_0.53            \n#&gt; [76] zoo_1.8-14            pkgconfig_2.0.3",
    "crumbs": [
      "R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Un approccio moderno all'analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/R/01_r_syntax.html#bibliografia",
    "href": "chapters/R/01_r_syntax.html#bibliografia",
    "title": "7  Un approccio moderno all’analisi dei dati",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nCollaboration, O. S. (2015). Estimating the reproducibility of psychological science. Science, 349(6251), aac4716.\n\n\nIrizarry, R. A. (2024). Introduction to Data Science: Data Wrangling and Visualization with R. CRC Press.\n\n\nKnuth, D. E. (1984). Literate programming. The Computer Journal, 27(2), 97–111.\n\n\nObels, P., Lakens, D., Coles, N. A., Gottfried, J., & Green, S. A. (2020). Analysis of open data and computational reproducibility in registered reports in psychology. Advances in Methods and Practices in Psychological Science, 3(2), 229–237.\n\n\nOkoye, K., & Hosseini, S. (2024). Introduction to R Programming and RStudio Integrated Development Environment (IDE). In R Programming: Statistical Data Analysis in Research (pp. 3–24). Springer.\n\n\nWickham, H., Çetinkaya-Rundel, M., & Grolemund, G. (2023). R for data science. \" O’Reilly Media, Inc.\".",
    "crumbs": [
      "R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Un approccio moderno all'analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/R/02_utility_functions.html",
    "href": "chapters/R/02_utility_functions.html",
    "title": "8  Utility functions",
    "section": "",
    "text": "Introduzione\nIn questo capitolo, esploreremo le principali funzioni di utilità in R per l’importazione di dati da file esterni e la raccolta di statistiche descrittive, fornendo una panoramica generale sui data frame.",
    "crumbs": [
      "R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Utility functions</span>"
    ]
  },
  {
    "objectID": "chapters/R/02_utility_functions.html#introduzione",
    "href": "chapters/R/02_utility_functions.html#introduzione",
    "title": "8  Utility functions",
    "section": "",
    "text": "Panoramica del capitolo\n\nConoscere e sapere utilizzare le principali funzioni di utilità di R.\nSapere come importare un data set in R e esportare un data set in un file esterno.\nUsare i percorsi relativi rispetto alla radice del progetto con here::here().\n\n\n\n\n\n\n\nPrerequisiti\n\n\n\n\n\n\nConsultare Introduction to Data Science: Data Wrangling and Visualization with R (Irizarry, 2024)\n\nLeggere R for Data Science (2e) (Wickham et al., 2023).\n\n\n\n\n\n\n\n\n\n\nPreparazione del Notebook\n\n\n\n\n\n\nhere::here(\"code\", \"_common.R\") |&gt; \n  source()\n\n# Load packages\nif (!requireNamespace(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyr)",
    "crumbs": [
      "R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Utility functions</span>"
    ]
  },
  {
    "objectID": "chapters/R/02_utility_functions.html#importare-dati-in-r-con-rioimport",
    "href": "chapters/R/02_utility_functions.html#importare-dati-in-r-con-rioimport",
    "title": "8  Utility functions",
    "section": "\n8.1 Importare dati in R con rio::import()\n",
    "text": "8.1 Importare dati in R con rio::import()\n\nPrima di analizzare i dati, è necessario importarli in R.\n\n8.1.1 Il problema: Tanti Formati, un’Unica Soluzione\nNella ricerca psicologica i dati possono essere forniti in molti formati:\n\nFile Excel (.xlsx) da questionari compilati in laboratorio,\nCSV (.csv) da piattaforme online come Qualtrics,\nFile SPSS (.sav) per confrontare studi precedenti,\nSolo testo (.txt) da esperimenti comportamentali.\n\nInvece di imparare funzioni diverse, una specifica per ciascun formato, il pacchetto rio offre un solo comando universale per le importazioni.\n\n8.1.2 Come Funziona import()\n\n# Carica il pacchetto (installalo prima con install.packages(\"rio\"))\nlibrary(rio)\n\n# Importa un file CSV da una cartella \"dati\" nel tuo progetto\nrisposte &lt;- rio::import(\"dati/questionario.csv\")\n\n# Importa un foglio Excel con i tempi di reazione\ntempi_reazione &lt;- rio::import(\"dati/esperimento1.xlsx\")\n\n# Importa un file SPSS con dati demografici\ndati_demografici &lt;- rio::import(\"dati/partecipanti.sav\")\nPerché è utile:\n\nriconosce automaticamente il formato dal nome del file;\ntraduce i dati in un formato R pronto per l’analisi (data.frame);\nconserva le etichette delle variabili (cruciale per questionari!).\n\n8.1.3 Esportare Dati con rio::export()\n\nDopo aver pulito i dati, è possibile salvarli in qualsiasi formato usando rio::export():\nrio::export(risposte, \"dati/cleaned/dati_puliti.xlsx\")\nrio::export(tempi_reazione, \"dati/cleaned/tempi_reazione.sav\")",
    "crumbs": [
      "R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Utility functions</span>"
    ]
  },
  {
    "objectID": "chapters/R/02_utility_functions.html#utilizzare-percorsi-relativi-con-herehere",
    "href": "chapters/R/02_utility_functions.html#utilizzare-percorsi-relativi-con-herehere",
    "title": "8  Utility functions",
    "section": "\n8.2 Utilizzare Percorsi Relativi con here::here()\n",
    "text": "8.2 Utilizzare Percorsi Relativi con here::here()\n\nQuando importiamo i dati da file esterni in R, succede spesso di commettere uno dei tre errori seguenti. Vediamo come eviarli.\n\n\nPercatori sbagliati\n# SBAGLIATO (il file non è nella cartella di lavoro)\nimport(\"questionario.csv\")  \n\n# CORRETTO: usa percorsi relativi o il pacchetto 'here'\nimport(\"dati/raw/questionario.csv\")  \n\nFile aperti in altri programmi\n“Errore: non posso aprire il file” → Chiudi Excel/SPSS e riprova\n\nCodifica caratteri strani\nSe vedi � nei testi, specifica l’encoding:\nimport(\"dati/testo.txt\", encoding = \"UTF-8\")\n\n\n\n8.2.1 Evitare Percorsi Assoluti\nCome vedremo meglio nel Capitolo 14, il primo passo di un progetto di analisi dei dati è l’organizzazione dei file in cartelle con una struttura chiara:\ntuo_progetto/\n├── dati/\n│   ├── raw/        # Dati originali\n│   └── cleaned/    # Dati elaborati\n├── script/\n└── rapporti/\nTutti i file e le cartelle devono essere contenuti nella directory del progetto.\nIl pacchetto here rende l’importazione dei dati più semplice, evitando problemi dovuti a percorsi assoluti che possono cambiare se si modifica la directory di lavoro o si sposta il progetto.\nLa funzione here() crea percorsi relativi a partire dalla radice del progetto (cioè dalla cartella che contiene il file .Rproj o da dove viene inizializzato il progetto RStudio).\nEsempio di utilizzo combinato con rio::import():\nlibrary(rio)\nlibrary(here)\n\n# Percorso robusto al file csv\ndati &lt;- import(here(\"data\", \"dati.csv\"))\n\n# Percorso robusto al file Excel\ndati_excel &lt;- import(here(\"data\", \"dati.xlsx\"))\nIn questo modo, l’importazione diventa indipendente dalla cartella di lavoro attuale e il codice sarà più facilmente condivisibile e riproducibile.\nVantaggi:\n\n\nSemplicità: rio::import() riconosce automaticamente il tipo di file.\n\nRobustezza: here::here() garantisce che il percorso sia sempre corretto, indipendentemente da dove viene eseguito lo script.\n\nQuesta combinazione rende le analisi riproducibili e consente di collaborare facilmente con altri ricercatori o studenti.",
    "crumbs": [
      "R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Utility functions</span>"
    ]
  },
  {
    "objectID": "chapters/R/02_utility_functions.html#funzioni-principali-e-loro-utilizzo",
    "href": "chapters/R/02_utility_functions.html#funzioni-principali-e-loro-utilizzo",
    "title": "8  Utility functions",
    "section": "\n8.3 Funzioni Principali e Loro Utilizzo",
    "text": "8.3 Funzioni Principali e Loro Utilizzo\nR offre una serie di funzioni per esplorare rapidamente i dati e comprenderne la struttura prima di passare a manipolazioni più avanzate.\n\n\n\n\n\n\nFunzione\nDescrizione\n\n\n\nsummary()\nRestituisce statistiche descrittive di base per ogni colonna di un data frame. Per le colonne numeriche, calcola valori come il minimo, massimo, media, mediana, primo e terzo quartile, e il numero di valori mancanti (se presenti). Per le colonne non numeriche, restituisce il tipo di dati (carattere, logico) e il conteggio delle categorie. Esempio: summary(iris) restituisce una sintesi delle colonne del dataset iris.\n\n\n\nstr() e glimpse()\n\nForniscono una rappresentazione sintetica delle informazioni di un data frame, come dimensione, nomi delle colonne, tipi di dati e valori iniziali. La funzione str() fa parte della configurazione base di R (pacchetto utils), mentre glimpse() è inclusa in dplyr (pacchetto tidyverse). Esempio: str(mtcars) o glimpse(mtcars).\n\n\n\nhead() e tail()\n\nPermettono di visualizzare rispettivamente le prime o ultime righe di un data frame. Utile per una rapida ispezione del contenuto. Si può specificare il numero di righe da mostrare (es. head(df, 10)), altrimenti il valore predefinito è sei righe. Esempio: head(iris) per vedere le prime righe del dataset iris.\n\n\n\nView() e view()\n\nVisualizzano un data frame in una finestra grafica tipo foglio di calcolo all’interno di RStudio. La funzione View() è parte della configurazione base di R, mentre view() è un alias fornito da tibble (pacchetto tidyverse). Utile per piccoli data frame, ma poco pratico per dataset di grandi dimensioni. Esempio: View(iris) apre il dataset iris nel visualizzatore di RStudio.\n\n\nunique()\nRestituisce i valori unici presenti in una colonna o in un vettore. Esempio: unique(iris$Species) restituisce le specie uniche nel dataset iris.\n\n\nnames()\nRestituisce i nomi delle colonne di un data frame. Esempio: names(mtcars) restituisce i nomi delle colonne del dataset mtcars.\n\n\nclass()\nIndica il tipo di dato di un oggetto in R, come numeric, character, logical, o data.frame. Esempio: class(iris) restituisce data.frame.\n\n\nlength()\nRestituisce il numero di elementi di un oggetto. Per i data frame, restituisce il numero di colonne. Esempio: length(iris) restituisce 5 (colonne).\n\n\n\nnrow() e ncol()\n\nRestituiscono rispettivamente il numero di righe e colonne di un data frame. Esempio: nrow(iris) restituisce 150 (righe), mentre ncol(iris) restituisce 5 (colonne).",
    "crumbs": [
      "R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Utility functions</span>"
    ]
  },
  {
    "objectID": "chapters/R/02_utility_functions.html#illustrazione",
    "href": "chapters/R/02_utility_functions.html#illustrazione",
    "title": "8  Utility functions",
    "section": "\n8.4 Illustrazione",
    "text": "8.4 Illustrazione\nImmagina di dover analizzare i dati del tuo esperimento sul sonno e la memoria, salvati nel file msleep.csv. La struttura del tuo progetto RStudio è organizzata così:\nmio_esperimento/\n├── mio_esperimento.Rproj\n├── data/\n│   └── msleep.csv\n├── script/\n│   └── analisi.R\n└── output/\nLa prima cosa da fare è caricare i pacchetti necessari:\nlibrary(rio)    # Per importare i dati\nlibrary(here)   # Per gestire i percorsi in modo affidabile\nA questo punto possiamo importare i dati:\n\nmsleep &lt;- rio::import(\n  here::here(  # Costruisce il percorso automaticamente\n    \"data\",    # Cartella dei dati\n    \"msleep.csv\"  # Nome del file\n  )\n)\n\nControlli post-importazione (fondamentali!)\n\nhead(msleep)\n#&gt;                         name      genus  vore        order conservation\n#&gt; 1                    Cheetah   Acinonyx carni    Carnivora           lc\n#&gt; 2                 Owl monkey      Aotus  omni     Primates             \n#&gt; 3            Mountain beaver Aplodontia herbi     Rodentia           nt\n#&gt; 4 Greater short-tailed shrew    Blarina  omni Soricomorpha           lc\n#&gt; 5                        Cow        Bos herbi Artiodactyla domesticated\n#&gt; 6           Three-toed sloth   Bradypus herbi       Pilosa             \n#&gt;   sleep_total sleep_rem sleep_cycle awake brainwt  bodywt\n#&gt; 1        12.1        NA          NA  11.9      NA  50.000\n#&gt; 2        17.0       1.8          NA   7.0 0.01550   0.480\n#&gt; 3        14.4       2.4          NA   9.6      NA   1.350\n#&gt; 4        14.9       2.3       0.133   9.1 0.00029   0.019\n#&gt; 5         4.0       0.7       0.667  20.0 0.42300 600.000\n#&gt; 6        14.4       2.2       0.767   9.6      NA   3.850\n\n\nstr(msleep)\n#&gt; 'data.frame':    83 obs. of  11 variables:\n#&gt;  $ name        : chr  \"Cheetah\" \"Owl monkey\" \"Mountain beaver\" \"Greater short-tailed shrew\" ...\n#&gt;  $ genus       : chr  \"Acinonyx\" \"Aotus\" \"Aplodontia\" \"Blarina\" ...\n#&gt;  $ vore        : chr  \"carni\" \"omni\" \"herbi\" \"omni\" ...\n#&gt;  $ order       : chr  \"Carnivora\" \"Primates\" \"Rodentia\" \"Soricomorpha\" ...\n#&gt;  $ conservation: chr  \"lc\" \"\" \"nt\" \"lc\" ...\n#&gt;  $ sleep_total : num  12.1 17 14.4 14.9 4 14.4 8.7 7 10.1 3 ...\n#&gt;  $ sleep_rem   : num  NA 1.8 2.4 2.3 0.7 2.2 1.4 NA 2.9 NA ...\n#&gt;  $ sleep_cycle : num  NA NA NA 0.133 0.667 ...\n#&gt;  $ awake       : num  11.9 7 9.6 9.1 20 9.6 15.3 17 13.9 21 ...\n#&gt;  $ brainwt     : num  NA 0.0155 NA 0.00029 0.423 NA NA NA 0.07 0.0982 ...\n#&gt;  $ bodywt      : num  50 0.48 1.35 0.019 600 ...\n\n\nglimpse(msleep)\n#&gt; Rows: 83\n#&gt; Columns: 11\n#&gt; $ name         &lt;chr&gt; \"Cheetah\", \"Owl monkey\", \"Mountain beaver\", \"Greater shor…\n#&gt; $ genus        &lt;chr&gt; \"Acinonyx\", \"Aotus\", \"Aplodontia\", \"Blarina\", \"Bos\", \"Bra…\n#&gt; $ vore         &lt;chr&gt; \"carni\", \"omni\", \"herbi\", \"omni\", \"herbi\", \"herbi\", \"carn…\n#&gt; $ order        &lt;chr&gt; \"Carnivora\", \"Primates\", \"Rodentia\", \"Soricomorpha\", \"Art…\n#&gt; $ conservation &lt;chr&gt; \"lc\", \"\", \"nt\", \"lc\", \"domesticated\", \"\", \"vu\", \"\", \"dome…\n#&gt; $ sleep_total  &lt;dbl&gt; 12.1, 17.0, 14.4, 14.9, 4.0, 14.4, 8.7, 7.0, 10.1, 3.0, 5…\n#&gt; $ sleep_rem    &lt;dbl&gt; NA, 1.8, 2.4, 2.3, 0.7, 2.2, 1.4, NA, 2.9, NA, 0.6, 0.8, …\n#&gt; $ sleep_cycle  &lt;dbl&gt; NA, NA, NA, 0.133, 0.667, 0.767, 0.383, NA, 0.333, NA, NA…\n#&gt; $ awake        &lt;dbl&gt; 11.9, 7.0, 9.6, 9.1, 20.0, 9.6, 15.3, 17.0, 13.9, 21.0, 1…\n#&gt; $ brainwt      &lt;dbl&gt; NA, 0.01550, NA, 0.00029, 0.42300, NA, NA, NA, 0.07000, 0…\n#&gt; $ bodywt       &lt;dbl&gt; 50.000, 0.480, 1.350, 0.019, 600.000, 3.850, 20.490, 0.04…\n\n\nnames(msleep)\n#&gt;  [1] \"name\"         \"genus\"        \"vore\"         \"order\"        \"conservation\"\n#&gt;  [6] \"sleep_total\"  \"sleep_rem\"    \"sleep_cycle\"  \"awake\"        \"brainwt\"     \n#&gt; [11] \"bodywt\"\n\n\ndim(msleep)\n#&gt; [1] 83 11\n\nErrori comuni e soluzioni.\n\n\n“File not found”:\n\nVerifica che:\n\nil file sia realmente in data/;\nil nome del file sia esatto (attenzione a .csv vs .CSV);\n\nnon ci siano spazi nel nome del file.\n\n\n\n\n\nPacchetti non installati:\n# Esegui una volta\ninstall.packages(\"rio\")\ninstall.packages(\"here\")\n\n\nProgetto non aperto:\n\nAssicurati di aver aperto il file .Rproj prima di iniziare.\n\n\n\nEsaminiamo le modalità della variabile qualitativa vore:\n\nunique(msleep$vore)\n#&gt; [1] \"carni\"   \"omni\"    \"herbi\"   \"\"        \"insecti\"\n\nSe vogliamo la numerosità di ciascuna categoria, possiamo usare table():\n\ntable(msleep$vore)\n#&gt; \n#&gt;           carni   herbi insecti    omni \n#&gt;       7      19      32       5      20\n\nSi noti che table() esclude i dati mancanti.\nStampiamo i nomi delle colonne del data frame:\n\nnames(msleep)\n#&gt;  [1] \"name\"         \"genus\"        \"vore\"         \"order\"        \"conservation\"\n#&gt;  [6] \"sleep_total\"  \"sleep_rem\"    \"sleep_cycle\"  \"awake\"        \"brainwt\"     \n#&gt; [11] \"bodywt\"\n\nEsaminiamo il tipo di variabile della colonna vore:\n\nclass(msleep$vore)\n#&gt; [1] \"character\"\n\nLe dimensioni del data frame sono date da:\n\ndim(msleep)\n#&gt; [1] 83 11\n\nladdove il primo valore è il numero di righe e il secondo valore è il numero di colonne.\nIl numero di elementi di un vettore è dato da:\n\nlength(msleep$vore)\n#&gt; [1] 83\n\nIn alternativa, possiamo usare nrow()\n\nnrow(msleep)\n#&gt; [1] 83\n\nper il numero di righe e ncol()\n\nncol(msleep)\n#&gt; [1] 11\n\nper il numero di colonne. In maniera equivalente:\n\ndim(msleep)[2]\n#&gt; [1] 11\n\n\n\n\n\n\n\nEsercizio\n\n\n\n\n\nIn questo esercizio, utilizzerai R per esplorare i dati raccolti con il questionario Satisfaction With Life Scale (SWLS) dagli studenti del tuo gruppo TPV. L’obiettivo è familiarizzare con le funzioni di base di R per caricare, visualizzare e manipolare i dati.\nParte 1: Operazioni Manuali\n\n\nCreazione e gestione degli oggetti in R\n\nScrivi su carta i comandi R che creerebbero un oggetto chiamato swls_scores contenente i punteggi di 10 studenti.\nQuali sono le regole per assegnare un nome a un oggetto in R?\n\n\n\nVisualizzazione dei dati\n\nScrivi il comando R per visualizzare il contenuto dell’oggetto swls_scores.\nCome puoi visualizzare solo i primi 5 valori del vettore?\n\n\n\nEsplorazione della struttura dei dati\n\nScrivi i comandi R per verificare il tipo di dati contenuti in swls_scores.\nCome puoi verificare quanti elementi contiene?\n\n\n\nParte 2: Esecuzione in R\n\n\nCreazione del dataset in R\n\nInserisci i dati in un oggetto chiamato swls_scores in R.\n\n\n\nVerifica della struttura dei dati\n\nUsa le funzioni str(), class(), length(), nrow(), ncol() su swls_scores.\nAnnota i risultati e spiega a parole loro significato.\n\n\n\nVisualizzazione dei dati\n\nUsa head() e tail() per esplorare i dati.\n\nQual è la differenza tra le due funzioni?\n\n\n\nIdentificazione dei valori unici\n\nUsa unique(swls_scores) per individuare i punteggi distinti.\n\n\n\nCreazione di una tabella con i dati\n\nTrasforma swls_scores in un data frame con una colonna \"Punteggio\" e una colonna \"Studente\" (numerata da 1 a 10).\n\n\n\nEsportazione dei dati\n\nSalva il data frame in un file CSV chiamato \"swls_data.csv\" usando write.csv().\n\n\n\nConsegna\n\nScrivi le risposte della Parte 1 su carta.\nScrivi il codice e i risultati della Parte 2 in un file .R e invialo come consegna.\n\n\n\n\n\n\n\n\n\n\nSoluzione\n\n\n\n\n\nParte 1: Operazioni Manuali\n\n\nCreazione e gestione degli oggetti in R\n\n\nConsideriamo dei valori di risposta arbitrari. Il comando per creare l’oggetto swls_scores è:\nswls_scores &lt;- c(20, 16, 23, 25, 11, 7, 20, 25, 15, 27)\n\n\nRegole per assegnare un nome a un oggetto in R:\n\nNon può iniziare con un numero.\nNon può contenere spazi o caratteri speciali (tranne _ e .).\nNon deve avere lo stesso nome di funzioni già esistenti.\n\n\n\n\n\nVisualizzazione dei dati\n\n\nPer visualizzare il contenuto:\nswls_scores\n\n\nPer visualizzare solo i primi 5 valori:\nhead(swls_scores, 5)\n\n\n\n\nEsplorazione della struttura dei dati\n\n\nPer verificare il tipo di dati:\nclass(swls_scores)\n\n\nPer verificare il numero di elementi:\nlength(swls_scores)\n\n\n\n\nParte 2: Esecuzione in R\n\n\nCreazione del dataset in R\nswls_scores &lt;- c(20, 16, 23, 25, 11, 7, 20, 25, 15, 27)\n\n\nVerifica della struttura dei dati\nstr(swls_scores)\nclass(swls_scores)\nlength(swls_scores)\n\n\nstr() mostra che swls_scores è un vettore numerico.\n\nclass() conferma che è di tipo \"numeric\".\n\nlength() indica che il vettore ha 10 elementi.\n\n\n\nVisualizzazione dei dati\nhead(swls_scores)\ntail(swls_scores)\n\n\nhead() mostra i primi 6 elementi, tail() gli ultimi 6.\n\n\n\nIdentificazione dei valori unici\nunique(swls_scores)\n\nRestituisce: 7, 11, 15, 16, 20, 23, 25, 27.\n\n\n\nCreazione di una tabella con i dati\ndf_swls &lt;- data.frame(Studente = 1:10, Punteggio = swls_scores)\ndf_swls\n\n\nEsportazione dei dati\nwrite.csv(df_swls, \"swls_data.csv\", row.names=FALSE)\n\n\nConclusione\nQuesti esercizi hanno introdotto i comandi di base per creare, visualizzare e manipolare dati in R.\n\n\n\n\n\n\n\n\n\nInformazioni sull’ambiente di sviluppo\n\n\n\n\n\n\nsessionInfo()\n#&gt; R version 4.5.1 (2025-06-13)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.6.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] pillar_1.11.0         tinytable_0.13.0      patchwork_1.3.2      \n#&gt;  [4] ggdist_3.3.3          tidybayes_3.0.7       bayesplot_1.14.0     \n#&gt;  [7] ggplot2_3.5.2         reliabilitydiag_0.2.1 priorsense_1.1.1     \n#&gt; [10] posterior_1.6.1       loo_2.8.0             rstan_2.32.7         \n#&gt; [13] StanHeaders_2.32.10   brms_2.22.0           Rcpp_1.1.0           \n#&gt; [16] sessioninfo_1.2.3     conflicted_1.2.0      janitor_2.2.1        \n#&gt; [19] matrixStats_1.5.0     modelr_0.1.11         tibble_3.3.0         \n#&gt; [22] dplyr_1.1.4           tidyr_1.3.1           rio_1.2.3            \n#&gt; [25] here_1.0.1           \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;  [1] gridExtra_2.3         inline_0.3.21         sandwich_3.1-1       \n#&gt;  [4] rlang_1.1.6           magrittr_2.0.3        multcomp_1.4-28      \n#&gt;  [7] snakecase_0.11.1      compiler_4.5.1        systemfonts_1.2.3    \n#&gt; [10] vctrs_0.6.5           stringr_1.5.1         pkgconfig_2.0.3      \n#&gt; [13] arrayhelpers_1.1-0    fastmap_1.2.0         backports_1.5.0      \n#&gt; [16] rmarkdown_2.29        ragg_1.5.0            purrr_1.1.0          \n#&gt; [19] xfun_0.53             cachem_1.1.0          jsonlite_2.0.0       \n#&gt; [22] broom_1.0.9           parallel_4.5.1        R6_2.6.1             \n#&gt; [25] stringi_1.8.7         RColorBrewer_1.1-3    lubridate_1.9.4      \n#&gt; [28] estimability_1.5.1    knitr_1.50            zoo_1.8-14           \n#&gt; [31] pacman_0.5.1          R.utils_2.13.0        Matrix_1.7-4         \n#&gt; [34] splines_4.5.1         timechange_0.3.0      tidyselect_1.2.1     \n#&gt; [37] abind_1.4-8           codetools_0.2-20      curl_7.0.0           \n#&gt; [40] pkgbuild_1.4.8        lattice_0.22-7        withr_3.0.2          \n#&gt; [43] bridgesampling_1.1-2  coda_0.19-4.1         evaluate_1.0.5       \n#&gt; [46] survival_3.8-3        RcppParallel_5.1.11-1 tensorA_0.36.2.1     \n#&gt; [49] checkmate_2.3.3       stats4_4.5.1          distributional_0.5.0 \n#&gt; [52] generics_0.1.4        rprojroot_2.1.1       rstantools_2.5.0     \n#&gt; [55] scales_1.4.0          xtable_1.8-4          glue_1.8.0           \n#&gt; [58] emmeans_1.11.2-8      tools_4.5.1           data.table_1.17.8    \n#&gt; [61] mvtnorm_1.3-3         grid_4.5.1            QuickJSR_1.8.0       \n#&gt; [64] colorspace_2.1-1      nlme_3.1-168          cli_3.6.5            \n#&gt; [67] textshaping_1.0.3     svUnit_1.0.8          Brobdingnag_1.2-9    \n#&gt; [70] V8_7.0.0              gtable_0.3.6          R.methodsS3_1.8.2    \n#&gt; [73] digest_0.6.37         TH.data_1.1-4         htmlwidgets_1.6.4    \n#&gt; [76] farver_2.1.2          memoise_2.0.1         htmltools_0.5.8.1    \n#&gt; [79] R.oo_1.27.1           lifecycle_1.0.4       MASS_7.3-65",
    "crumbs": [
      "R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Utility functions</span>"
    ]
  },
  {
    "objectID": "chapters/R/02_utility_functions.html#bibliografia",
    "href": "chapters/R/02_utility_functions.html#bibliografia",
    "title": "8  Utility functions",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nIrizarry, R. A. (2024). Introduction to Data Science: Data Wrangling and Visualization with R. CRC Press.\n\n\nWickham, H., Çetinkaya-Rundel, M., & Grolemund, G. (2023). R for data science. \" O’Reilly Media, Inc.\".",
    "crumbs": [
      "R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Utility functions</span>"
    ]
  },
  {
    "objectID": "chapters/R/03_r_programming.html",
    "href": "chapters/R/03_r_programming.html",
    "title": "9  Programmazione",
    "section": "",
    "text": "Introduzione\nIn questo capitolo esploreremo tre strumenti fondamentali per la scrittura di codice in R: le funzioni, le istruzioni condizionali e i cicli. Questi elementi costituiscono la base per sviluppare script flessibili, efficienti e riutilizzabili, essenziali per ogni programmatore o analista che utilizza R.",
    "crumbs": [
      "R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Programmazione</span>"
    ]
  },
  {
    "objectID": "chapters/R/03_r_programming.html#introduzione",
    "href": "chapters/R/03_r_programming.html#introduzione",
    "title": "9  Programmazione",
    "section": "",
    "text": "Prerequisiti\n\n\n\n\n\n\nConsultare Introduction to Data Science: Data Wrangling and Visualization with R (Irizarry, 2024)\n\nLeggere R for Data Science (2e).\nConsultare la Tidyverse style guide.\n\n\n\n\n\n\n\n\n\n\nPreparazione del Notebook\n\n\n\n\n\n\nhere::here(\"code\", \"_common.R\") |&gt; \n  source()\n\n# Load packages\nif (!requireNamespace(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyr)",
    "crumbs": [
      "R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Programmazione</span>"
    ]
  },
  {
    "objectID": "chapters/R/03_r_programming.html#funzioni",
    "href": "chapters/R/03_r_programming.html#funzioni",
    "title": "9  Programmazione",
    "section": "\n9.1 Funzioni",
    "text": "9.1 Funzioni\nR offre un’ampia gamma di funzioni integrate per supportare l’analisi statistica, la manipolazione dei dati e la visualizzazione grafica, rendendolo uno strumento estremamente versatile per diverse esigenze.\nEsempi di funzioni comuni includono:\n\n# Sommare numeri\nsum(1, 2, 3)  # Restituisce la somma dei numeri\n#&gt; [1] 6\n\n\n# Creare un grafico semplice\nplot(1:10, 1:10)  # Crea un grafico a dispersione dei valori\n\n\n\n\n\n\n\nIn sostanza, una funzione è un blocco di codice progettato per svolgere un’operazione specifica. Puoi pensare a una funzione come a una “black box”: fornisci un input (i dati), la funzione elabora l’informazione attraverso le sue istruzioni e restituisce un output (il risultato). Questo approccio modulare semplifica il lavoro, permettendo di riutilizzare e combinare facilmente diverse operazioni.\n\n9.1.1 Creare Funzioni Personalizzate\nLa creazione di funzioni personalizzate in R è uno strumento essenziale per migliorare la programmazione, soprattutto per gestire operazioni ripetitive o complesse. Le funzioni consentono di rendere il codice più leggibile, efficiente e riutilizzabile, promuovendo un approccio organizzato e chiaro alla risoluzione dei problemi.\n\n9.1.1.1 Vantaggi delle Funzioni Personalizzate\nL’uso di funzioni personalizzate offre numerosi benefici:\n\n\nChiarezza e leggibilità: Un nome descrittivo permette di comprendere immediatamente lo scopo della funzione, anche a distanza di tempo o per altri utenti che leggono il codice.\n\n\nManutenzione semplificata: Modificare il codice all’interno di una funzione aggiorna automaticamente tutte le sue occorrenze, riducendo il rischio di errori e semplificando il debugging.\n\n\nRiduzione degli errori: Si evitano gli errori tipici del copia-e-incolla, come omissioni o incoerenze nei programmi complessi.\n\n\nRiutilizzabilità: Una funzione ben progettata può essere utilizzata in più contesti o progetti, risparmiando tempo e sforzi.\n\n9.1.1.2 Quando Creare una Funzione?\nUn buon criterio per decidere se creare una funzione è osservare se il medesimo blocco di codice viene copiato più volte. Se ti trovi a ripetere lo stesso codice più di due volte, probabilmente è il momento di creare una funzione. Questo aiuta a scrivere codice più pulito, scalabile e professionale, migliorando anche la sostenibilità del lavoro a lungo termine.\n\n9.1.2 Sintassi di una Funzione\nLa struttura base di una funzione in R è la seguente:\nnome_funzione &lt;- function(argomenti) {\n  # Corpo della funzione\n  codice\n  return(risultato)  # Facoltativo: restituisce il valore calcolato\n}\n\n\nnome_funzione: Nome della funzione, scelto per descrivere chiaramente la sua finalità.\n\n\nargomenti: Parametri necessari per eseguire le operazioni all’interno della funzione.\n\n\ncodice: Le istruzioni che definiscono il comportamento della funzione.\n\n\nrisultato: Il valore restituito dalla funzione. Se non si usa return(), R restituisce l’ultimo valore calcolato.\n\n\nEsempio 9.1 Immaginiamo di voler creare una funzione per sommare due numeri.\nsomma_due &lt;- function(a, b) {\n  a + b  # Restituisce la somma dei due numeri\n}\nPer utilizzarla, basta richiamarla specificando i parametri:\nsomma_due(5, 3)  # Restituisce 8\nQuesto approccio aiuta a scrivere codice più leggibile e facile da gestire. Ad esempio, se in futuro volessi modificare il comportamento della somma (ad esempio, aggiungere un messaggio di log), basterà intervenire solo all’interno della funzione.\n\n\nEsempio 9.2 Immaginiamo di avere un dataset con i punteggi di 10 individui su 3 subscale di un test psicometrico. L’obiettivo è:\n\nCreare una funzione per calcolare il punteggio totale di un individuo.\nCreare una funzione per trovare il massimo punteggio totale nel campione.\nCreare una funzione per individuare chi ha ottenuto il massimo punteggio.\n\nPasso 1: Simulazione dei Dati. Simuliamo i punteggi di 10 individui su 3 subscale:\n\n# Simulazione dei punteggi\nset.seed(123)\npunteggi &lt;- data.frame(\n  individuo = paste(\"Individuo\", 1:10),\n  subscale1 = sample(30:50, 10, replace = TRUE),\n  subscale2 = sample(40:60, 10, replace = TRUE),\n  subscale3 = sample(35:55, 10, replace = TRUE)\n)\nprint(punteggi)\n#&gt;       individuo subscale1 subscale2 subscale3\n#&gt; 1   Individuo 1        44        44        48\n#&gt; 2   Individuo 2        48        58        51\n#&gt; 3   Individuo 3        43        48        45\n#&gt; 4   Individuo 4        32        42        41\n#&gt; 5   Individuo 5        39        47        55\n#&gt; 6   Individuo 6        47        46        46\n#&gt; 7   Individuo 7        40        49        49\n#&gt; 8   Individuo 8        34        48        44\n#&gt; 9   Individuo 9        49        58        47\n#&gt; 10 Individuo 10        43        43        41\n\nLa funzione sample() in R è utilizzata per estrarre casualmente un sottoinsieme di valori da un vettore. Nell’esempio sopra, sample() viene utilizzata per generare casualmente i punteggi delle subscale dei test psicometrici.\nNell’istruzione subscale1 &lt;- sample(30:50, 10, replace = TRUE)\n\n\n30:50: Rappresenta il vettore di numeri interi da cui vengono estratti i punteggi (valori possibili tra 30 e 50).\n\n10: Indica che vogliamo estrarre 10 valori.\n\nreplace = TRUE: Consente che lo stesso valore possa essere estratto più volte (estrazione con ripetizione).\n\nPasso 2: Creazione delle Funzioni.\n\n\nCalcolo del punteggio totale per ogni individuo\nQuesta funzione somma i punteggi delle subscale di un individuo:\n\ncalcola_totale &lt;- function(subscale1, subscale2, subscale3) {\n  return(subscale1 + subscale2 + subscale3)\n}\n\n\n\nTrovare il punteggio massimo nel campione\nQuesta funzione accetta un vettore di punteggi totali e restituisce il valore massimo:\n\ntrova_massimo &lt;- function(punteggi_totali) {\n  return(max(punteggi_totali))\n}\n\n\n\nIndividuare l’individuo con il punteggio massimo\nQuesta funzione accetta un data frame con i punteggi e restituisce il nome dell’individuo con il punteggio più alto:\n\ntrova_individuo_massimo &lt;- function(punteggi) {\n  punteggi_totali &lt;- rowSums(punteggi[, c(\"subscale1\", \"subscale2\", \"subscale3\")])\n  indice_massimo &lt;- which.max(punteggi_totali)\n  return(punteggi$individuo[indice_massimo])\n}\n\nLa funzione which.max() restituisce l’indice della posizione in cui si trova il valore massimo in un vettore.\n\n\nPasso 3: Applicazione delle Funzioni\n\n\nCalcolo dei punteggi totali per ogni individuo\nApplichiamo la funzione ai dati simulati:\n\npunteggi$punteggio_totale &lt;- with(\n  punteggi, calcola_totale(subscale1, subscale2, subscale3)\n )\nprint(punteggi)\n#&gt;       individuo subscale1 subscale2 subscale3 punteggio_totale\n#&gt; 1   Individuo 1        44        44        48              136\n#&gt; 2   Individuo 2        48        58        51              157\n#&gt; 3   Individuo 3        43        48        45              136\n#&gt; 4   Individuo 4        32        42        41              115\n#&gt; 5   Individuo 5        39        47        55              141\n#&gt; 6   Individuo 6        47        46        46              139\n#&gt; 7   Individuo 7        40        49        49              138\n#&gt; 8   Individuo 8        34        48        44              126\n#&gt; 9   Individuo 9        49        58        47              154\n#&gt; 10 Individuo 10        43        43        41              127\n\n\n\nTroviamo il punteggio massimo nel campione\n\nmassimo &lt;- trova_massimo(punteggi$punteggio_totale)\nprint(massimo)\n#&gt; [1] 157\n\n\n\nTroviamo chi ha il punteggio massimo\n\nindividuo_massimo &lt;- trova_individuo_massimo(punteggi)\nprint(individuo_massimo)\n#&gt; [1] \"Individuo 2\"\n\n\n\n\n\n9.1.3 Stile\nÈ consigliato di usare nomi di funzioni chiari e descrittivi, preferibilmente verbi (es. compute_mean()). Inoltre, è importante mantenere una struttura leggibile, con spazi coerenti e indentazione.",
    "crumbs": [
      "R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Programmazione</span>"
    ]
  },
  {
    "objectID": "chapters/R/03_r_programming.html#istruzioni-condizionali-in-r",
    "href": "chapters/R/03_r_programming.html#istruzioni-condizionali-in-r",
    "title": "9  Programmazione",
    "section": "\n9.2 Istruzioni Condizionali in R",
    "text": "9.2 Istruzioni Condizionali in R\nLe istruzioni condizionali permettono di introdurre logica nel tuo codice. Ad esempio, l’operazione x * y si limita a moltiplicare i valori di x e y, senza alcuna logica aggiunta. Con le istruzioni condizionali, puoi dire al programma di eseguire diverse operazioni a seconda che una condizione sia vera (TRUE) o falsa (FALSE).\nL’istruzione condizionale più comune in R è if. Può essere letta come: “Se la condizione è vera, esegui un’azione”. Con else, si estende la logica: “Se la condizione è vera, fai qualcosa; altrimenti fai qualcos’altro”.\nLa struttura generale è questa:\nif (condizione) {\n  # Codice eseguito se la condizione è TRUE\n} else {\n  # Codice eseguito se la condizione è FALSE\n}\nImmagina questa situazione:\n\n“Se un partecipante al test psicologico riporta un punteggio elevato sulla scala di ansia (es. &gt; 15), consigliagli un esercizio di rilassamento. Altrimenti, non è necessario.”\n\nVediamo come rappresentare questa situazione in R.\n\nanxiety_score &lt;- 18 # Punteggio riportato dal partecipante\n\nif (anxiety_score &gt; 15) {\n    exercise &lt;- \"rilassamento\"\n} else {\n    exercise &lt;- \"nessun esercizio\"\n}\n\nexercise\n#&gt; [1] \"rilassamento\"\n\nSe il punteggio è maggiore di 15, il risultato sarà:\n[1] \"rilassamento\"\nSe il punteggio è inferiore o uguale a 15, il risultato sarà:\n[1] \"nessun esercizio\"\n\n9.2.1 Uso di ifelse()\n\nUn’alternativa più compatta a if e else è la funzione ifelse(), utile soprattutto per vettori. Ad esempio, supponiamo di avere i punteggi di ansia di un gruppo di partecipanti e vogliamo decidere se assegnare un esercizio di rilassamento a ciascuno:\n\nanxiety_scores &lt;- c(12, 18, 9, 22, 15)\nexercises &lt;- ifelse(anxiety_scores &gt; 15, \"rilassamento\", \"nessun esercizio\")\n\nIl risultato sarà:\n\nexercises\n#&gt; [1] \"nessun esercizio\" \"rilassamento\"     \"nessun esercizio\" \"rilassamento\"    \n#&gt; [5] \"nessun esercizio\"\n\n\n9.2.2 Creare una Funzione con Istruzioni Condizionali\nLe istruzioni condizionali possono essere racchiuse in una funzione per rendere il codice più flessibile e riutilizzabile. Ad esempio, supponiamo di voler personalizzare un feedback per un partecipante in base al punteggio ottenuto in un questionario:\n\nfeedback &lt;- function(score) {\n    if (score &gt; 15) {\n        \"Consigliamo un esercizio di rilassamento.\"\n    } else if (score &gt; 10) {\n        \"Monitoriamo la situazione, ma non è necessario alcun intervento.\"\n    } else {\n        \"Nessun intervento necessario.\"\n    }\n}\n\n\nfeedback(18)\n#&gt; [1] \"Consigliamo un esercizio di rilassamento.\"\n\n\nfeedback(12)\n#&gt; [1] \"Monitoriamo la situazione, ma non è necessario alcun intervento.\"\n\n\nfeedback(8)\n#&gt; [1] \"Nessun intervento necessario.\"\n\nIn conclusione, le istruzioni condizionali come if, else e ifelse() sono strumenti fondamentali per introdurre logica e controllo nel tuo codice. Puoi usarle per prendere decisioni, gestire errori e rendere il tuo codice più flessibile ed efficiente. Creare funzioni che incorporano queste istruzioni è un passo fondamentale per scrivere codice ordinato e riutilizzabile in contesti psicologici e non solo.\n\n9.2.3 Combinare Operatori Logici in R\nFinora abbiamo creato funzioni abbastanza semplici e mirate. Ora proviamo a realizzare una funzione leggermente più complessa. Immaginiamo di voler determinare se una persona ha avuto una buona giornata basandoci su due criteri:\n\n\nLivello di stress: basso (TRUE) o alto (FALSE).\n\nLivello di supporto sociale percepito: alto (TRUE) o basso (FALSE).\n\nVogliamo creare una funzione che prenda questi due fattori e restituisca un messaggio che descrive come potrebbe essere stata la giornata della persona.\nEcco come possiamo costruire la funzione:\n\ngood_day &lt;- function(low_stress, high_support) {\n    if (low_stress == TRUE && high_support == TRUE) {\n        \"Giornata fantastica! Ti senti calmo e supportato.\"\n    } else if (low_stress == FALSE && high_support == TRUE) {\n        \"Il supporto sociale ti aiuta a gestire lo stress elevato.\"\n    } else if (low_stress == TRUE && high_support == FALSE) {\n        \"Nonostante lo stress sia basso, la mancanza di supporto sociale pesa.\"\n    } else if (low_stress == FALSE && high_support == FALSE) {\n        \"Giornata difficile: stress elevato e poco supporto sociale.\"\n    }\n}\n\nEsempi di utilizzo.\nCaso 1: Stress basso e supporto sociale alto\n\ngood_day(low_stress = TRUE, high_support = TRUE)\n#&gt; [1] \"Giornata fantastica! Ti senti calmo e supportato.\"\n\nCaso 2: Stress elevato e supporto sociale alto.\n\ngood_day(FALSE, TRUE)\n#&gt; [1] \"Il supporto sociale ti aiuta a gestire lo stress elevato.\"\n\nCaso 3: Stress basso e supporto sociale basso.\n\ngood_day(TRUE, FALSE)\n#&gt; [1] \"Nonostante lo stress sia basso, la mancanza di supporto sociale pesa.\"\n\nCaso 4: Stress elevato e supporto sociale basso.\n\ngood_day(FALSE, FALSE)\n#&gt; [1] \"Giornata difficile: stress elevato e poco supporto sociale.\"\n\nLa funzione considera tutte le combinazioni di stress e supporto sociale:\n\n\nStress basso e supporto alto: giornata ideale.\n\nStress elevato e supporto alto: il supporto aiuta a mitigare lo stress.\n\nStress basso e supporto basso: la mancanza di supporto rovina una situazione potenzialmente buona.\n\nStress elevato e supporto basso: la situazione peggiore.\n\nNell’esempio abbiamo usato i seguenti operatori logici:\n\n\n&& (AND logico): Entrambe le condizioni devono essere vere.\n\n== (uguale a): Verifica se una variabile è vera o falsa.\n\nAd esempio, questa condizione:\nif (low_stress == TRUE && high_support == TRUE)\nverifica se il livello di stress è basso e il supporto sociale è alto.\nIn conclusione, questa funzione dimostra come combinare condizioni logiche complesse utilizzando operatori logici come && (AND) e || (OR). Grazie a questi strumenti, possiamo gestire facilmente logiche più articolate, mantenendo il codice leggibile e funzionale.\n\n9.2.4 Gli operatori Logici in R\nGli operatori logici sono essenziali per definire le condizioni nelle istruzioni if. Ecco una tabella riassuntiva con i principali operatori:\n\n\n\n\n\n\n\n\nOperatore\nDescrizione tecnica\nSignificato\nEsempio\n\n\n\n&&\nAND logico\nEntrambe le condizioni devono essere vere\nif(cond1 == test && cond2 == test)\n\n\n||\nOR logico\nAlmeno una condizione deve essere vera\nif(cond1 == test || cond2 == test)\n\n\n&lt;\nMinore di\nX è minore di Y\nif(X &lt; Y)\n\n\n&gt;\nMaggiore di\nX è maggiore di Y\nif(X &gt; Y)\n\n\n&lt;=\nMinore o uguale a\nX è minore o uguale a Y\nif(X &lt;= Y)\n\n\n&gt;=\nMaggiore o uguale a\nX è maggiore o uguale a Y\nif(X &gt;= Y)\n\n\n==\nUguale a\nX è uguale a Y\nif(X == Y)\n\n\n!=\nDiverso da\nX è diverso da Y\nif(X != Y)",
    "crumbs": [
      "R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Programmazione</span>"
    ]
  },
  {
    "objectID": "chapters/R/03_r_programming.html#cicli-in-r",
    "href": "chapters/R/03_r_programming.html#cicli-in-r",
    "title": "9  Programmazione",
    "section": "\n9.3 Cicli in R",
    "text": "9.3 Cicli in R\nR è particolarmente efficace nell’eseguire attività ripetitive. Quando dobbiamo ripetere un’operazione più volte, possiamo utilizzare un ciclo. I cicli eseguono un insieme di istruzioni per un numero specifico di volte o fino a quando una determinata condizione non è soddisfatta.\nIn R esistono tre tipi principali di cicli:\n\n\nCiclo for: ripete un’operazione per un numero definito di iterazioni.\n\nCiclo while: continua a eseguire le istruzioni fino a quando una condizione logica è soddisfatta.\n\nCiclo repeat: itera indefinitamente fino a quando non viene esplicitamente interrotto con un’istruzione break.\n\nI cicli sono strumenti essenziali in tutti i linguaggi di programmazione, ma in R il loro utilizzo dovrebbe essere valutato attentamente, poiché spesso esistono alternative più efficienti come le funzioni della famiglia apply.\n\n9.3.1 Il ciclo for\n\nIl ciclo for è il più utilizzato per eseguire un’operazione un numero definito di volte. Ecco un esempio base:\n\nfor (i in 1:5) {\n    print(i)\n}\n#&gt; [1] 1\n#&gt; [1] 2\n#&gt; [1] 3\n#&gt; [1] 4\n#&gt; [1] 5\n\nCome funziona?\n\nL’indice i prende il primo valore della sequenza 1:5 (cioè 1).\nIl corpo del ciclo, ovvero il codice tra { }, viene eseguito.\nAl termine di ogni iterazione, i assume il valore successivo nella sequenza, e il processo si ripete fino all’ultimo valore (5 in questo caso).\n\nAggiungere logica nel corpo del ciclo\nPossiamo aggiungere operazioni all’interno del ciclo, come ad esempio sommare 1 a ogni valore:\n\nfor (i in 1:5) {\n    print(i + 1)\n}\n#&gt; [1] 2\n#&gt; [1] 3\n#&gt; [1] 4\n#&gt; [1] 5\n#&gt; [1] 6\n\n\n9.3.2 Il ciclo while\n\nIl ciclo while continua a eseguire le istruzioni fino a quando una condizione logica è soddisfatta. Ecco un esempio:\n\ni &lt;- 0\nwhile (i &lt;= 4) {\n    i &lt;- i + 1\n    print(i)\n}\n#&gt; [1] 1\n#&gt; [1] 2\n#&gt; [1] 3\n#&gt; [1] 4\n#&gt; [1] 5\n\nCome funziona?\n\nLa condizione logica (i &lt;= 4) viene verificata prima di ogni iterazione.\nSe la condizione è vera, il ciclo esegue il codice tra { }.\nQuando la condizione diventa falsa (i &gt; 4), il ciclo si interrompe.\n\n9.3.3 Ciclo repeat\n\nIl ciclo repeat esegue il codice indefinitamente, a meno che non venga interrotto con un’istruzione break:\n\ni &lt;- 0\nrepeat {\n    i &lt;- i + 1\n    print(i)\n    if (i &gt;= 5) {\n        break\n    }\n}\n#&gt; [1] 1\n#&gt; [1] 2\n#&gt; [1] 3\n#&gt; [1] 4\n#&gt; [1] 5\n\nQuando usarlo?\nIl ciclo repeat è raro e viene utilizzato solo in situazioni molto particolari. Nella maggior parte dei casi, for o while sono più adatti.\n\n9.3.4 Evitare i cicli: la famiglia di funzioni apply\n\nI cicli in R sono relativamente lenti, specialmente con dataset di grandi dimensioni. Quando possibile, è preferibile usare funzioni della famiglia apply per ottenere lo stesso risultato in modo più efficiente e con meno rischi di errore.\n\n9.3.4.1 La funzione lapply()\n\nlapply() esegue una funzione su ciascun elemento di una lista o vettore e restituisce una lista con i risultati.\nEsempio:\n\nlapply(0:4, function(a) {\n    a + 1\n})\n#&gt; [[1]]\n#&gt; [1] 1\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 2\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 3\n#&gt; \n#&gt; [[4]]\n#&gt; [1] 4\n#&gt; \n#&gt; [[5]]\n#&gt; [1] 5\n\n\n9.3.4.2 La funzione sapply()\n\nlapply() restituisce una lista, ma se vuoi un vettore come output, usa sapply():\n\nsapply(0:4, function(a) {\n    a + 1\n})\n#&gt; [1] 1 2 3 4 5\n\n\n9.3.5 Quando usare i cicli?\nI cicli sono utili quando:\n\nDevi simulare modelli complessi (es. modelli ricorsivi).\nHai bisogno di operazioni che dipendono dai risultati delle iterazioni precedenti.\n\nIn tutti gli altri casi, considera alternative come apply(), lapply() o funzioni simili per un codice più efficiente e meno soggetto a errori.",
    "crumbs": [
      "R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Programmazione</span>"
    ]
  },
  {
    "objectID": "chapters/R/03_r_programming.html#linee-guida-per-scrivere-codice",
    "href": "chapters/R/03_r_programming.html#linee-guida-per-scrivere-codice",
    "title": "9  Programmazione",
    "section": "\n9.4 Linee Guida per Scrivere Codice",
    "text": "9.4 Linee Guida per Scrivere Codice\nDi seguito trovi alcune linee guida per scrivere codice chiaro, conciso e riutilizzabile:\n\nEvita di ripeterti: Segui il principio Don’t Repeat Yourself (DRY). Scrivi funzioni e utilizza funzioni come map (per applicare un pezzo di codice iterativamente a tutti gli elementi di un oggetto) per evitare di copiare e incollare variazioni minime dello stesso codice in più parti del progetto.\nSegui uno stile coerente: Adotta una guida di stile per mantenere uniformità nel tuo codice. Per R, raccomandiamo la guida di stile del “tidyverse”, scritta da Hadley Wickham. Questa guida, derivata dalla Google R Style Guide, fornisce istruzioni dettagliate su sintassi del codice, nomi delle variabili, spaziature, indentazioni, commenti, convenzioni per scrivere funzioni, utilizzo delle pipe (metodo per concatenare funzioni), e altro ancora.\nCommenta abbondantemente: Usa i commenti (ad esempio, con #) per spiegare perché ogni parte del codice è necessaria e cosa fa. I commenti rendono il codice più leggibile e facilitano la manutenzione futura.\nTesta il tuo codice: Ogni volta che scrivi codice, verifica che funzioni come previsto. Puoi farlo scrivendo funzioni di test specifiche o controllando manualmente che l’output corrisponda alle aspettative. Abituati a pensare a eventuali edge cases (casi limite) in cui il tuo codice potrebbe non comportarsi come previsto.\nEsegui una revisione del codice: Quando possibile, fai revisionare il tuo codice da un’altra persona per individuare errori e incoerenze. Se non hai nessuno a disposizione, puoi rivedere il tuo codice autonomamente: rileggendo con attenzione, è sorprendente il numero di errori che si possono individuare!\n\nSeguendo queste linee guida, potrai scrivere codice più robusto, leggibile e facile da mantenere nel tempo.1",
    "crumbs": [
      "R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Programmazione</span>"
    ]
  },
  {
    "objectID": "chapters/R/03_r_programming.html#riflessioni-conclusive",
    "href": "chapters/R/03_r_programming.html#riflessioni-conclusive",
    "title": "9  Programmazione",
    "section": "\n9.5 Riflessioni Conclusive",
    "text": "9.5 Riflessioni Conclusive\nScrivere funzioni è un passaggio essenziale per migliorare la leggibilità, l’efficienza e la riutilizzabilità del codice. Funzioni ben progettate semplificano le modifiche, riducono errori e rendono il lavoro più chiaro, sia per te stesso che per i collaboratori futuri. Se trovi che stai copiando e incollando codice più volte, è il momento di pensare a creare una funzione.\nLe istruzioni condizionali, come if, else e ifelse(), sono fondamentali per introdurre logica e controllo nel codice. Permettono di gestire scenari diversi e prendere decisioni dinamiche, migliorando la flessibilità e l’efficienza dei tuoi script. Combinando queste istruzioni con operatori logici come && e ||, puoi affrontare situazioni complesse con un codice chiaro e leggibile.\nI cicli sono potenti strumenti per eseguire operazioni ripetitive, ma in R il loro utilizzo dovrebbe essere limitato ai casi in cui non esistono alternative più efficienti. Le funzioni apply() e simili rappresentano spesso un’opzione migliore per manipolare dati in modo più rapido e leggibile.\n\n\n\n\n\n\nEsercizio\n\n\n\n\n\nIn questo esercizio, utilizzerai R per praticare la creazione di funzioni, l’uso delle istruzioni condizionali e l’applicazione dei cicli. L’obiettivo è comprendere come scrivere codice più strutturato, riutilizzabile ed efficiente.\nParte 1: Comprensione Teorica\n\n\nCos’è una funzione in R?\n\nDescrivi con parole tue cosa fa una funzione e perché è utile.\n\n\n\nSintassi delle funzioni\n\nScrivi la struttura generale di una funzione in R.\n\n\n\nUso di istruzioni condizionali\n\nQual è la differenza tra if, else e ifelse()? Fornisci un esempio per ciascuno.\n\n\n\nCicli in R\n\nQual è la differenza tra for, while e repeat?\n\n\n\nParte 2: Creazione ed Esecuzione in R\n\n\nCreazione di una funzione per calcolare il punteggio totale SWLS\n\nScrivi una funzione in R chiamata calcola_SWLS() che accetta un vettore con 5 punteggi SWLS e restituisce il totale.\n\n\n\nCondizione per determinare la soddisfazione\n\nScrivi una funzione valuta_soddisfazione() che prende un punteggio SWLS totale e restituisce:\n\n\n\"Alta soddisfazione\" se il punteggio è sopra 24.\n\n\"Soddisfazione moderata\" se è tra 15 e 24.\n\n\"Bassa soddisfazione\" se è inferiore a 15.\n\n\n\n\n\nApplicare una funzione a più individui\n\nScrivi un ciclo for che calcola la soddisfazione per un gruppo di 5 persone e stampa il risultato.\n\n\n\nUso di ifelse()\n\nUsa ifelse() per determinare rapidamente se i punteggi di 5 individui indicano soddisfazione alta (&gt; 24) o bassa (≤ 24).\n\n\n\nCiclo while per controllare input\n\nScrivi un ciclo while che continua a chiedere all’utente di inserire un punteggio SWLS fino a quando non inserisce un valore valido (compreso tra 5 e 35).\n\n\n\nEsportazione dei dati\n\n\n\nSalva in un file CSV \"swls_results.csv\" un data frame contenente i punteggi SWLS e la valutazione della soddisfazione.\n\nConsegna\n\nScrivi le risposte della Parte 1 su carta.\nScrivi il codice e i risultati della Parte 2 in un file .R e invialo come consegna.\n\n\n\n\n\n\n\n\n\n\nSoluzione\n\n\n\n\n\nParte 1: Comprensione Teorica\n\n\nCos’è una funzione in R?\n\nUna funzione è un blocco di codice che esegue un’operazione specifica. Permette di scrivere codice riutilizzabile e più organizzato.\n\n\n\nSintassi delle funzioni\nnome_funzione &lt;- function(argomenti) {\n  # Corpo della funzione\n  return(risultato)\n}\n\n\nUso di istruzioni condizionali\n\n\nif: Controlla una condizione e esegue codice solo se è vera.\n\nif (x &gt; 10) { print(\"Maggiore di 10\") }\n\n\nelse: Esegue codice alternativo se la condizione è falsa.\n\nif (x &gt; 10) { print(\"Maggiore di 10\") } else { print(\"10 o meno\") }\n\n\nifelse(): Alternativa vettorializzata a if.\n\ny &lt;- ifelse(x &gt; 10, \"Alto\", \"Basso\")\n\n\nCicli in R\n\n\nfor: Itera su una sequenza.\n\nfor (i in 1:5) { print(i) }\n\n\nwhile: Continua fino a quando una condizione è vera.\n\ni &lt;- 1\nwhile (i &lt;= 5) { print(i); i &lt;- i + 1 }\n\n\nrepeat: Ripete fino a un break.\n\ni &lt;- 1\nrepeat { print(i); i &lt;- i + 1; if (i &gt; 5) break }\n\n\nParte 2: Creazione ed Esecuzione in R\n\n\nCreazione della funzione per il punteggio totale SWLS\ncalcola_SWLS &lt;- function(punteggi) {\n  return(sum(punteggi))\n}\n\n\nCondizione per determinare la soddisfazione\nvaluta_soddisfazione &lt;- function(score) {\n  if (score &gt; 24) {\n    return(\"Alta soddisfazione\")\n  } else if (score &gt;= 15) {\n    return(\"Soddisfazione moderata\")\n  } else {\n    return(\"Bassa soddisfazione\")\n  }\n}\n\n\nApplicazione della funzione a più individui\npunteggi_lista &lt;- list(c(25, 27, 22, 24, 28), c(18, 20, 17, 16, 19))\nfor (punteggi in punteggi_lista) {\n  print(valuta_soddisfazione(calcola_SWLS(punteggi)))\n}\n\n\nUso di ifelse()\npunteggi_totali &lt;- c(28, 19, 15, 10, 25)\nsoddisfazione &lt;- ifelse(punteggi_totali &gt; 24, \"Alta\", \"Bassa\")\nprint(soddisfazione)\n\n\nCiclo while per controllare input\nscore &lt;- 0\nwhile (score &lt; 5 || score &gt; 35) {\n  score &lt;- as.numeric(readline(prompt = \"Inserisci un punteggio SWLS (5-35): \"))\n}\n\nEsportazione dei dati\n\ndf &lt;- data.frame(Punteggio = punteggi_totali, Soddisfazione = soddisfazione)\nwrite.csv(df, \"swls_results.csv\", row.names = FALSE)\nConclusione\nQuesti esercizi hanno mostrato come scrivere funzioni, utilizzare condizioni e cicli per strutturare meglio il codice in R.\n\n\n\n\n\n\n\n\n\nInformazioni sull’ambiente di sviluppo\n\n\n\n\n\n\nsessionInfo()\n#&gt; R version 4.5.1 (2025-06-13)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.6.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] pillar_1.11.0         tinytable_0.13.0      patchwork_1.3.2      \n#&gt;  [4] ggdist_3.3.3          tidybayes_3.0.7       bayesplot_1.14.0     \n#&gt;  [7] ggplot2_3.5.2         reliabilitydiag_0.2.1 priorsense_1.1.1     \n#&gt; [10] posterior_1.6.1       loo_2.8.0             rstan_2.32.7         \n#&gt; [13] StanHeaders_2.32.10   brms_2.22.0           Rcpp_1.1.0           \n#&gt; [16] sessioninfo_1.2.3     conflicted_1.2.0      janitor_2.2.1        \n#&gt; [19] matrixStats_1.5.0     modelr_0.1.11         tibble_3.3.0         \n#&gt; [22] dplyr_1.1.4           tidyr_1.3.1           rio_1.2.3            \n#&gt; [25] here_1.0.1           \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;  [1] svUnit_1.0.8          tidyselect_1.2.1      farver_2.1.2         \n#&gt;  [4] fastmap_1.2.0         TH.data_1.1-4         tensorA_0.36.2.1     \n#&gt;  [7] pacman_0.5.1          digest_0.6.37         timechange_0.3.0     \n#&gt; [10] estimability_1.5.1    lifecycle_1.0.4       survival_3.8-3       \n#&gt; [13] magrittr_2.0.3        compiler_4.5.1        rlang_1.1.6          \n#&gt; [16] tools_4.5.1           knitr_1.50            bridgesampling_1.1-2 \n#&gt; [19] htmlwidgets_1.6.4     curl_7.0.0            pkgbuild_1.4.8       \n#&gt; [22] RColorBrewer_1.1-3    abind_1.4-8           multcomp_1.4-28      \n#&gt; [25] withr_3.0.2           purrr_1.1.0           grid_4.5.1           \n#&gt; [28] stats4_4.5.1          colorspace_2.1-1      xtable_1.8-4         \n#&gt; [31] inline_0.3.21         emmeans_1.11.2-8      scales_1.4.0         \n#&gt; [34] MASS_7.3-65           cli_3.6.5             mvtnorm_1.3-3        \n#&gt; [37] rmarkdown_2.29        ragg_1.5.0            generics_0.1.4       \n#&gt; [40] RcppParallel_5.1.11-1 cachem_1.1.0          stringr_1.5.1        \n#&gt; [43] splines_4.5.1         parallel_4.5.1        vctrs_0.6.5          \n#&gt; [46] V8_7.0.0              Matrix_1.7-4          sandwich_3.1-1       \n#&gt; [49] jsonlite_2.0.0        arrayhelpers_1.1-0    systemfonts_1.2.3    \n#&gt; [52] glue_1.8.0            codetools_0.2-20      distributional_0.5.0 \n#&gt; [55] lubridate_1.9.4       stringi_1.8.7         gtable_0.3.6         \n#&gt; [58] QuickJSR_1.8.0        htmltools_0.5.8.1     Brobdingnag_1.2-9    \n#&gt; [61] R6_2.6.1              textshaping_1.0.3     rprojroot_2.1.1      \n#&gt; [64] evaluate_1.0.5        lattice_0.22-7        backports_1.5.0      \n#&gt; [67] memoise_2.0.1         broom_1.0.9           snakecase_0.11.1     \n#&gt; [70] rstantools_2.5.0      coda_0.19-4.1         gridExtra_2.3        \n#&gt; [73] nlme_3.1-168          checkmate_2.3.3       xfun_0.53            \n#&gt; [76] zoo_1.8-14            pkgconfig_2.0.3",
    "crumbs": [
      "R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Programmazione</span>"
    ]
  },
  {
    "objectID": "chapters/R/03_r_programming.html#bibliografia",
    "href": "chapters/R/03_r_programming.html#bibliografia",
    "title": "9  Programmazione",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nIrizarry, R. A. (2024). Introduction to Data Science: Data Wrangling and Visualization with R. CRC Press.",
    "crumbs": [
      "R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Programmazione</span>"
    ]
  },
  {
    "objectID": "chapters/R/03_r_programming.html#footnotes",
    "href": "chapters/R/03_r_programming.html#footnotes",
    "title": "9  Programmazione",
    "section": "",
    "text": "Un’ottima introduzione alle regole di stile per un progetto di analisi dei dati è fornita in questo capitolo.↩︎",
    "crumbs": [
      "R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Programmazione</span>"
    ]
  },
  {
    "objectID": "chapters/R/04_r_packages.html",
    "href": "chapters/R/04_r_packages.html",
    "title": "10  Pacchetti",
    "section": "",
    "text": "Introduzione\nI pacchetti R sono estensioni del linguaggio di programmazione statistica R. Questi pacchetti forniscono una raccolta di risorse che possono essere utilizzate per ampliare le funzionalità di base di R. Ogni pacchetto generalmente include:\nI pacchetti R sono distribuiti e installati attraverso repository centralizzati, il più noto dei quali è CRAN (Comprehensive R Archive Network). CRAN garantisce la qualità e l’affidabilità dei pacchetti, sottoponendoli a controlli rigorosi prima della pubblicazione.\nLa vasta disponibilità di pacchetti è una delle ragioni principali della popolarità di R.",
    "crumbs": [
      "R",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Pacchetti</span>"
    ]
  },
  {
    "objectID": "chapters/R/04_r_packages.html#introduzione",
    "href": "chapters/R/04_r_packages.html#introduzione",
    "title": "10  Pacchetti",
    "section": "",
    "text": "Codice: funzioni e script scritti in R (e talvolta in altri linguaggi come C++ o Fortran) che implementano specifiche analisi o strumenti.\nDati: dataset di esempio o utili per testare e dimostrare le funzionalità del pacchetto.\nDocumentazione: file descrittivi che spiegano come utilizzare il pacchetto, spesso in formato manuale o vignette (tutorial pratici).\n\n\n\n\n\n\n\n\n\nPrerequisiti\n\n\n\n\n\n\nConsultare Introduction to Data Science: Data Wrangling and Visualization with R (Irizarry, 2024)\nLeggere R for Data Science (2e).",
    "crumbs": [
      "R",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Pacchetti</span>"
    ]
  },
  {
    "objectID": "chapters/R/04_r_packages.html#installare-i-pacchetti-r",
    "href": "chapters/R/04_r_packages.html#installare-i-pacchetti-r",
    "title": "10  Pacchetti",
    "section": "10.1 Installare i pacchetti R",
    "text": "10.1 Installare i pacchetti R\nQuando installi R, vengono installati automaticamente alcuni pacchetti base. Tuttavia, hai la possibilità di aggiungere ulteriori pacchetti che trovi utili per i tuoi scopi. Questi pacchetti sono memorizzati sui server di R (mirror), e l’installazione di un nuovo pacchetto richiede una connessione internet al mirror CRAN che hai scelto durante l’installazione di R.\nPer installare un pacchetto, utilizza il comando:\ninstall.packages(\"&lt;nome_pacchetto&gt;\")\nSostituisci &lt;nome_pacchetto&gt; con il nome del pacchetto che desideri installare. Ad esempio, se vuoi installare il pacchetto rio (utile per importare i dati in R), puoi digitare:\ninstall.packages(\"rio\")   # Non dimenticare le virgolette!",
    "crumbs": [
      "R",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Pacchetti</span>"
    ]
  },
  {
    "objectID": "chapters/R/04_r_packages.html#caricamento-di-un-pacchetto",
    "href": "chapters/R/04_r_packages.html#caricamento-di-un-pacchetto",
    "title": "10  Pacchetti",
    "section": "10.2 Caricamento di un pacchetto",
    "text": "10.2 Caricamento di un pacchetto\nOgni volta che avvii una nuova sessione di R, se desideri utilizzare un pacchetto, devi caricarlo manualmente. Questo si fa con il comando library(). Ad esempio, dopo aver installato rio, per utilizzarlo digita:\nlibrary(rio)   # Nota: le virgolette non sono necessarie, ma puoi usarle se preferisci.",
    "crumbs": [
      "R",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Pacchetti</span>"
    ]
  },
  {
    "objectID": "chapters/R/04_r_packages.html#utilizzo-delle-funzioni-di-un-pacchetto-senza-caricarlo",
    "href": "chapters/R/04_r_packages.html#utilizzo-delle-funzioni-di-un-pacchetto-senza-caricarlo",
    "title": "10  Pacchetti",
    "section": "10.3 Utilizzo delle funzioni di un pacchetto senza caricarlo",
    "text": "10.3 Utilizzo delle funzioni di un pacchetto senza caricarlo\nSe hai bisogno di utilizzare una funzione specifica di un pacchetto, ma sai che la userai solo una volta, puoi evitare di caricare l’intero pacchetto con library(). Ad esempio, invece di scrivere:\nlibrary(nome_pacchetto)\nfunzione_specifica(x = 2, sd = 3)\npuoi accedere direttamente alla funzione usando l’operatore ::, come indicato di segtuito:\nnome_pacchetto::funzione_specifica(x = 2, sd = 3)\nQuesto approccio è utile per funzioni che usi raramente o una sola volta. Personalmente, utilizzo :: anche quando ho già caricato il pacchetto, per ricordare ad un “me futuro” da quale pacchetto proviene una determinata funzione. Questo può rendere il codice più leggibile e comprensibile nel tempo.",
    "crumbs": [
      "R",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Pacchetti</span>"
    ]
  },
  {
    "objectID": "chapters/R/04_r_packages.html#bibliografia",
    "href": "chapters/R/04_r_packages.html#bibliografia",
    "title": "10  Pacchetti",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nIrizarry, R. A. (2024). Introduction to Data Science: Data Wrangling and Visualization with R. CRC Press.",
    "crumbs": [
      "R",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Pacchetti</span>"
    ]
  },
  {
    "objectID": "chapters/R/05_dplyr.html",
    "href": "chapters/R/05_dplyr.html",
    "title": "11  Usare dplyr",
    "section": "",
    "text": "Introduzione\nL’obiettivo di questo capitolo è fornire un’introduzione alle funzioni principali del pacchetto dplyr per le operazioni di data wrangling, cioè per il preprocessing e la pulizia dei dati. In R, queste operazioni sono strettamente legate al concetto di “data tidying”, che si riferisce all’organizzazione sistematica dei dati per facilitare l’analisi.1\nL’essenza del “data tidying” è organizzare i dati in un formato che sia facile da gestire e analizzare. Anche se gli stessi dati possono essere rappresentati in vari modi, non tutte le rappresentazioni sono ugualmente efficienti o facili da usare. Un dataset “tidy” segue tre principi fondamentali che lo rendono particolarmente pratico:\nIl pacchetto R {dplyr} e gli altri pacchetti del tidyverse sono progettati specificamente per lavorare con dati in formato “tidy”, permettendo agli utenti di eseguire operazioni di manipolazione e visualizzazione in modo più intuitivo ed efficiente.",
    "crumbs": [
      "R",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Usare `dplyr`</span>"
    ]
  },
  {
    "objectID": "chapters/R/05_dplyr.html#introduzione",
    "href": "chapters/R/05_dplyr.html#introduzione",
    "title": "11  Usare dplyr",
    "section": "",
    "text": "Ogni variabile è una colonna: ogni colonna nel dataset rappresenta una singola variabile.\n\nOgni osservazione è una riga: ogni riga nel dataset rappresenta un’unica osservazione.\n\nOgni valore è una cella: ogni cella del dataset contiene un singolo valore.\n\n\n\n\n\n\n\n\nPrerequisiti\n\n\n\n\n\n\nLeggere R for Data Science (2e).\nConsultare Data cleaning for social scientists.\nLeggere il capitolo Data Wrangling di Statistical Inference via Data Science: A ModernDive into R and the Tidyverse (Second Edition).\nConsultare Introduction to Data Science: Data Wrangling and Visualization with R (Irizarry, 2024)\n\n\n\n\n\n\n\n\n\n\n\nPreparazione del Notebook\n\n\n\n\n\n\nhere::here(\"code\", \"_common.R\") |&gt; source()\n\n# Load packages\nif (!requireNamespace(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyr, mice, missForest)",
    "crumbs": [
      "R",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Usare `dplyr`</span>"
    ]
  },
  {
    "objectID": "chapters/R/05_dplyr.html#pipe",
    "href": "chapters/R/05_dplyr.html#pipe",
    "title": "11  Usare dplyr",
    "section": "\n11.1 Pipe",
    "text": "11.1 Pipe\nIl pacchetto dplyr, così come l’intero ecosistema tidyverse, fa largo uso dell’operatore pipe, che consente di concatenare una sequenza di operazioni in modo leggibile ed efficiente. In R, esistono due principali notazioni per il pipe:\n\n\n|&gt;: introdotto nativamente a partire dalla versione 4.1.0 di R.\n\n%&gt;%: introdotto dal pacchetto magrittr, ed è una delle componenti centrali del tidyverse.\n\nEntrambi gli operatori permettono di ottenere risultati simili e, per la maggior parte degli utilizzi, possono essere considerati intercambiabili. Tuttavia, è importante sottolineare alcune differenze:\n\n\n|&gt; è integrato nel linguaggio R e non richiede pacchetti aggiuntivi.\n\n%&gt;%, essendo parte di magrittr, richiede che il pacchetto sia installato e caricato (library(magrittr) o automaticamente tramite tidyverse).\n\nConsideriamo l’esempio seguente (che anticipa l’uso della funzione filter() che descriveremo in seguito). Un’operazione comune è filtrare un data frame e calcolare la media di una colonna. Con il pipe, questa sequenza di operazioni diventa più leggibile:\n\n# Usando %&gt;%\niris %&gt;%\n  dplyr::filter(Species == \"setosa\") |&gt; \n  summarise(\n    mean_sepal_length = mean(Sepal.Length)\n  ) \n#&gt;   mean_sepal_length\n#&gt; 1              5.01\n\n\n# Usando |&gt;\niris |&gt; \n  dplyr::filter(Species == \"setosa\") |&gt; \n  summarise(\n    mean_sepal_length = mean(Sepal.Length)\n  ) \n#&gt;   mean_sepal_length\n#&gt; 1              5.01\n\n\n11.1.1 Cosa Fa la Pipe?\nLa pipe è uno strumento potente che permette di collegare in modo diretto l’output di una funzione come input della funzione successiva. Questo approccio:\n\nRiduce la necessità di creare variabili intermedie.\nMigliora la leggibilità del codice.\nRende il flusso delle operazioni più chiaro e lineare.\n\nOgni funzione applicata con la pipe riceve automaticamente l’output della funzione precedente come suo primo argomento. Ciò consente di scrivere sequenze di operazioni in un formato compatto e intuitivo.\nEcco un altro esempio:\n\n# Utilizzo della pipe per trasformare un dataset\ndf &lt;- data.frame(\n  id = 1:5,\n  value = c(10, 20, 30, 40, 50)\n)\n\n# Filtra i dati, seleziona colonne e calcola nuovi valori\ndf_clean &lt;- df |&gt;\n  dplyr::filter(value &gt; 20) |&gt;\n  dplyr::select(id, value) |&gt;\n  mutate(squared_value = value^2)\n\nIn questa sequenza, il dataset originale df viene filtrato, le colonne desiderate vengono selezionate e viene aggiunta una nuova colonna con il valore al quadrato.\n\nhead(df_clean)\n#&gt;   id value squared_value\n#&gt; 1  3    30           900\n#&gt; 2  4    40          1600\n#&gt; 3  5    50          2500\n\nIn sintesi, la pipe è uno strumento fondamentale per scrivere codice R moderno e leggibile, indipendentemente dal fatto che si utilizzi |&gt; o %&gt;%.",
    "crumbs": [
      "R",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Usare `dplyr`</span>"
    ]
  },
  {
    "objectID": "chapters/R/05_dplyr.html#verbi",
    "href": "chapters/R/05_dplyr.html#verbi",
    "title": "11  Usare dplyr",
    "section": "\n11.2 Verbi",
    "text": "11.2 Verbi\nLe funzioni principali (“verbi) di dplyr sono le seguenti:\n\n\n\n\n\n\nVerbo dplyr\nDescrizione\n\n\n\nselect()\nSeleziona colonne\n\n\nfilter()\nFiltra righe\n\n\narrange()\nRiordina o organizza le righe\n\n\nmutate()\nCrea nuove colonne\n\n\nsummarise()\nRiassume i valori\n\n\ngroup_by()\nConsente di eseguire operazioni di gruppo\n\n\n\nI verbi di dplyr sono suddivisi in quattro gruppi, in base all’elemento su cui operano: righe, colonne, gruppi o tabelle.\nInoltre, le diverse funzioni bind_ e _joins permettono di combinare più tibbles (ovvero, data frame) in uno solo.\nPer fare un esempio prarico, usiamo nuovamente il dataset msleep.\n\ndata(msleep)\ndim(msleep)\n#&gt; [1] 83 11\n\nEsaminiamo i dati:\n\nglimpse(msleep)\n#&gt; Rows: 83\n#&gt; Columns: 11\n#&gt; $ name         &lt;chr&gt; \"Cheetah\", \"Owl monkey\", \"Mountain beaver\", \"Greater shor…\n#&gt; $ genus        &lt;chr&gt; \"Acinonyx\", \"Aotus\", \"Aplodontia\", \"Blarina\", \"Bos\", \"Bra…\n#&gt; $ vore         &lt;chr&gt; \"carni\", \"omni\", \"herbi\", \"omni\", \"herbi\", \"herbi\", \"carn…\n#&gt; $ order        &lt;chr&gt; \"Carnivora\", \"Primates\", \"Rodentia\", \"Soricomorpha\", \"Art…\n#&gt; $ conservation &lt;chr&gt; \"lc\", NA, \"nt\", \"lc\", \"domesticated\", NA, \"vu\", NA, \"dome…\n#&gt; $ sleep_total  &lt;dbl&gt; 12.1, 17.0, 14.4, 14.9, 4.0, 14.4, 8.7, 7.0, 10.1, 3.0, 5…\n#&gt; $ sleep_rem    &lt;dbl&gt; NA, 1.8, 2.4, 2.3, 0.7, 2.2, 1.4, NA, 2.9, NA, 0.6, 0.8, …\n#&gt; $ sleep_cycle  &lt;dbl&gt; NA, NA, NA, 0.133, 0.667, 0.767, 0.383, NA, 0.333, NA, NA…\n#&gt; $ awake        &lt;dbl&gt; 11.9, 7.0, 9.6, 9.1, 20.0, 9.6, 15.3, 17.0, 13.9, 21.0, 1…\n#&gt; $ brainwt      &lt;dbl&gt; NA, 0.01550, NA, 0.00029, 0.42300, NA, NA, NA, 0.07000, 0…\n#&gt; $ bodywt       &lt;dbl&gt; 50.000, 0.480, 1.350, 0.019, 600.000, 3.850, 20.490, 0.04…\n\nLe colonne, nell’ordine, corrispondono a quanto segue:\n\n\nNome colonna\nDescrizione\n\n\n\nname\nNome comune\n\n\ngenus\nRango tassonomico\n\n\nvore\nCarnivoro, onnivoro o erbivoro?\n\n\norder\nRango tassonomico\n\n\nconservation\nStato di conservazione del mammifero\n\n\nsleep_total\nQuantità totale di sonno, in ore\n\n\nsleep_rem\nSonno REM, in ore\n\n\nsleep_cycle\nDurata del ciclo di sonno, in ore\n\n\nawake\nQuantità di tempo trascorso sveglio, in ore\n\n\nbrainwt\nPeso del cervello, in chilogrammi\n\n\nbodywt\nPeso corporeo, in chilogrammi",
    "crumbs": [
      "R",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Usare `dplyr`</span>"
    ]
  },
  {
    "objectID": "chapters/R/05_dplyr.html#righe",
    "href": "chapters/R/05_dplyr.html#righe",
    "title": "11  Usare dplyr",
    "section": "\n11.3 Righe",
    "text": "11.3 Righe\nI verbi più importanti che operano sulle righe di un dataset sono filter(), che seleziona le righe da includere senza modificarne l’ordine, e arrange(), che cambia l’ordine delle righe senza alterare la selezione delle righe presenti.\n\nmsleep |&gt;\n  dplyr::filter(sleep_total &lt; 4) |&gt;\n  arrange(sleep_total)\n#&gt; # A tibble: 9 × 11\n#&gt;   name             genus         vore  order          conservation sleep_total\n#&gt;   &lt;chr&gt;            &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt;          &lt;chr&gt;              &lt;dbl&gt;\n#&gt; 1 Giraffe          Giraffa       herbi Artiodactyla   cd                   1.9\n#&gt; 2 Pilot whale      Globicephalus carni Cetacea        cd                   2.7\n#&gt; 3 Horse            Equus         herbi Perissodactyla domesticated         2.9\n#&gt; 4 Roe deer         Capreolus     herbi Artiodactyla   lc                   3  \n#&gt; 5 Donkey           Equus         herbi Perissodactyla domesticated         3.1\n#&gt; 6 African elephant Loxodonta     herbi Proboscidea    vu                   3.3\n#&gt; 7 Caspian seal     Phoca         carni Carnivora      vu                   3.5\n#&gt; 8 Sheep            Ovis          herbi Artiodactyla   domesticated         3.8\n#&gt; 9 Asian elephant   Elephas       herbi Proboscidea    en                   3.9\n#&gt;   sleep_rem sleep_cycle awake brainwt bodywt\n#&gt;       &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1       0.4          NA  22.1 NA       900. \n#&gt; 2       0.1          NA  21.4 NA       800  \n#&gt; 3       0.6           1  21.1  0.655   521  \n#&gt; 4      NA            NA  21    0.0982   14.8\n#&gt; 5       0.4          NA  20.9  0.419   187  \n#&gt; 6      NA            NA  20.7  5.71   6654  \n#&gt; 7       0.4          NA  20.5 NA        86  \n#&gt; 8       0.6          NA  20.2  0.175    55.5\n#&gt; 9      NA            NA  20.1  4.60   2547\n\nPossiamo usare filter() speficicano più di una condizione logica.\n\nmsleep |&gt;\n  dplyr::filter((sleep_total &lt; 4 & bodywt &gt; 100) | brainwt &gt; 1) |&gt;\n  arrange(sleep_total)\n#&gt; # A tibble: 7 × 11\n#&gt;   name             genus         vore  order          conservation sleep_total\n#&gt;   &lt;chr&gt;            &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt;          &lt;chr&gt;              &lt;dbl&gt;\n#&gt; 1 Giraffe          Giraffa       herbi Artiodactyla   cd                   1.9\n#&gt; 2 Pilot whale      Globicephalus carni Cetacea        cd                   2.7\n#&gt; 3 Horse            Equus         herbi Perissodactyla domesticated         2.9\n#&gt; 4 Donkey           Equus         herbi Perissodactyla domesticated         3.1\n#&gt; 5 African elephant Loxodonta     herbi Proboscidea    vu                   3.3\n#&gt; 6 Asian elephant   Elephas       herbi Proboscidea    en                   3.9\n#&gt; 7 Human            Homo          omni  Primates       &lt;NA&gt;                 8  \n#&gt;   sleep_rem sleep_cycle awake brainwt bodywt\n#&gt;       &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1       0.4        NA    22.1  NA       900.\n#&gt; 2       0.1        NA    21.4  NA       800 \n#&gt; 3       0.6         1    21.1   0.655   521 \n#&gt; 4       0.4        NA    20.9   0.419   187 \n#&gt; 5      NA          NA    20.7   5.71   6654 \n#&gt; 6      NA          NA    20.1   4.60   2547 \n#&gt; 7       1.9         1.5  16     1.32     62",
    "crumbs": [
      "R",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Usare `dplyr`</span>"
    ]
  },
  {
    "objectID": "chapters/R/05_dplyr.html#colonne",
    "href": "chapters/R/05_dplyr.html#colonne",
    "title": "11  Usare dplyr",
    "section": "\n11.4 Colonne",
    "text": "11.4 Colonne\nEsistono quattro verbi principali che modificano le colonne di un dataset senza cambiare le righe:\n\n\nrelocate() cambia la posizione delle colonne;\n\nrename() modifica i nomi delle colonne;\n\nselect() seleziona le colonne da includere o escludere;\n\nmutate() crea nuove colonne a partire da quelle esistenti.\n\n\nmsleep2 &lt;- msleep |&gt;\n  mutate(\n    rem_prop = sleep_rem / sleep_total * 100\n  ) |&gt;\n  dplyr::select(name, vore, rem_prop, sleep_total) |&gt;\n  arrange(desc(rem_prop))\n\nglimpse(msleep2)\n#&gt; Rows: 83\n#&gt; Columns: 4\n#&gt; $ name        &lt;chr&gt; \"European hedgehog\", \"Thick-tailed opposum\", \"Giant armadi…\n#&gt; $ vore        &lt;chr&gt; \"omni\", \"carni\", \"insecti\", \"omni\", \"carni\", \"omni\", \"omni…\n#&gt; $ rem_prop    &lt;dbl&gt; 34.7, 34.0, 33.7, 29.2, 28.7, 27.2, 26.4, 26.2, 25.6, 25.0…\n#&gt; $ sleep_total &lt;dbl&gt; 10.1, 19.4, 18.1, 8.9, 10.1, 18.0, 9.1, 10.3, 12.5, 8.4, 1…\n\nIn questo esempio, utilizziamo mutate() per creare una nuova colonna rem_prop che rappresenta la percentuale di sonno REM sul totale del sonno. Successivamente, select() viene utilizzato per scegliere solo alcune colonne del dataset, e infine desc(rem_prop) ordina i valori di rem_prop in ordine decrescente, dal valore maggiore a quello minore.\nPer cambiare il nome di una colonna possiamo usare rename(). Inoltre, possiamo cambiare l’ordine delle variabili con relocate().\n\nmsleep2 |&gt;\n  rename(rem_perc = rem_prop) |&gt;\n  relocate(rem_perc, .before = name)\n#&gt; # A tibble: 83 × 4\n#&gt;    rem_perc name                   vore    sleep_total\n#&gt;       &lt;dbl&gt; &lt;chr&gt;                  &lt;chr&gt;         &lt;dbl&gt;\n#&gt;  1     34.7 European hedgehog      omni           10.1\n#&gt;  2     34.0 Thick-tailed opposum   carni          19.4\n#&gt;  3     33.7 Giant armadillo        insecti        18.1\n#&gt;  4     29.2 Tree shrew             omni            8.9\n#&gt;  5     28.7 Dog                    carni          10.1\n#&gt;  6     27.2 North American Opossum omni           18  \n#&gt;  7     26.4 Pig                    omni            9.1\n#&gt;  8     26.2 Desert hedgehog        &lt;NA&gt;           10.3\n#&gt;  9     25.6 Domestic cat           carni          12.5\n#&gt; 10     25   Eastern american mole  insecti         8.4\n#&gt; # ℹ 73 more rows",
    "crumbs": [
      "R",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Usare `dplyr`</span>"
    ]
  },
  {
    "objectID": "chapters/R/05_dplyr.html#gruppi",
    "href": "chapters/R/05_dplyr.html#gruppi",
    "title": "11  Usare dplyr",
    "section": "\n11.5 Gruppi",
    "text": "11.5 Gruppi\nIl verbo group_by() viene utilizzato per suddividere un dataset in gruppi, in base a una o più variabili, che siano rilevanti per l’analisi. Questo permette di eseguire operazioni di sintesi su ciascun gruppo separatamente, ottenendo informazioni aggregate.\nAd esempio, nel codice seguente:\n\nmsleep |&gt;\n  group_by(order) |&gt;\n  summarise(\n    avg_sleep = mean(sleep_total),\n    min_sleep = min(sleep_total),\n    max_sleep = max(sleep_total),\n    total = n()\n  ) |&gt;\n  arrange(desc(avg_sleep))\n#&gt; # A tibble: 19 × 5\n#&gt;    order           avg_sleep min_sleep max_sleep total\n#&gt;    &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;\n#&gt;  1 Chiroptera          19.8       19.7      19.9     2\n#&gt;  2 Didelphimorphia     18.7       18        19.4     2\n#&gt;  3 Cingulata           17.8       17.4      18.1     2\n#&gt;  4 Afrosoricida        15.6       15.6      15.6     1\n#&gt;  5 Pilosa              14.4       14.4      14.4     1\n#&gt;  6 Rodentia            12.5        7        16.6    22\n#&gt;  7 Diprotodontia       12.4       11.1      13.7     2\n#&gt;  8 Soricomorpha        11.1        8.4      14.9     5\n#&gt;  9 Primates            10.5        8        17      12\n#&gt; 10 Erinaceomorpha      10.2       10.1      10.3     2\n#&gt; 11 Carnivora           10.1        3.5      15.8    12\n#&gt; 12 Scandentia           8.9        8.9       8.9     1\n#&gt; 13 Monotremata          8.6        8.6       8.6     1\n#&gt; 14 Lagomorpha           8.4        8.4       8.4     1\n#&gt; 15 Hyracoidea           5.67       5.3       6.3     3\n#&gt; 16 Artiodactyla         4.52       1.9       9.1     6\n#&gt; 17 Cetacea              4.5        2.7       5.6     3\n#&gt; 18 Proboscidea          3.6        3.3       3.9     2\n#&gt; 19 Perissodactyla       3.47       2.9       4.4     3\n\n\ngroup_by(order) suddivide il dataset msleep in gruppi, ciascuno corrispondente a un valore distinto della variabile order.\n\nSuccessivamente, summarise() calcola diverse statistiche per ogni gruppo:\n\n\navg_sleep è la media del totale del sonno (sleep_total) all’interno di ciascun gruppo.\n\nmin_sleep è il valore minimo di sleep_total in ogni gruppo.\n\nmax_sleep è il valore massimo di sleep_total in ogni gruppo.\n\ntotal è il numero di osservazioni (o righe) per ciascun gruppo, calcolato con la funzione n().\n\n\nInfine, arrange(desc(avg_sleep)) ordina i risultati in ordine decrescente in base alla media del sonno totale (avg_sleep), mostrando prima i gruppi con la media di sonno più alta.\n\nQuesto tipo di approccio è utile quando si vuole analizzare come cambiano le caratteristiche dei dati a seconda dei gruppi specifici, fornendo una visione più dettagliata e utile.",
    "crumbs": [
      "R",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Usare `dplyr`</span>"
    ]
  },
  {
    "objectID": "chapters/R/05_dplyr.html#riflessioni-conclusive",
    "href": "chapters/R/05_dplyr.html#riflessioni-conclusive",
    "title": "11  Usare dplyr",
    "section": "Riflessioni conclusive",
    "text": "Riflessioni conclusive\nIl data wrangling è una delle fasi più importanti in qualsiasi pipeline di analisi dei dati. In questo capitolo abbiamo introdotto l’uso del pacchetto tidyverse di R per la manipolazione dei dati e il suo utilizzo in scenari di base. Tuttavia, il tidyverse è un ecosistema ampio e qui abbiamo trattato solo gli elementi fondamentali. Per approfondire, si consiglia di consultare ulteriori risorse come quelle disponibili sul sito web del tidyverse e il libro R for Data Science (2e), di cui esiste anche una traduzione italiana.\n\n\n\n\n\n\nEsercizio\n\n\n\n\n\nIn questo esercizio, utilizzerai il pacchetto dplyr per imparare a manipolare e trasformare i dati della SWLS (Satisfaction With Life Scale). Gli esercizi ti aiuteranno a consolidare la conoscenza dei principali verbi di dplyr, inclusi filter(), select(), mutate(), arrange() e group_by().\nParte 1: Comprensione Teorica\n\n\nCos’è un dataset “tidy”?\n\nDescrivi con parole tue cosa significa avere un dataset “tidy” e quali sono le sue tre caratteristiche principali.\n\n\n\nCos’è la pipe (%&gt;% o |&gt;) e perché è utile?\n\nSpiega a cosa serve l’operatore pipe e fornisci un esempio di utilizzo.\n\n\n\nQuali sono i verbi principali di dplyr?\n\nElenca e spiega brevemente i sei verbi principali di dplyr per la manipolazione dei dati.\n\n\n\nCosa fa il verbo group_by()?\n\nSpiega il suo scopo e come viene utilizzato in combinazione con summarise().\n\n\n\nParte 2: Applicazione Pratica con i Dati SWLS\n\n\nCaricamento dei dati SWLS\n\nCrea un data frame in R contenente i punteggi SWLS che hai raccolto.\n\n\n\nSelezione delle colonne\n\nUsa select() per mantenere solo le colonne con i punteggi degli item.\n\n\n\nFiltraggio dei dati\n\nUsa filter() per selezionare solo gli individui che hanno un punteggio totale superiore a 20.\n\n\n\nCreazione di una nuova colonna\n\nUsa mutate() per calcolare il punteggio totale della SWLS per ciascun individuo e salvarlo in una nuova colonna chiamata punteggio_totale.\n\n\n\nRiordinamento dei dati\n\nUsa arrange() per ordinare il dataset in base al punteggio totale, dal più alto al più basso.\n\n\n\nRaggruppamento e sintesi dei dati\n\n\n\nUsa group_by() e summarise() per calcolare la media e la deviazione standard del punteggio SWLS totale nel dataset.\n\nConsegna\n\nScrivi le risposte della Parte 1 su carta.\nScrivi il codice e i risultati della Parte 2 in un file .R e invialo come consegna.\n\n\n\n\n\n\n\n\n\n\nSoluzione\n\n\n\n\n\nParte 1: Comprensione Teorica\n\n\nCos’è un dataset “tidy”?\n\nUn dataset “tidy” è un dataset organizzato in modo sistematico per facilitare l’analisi. Le sue tre caratteristiche principali sono:\n\nOgni variabile è una colonna.\nOgni osservazione è una riga.\nOgni valore è una cella.\n\n\n\n\n\nCos’è la pipe (%&gt;% o |&gt;) e perché è utile?\n\nLa pipe (%&gt;% o |&gt;) permette di concatenare più operazioni di manipolazione dati in modo leggibile ed efficiente.\n\nEsempio:\ndf |&gt; \n  filter(score &gt; 20) |&gt; \n  select(name, score)\n\n\n\n\nQuali sono i verbi principali di dplyr?\n\n\nselect(): Seleziona colonne.\n\nfilter(): Filtra righe.\n\narrange(): Riordina le righe.\n\nmutate(): Crea nuove colonne.\n\nsummarise(): Riassume i dati.\n\ngroup_by(): Permette di raggruppare i dati.\n\n\n\nCosa fa il verbo group_by()?\n\ngroup_by() suddivide i dati in gruppi, permettendo di applicare funzioni di aggregazione con summarise().\n\nEsempio:\ndf |&gt; \n  group_by(gruppo) |&gt; \n  summarise(media = mean(score), sd = sd(score))\n\n\n\n\nParte 2: Applicazione Pratica con i Dati SWLS\n\n\nCaricamento dei dati SWLS Per svolgere l’esercizio, simuliamo i dati di 10 individui su 5 item (numeri casuali da 1 a 7):\nset.seed(123)\nswls &lt;- data.frame(\n  id = 1:10,\n  item1 = sample(1:7, 10, replace = TRUE),\n  item2 = sample(1:7, 10, replace = TRUE),\n  item3 = sample(1:7, 10, replace = TRUE),\n  item4 = sample(1:7, 10, replace = TRUE),\n  item5 = sample(1:7, 10, replace = TRUE)\n)\nprint(swls)\n\n\nSelezione delle colonne\nswls_selected &lt;- swls |&gt; select(item1:item5)\n\n\nFiltraggio dei dati\nswls_filtered &lt;- swls |&gt; filter(rowSums(select(swls, item1:item5)) &gt; 20)\n\n\nCreazione di una nuova colonna\nswls &lt;- swls |&gt; mutate(punteggio_totale = rowSums(select(swls, item1:item5)))\n\n\nRiordinamento dei dati\nswls_sorted &lt;- swls |&gt; arrange(desc(punteggio_totale))\n\nRaggruppamento e sintesi dei dati\n\nswls_summary &lt;- swls |&gt; \n  summarise(media = mean(punteggio_totale), sd = sd(punteggio_totale))\nConclusione\nQuesti esercizi hanno mostrato come usare dplyr per manipolare dati in modo efficace e leggibile.\n\n\n\n\n\n\n\n\n\nInformazioni sull’ambiente di sviluppo\n\n\n\n\n\n\nsessionInfo()\n#&gt; R version 4.5.1 (2025-06-13)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.6.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] missForest_1.5        mice_3.18.0           pillar_1.11.0        \n#&gt;  [4] tinytable_0.13.0      patchwork_1.3.2       ggdist_3.3.3         \n#&gt;  [7] tidybayes_3.0.7       bayesplot_1.14.0      ggplot2_3.5.2        \n#&gt; [10] reliabilitydiag_0.2.1 priorsense_1.1.1      posterior_1.6.1      \n#&gt; [13] loo_2.8.0             rstan_2.32.7          StanHeaders_2.32.10  \n#&gt; [16] brms_2.22.0           Rcpp_1.1.0            sessioninfo_1.2.3    \n#&gt; [19] conflicted_1.2.0      janitor_2.2.1         matrixStats_1.5.0    \n#&gt; [22] modelr_0.1.11         tibble_3.3.0          dplyr_1.1.4          \n#&gt; [25] tidyr_1.3.1           rio_1.2.3             here_1.0.1           \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;  [1] Rdpack_2.6.4          gridExtra_2.3         inline_0.3.21        \n#&gt;  [4] sandwich_3.1-1        rlang_1.1.6           magrittr_2.0.3       \n#&gt;  [7] multcomp_1.4-28       snakecase_0.11.1      compiler_4.5.1       \n#&gt; [10] systemfonts_1.2.3     vctrs_0.6.5           stringr_1.5.1        \n#&gt; [13] pkgconfig_2.0.3       shape_1.4.6.1         arrayhelpers_1.1-0   \n#&gt; [16] fastmap_1.2.0         backports_1.5.0       utf8_1.2.6           \n#&gt; [19] rmarkdown_2.29        itertools_0.1-3       nloptr_2.2.1         \n#&gt; [22] ragg_1.5.0            purrr_1.1.0           jomo_2.7-6           \n#&gt; [25] xfun_0.53             glmnet_4.1-10         randomForest_4.7-1.2 \n#&gt; [28] cachem_1.1.0          jsonlite_2.0.0        pan_1.9              \n#&gt; [31] broom_1.0.9           parallel_4.5.1        R6_2.6.1             \n#&gt; [34] stringi_1.8.7         RColorBrewer_1.1-3    rpart_4.1.24         \n#&gt; [37] boot_1.3-32           lubridate_1.9.4       estimability_1.5.1   \n#&gt; [40] iterators_1.0.14      knitr_1.50            zoo_1.8-14           \n#&gt; [43] pacman_0.5.1          nnet_7.3-20           Matrix_1.7-4         \n#&gt; [46] splines_4.5.1         timechange_0.3.0      tidyselect_1.2.1     \n#&gt; [49] abind_1.4-8           codetools_0.2-20      curl_7.0.0           \n#&gt; [52] doRNG_1.8.6.2         pkgbuild_1.4.8        lattice_0.22-7       \n#&gt; [55] withr_3.0.2           bridgesampling_1.1-2  coda_0.19-4.1        \n#&gt; [58] evaluate_1.0.5        survival_3.8-3        RcppParallel_5.1.11-1\n#&gt; [61] rngtools_1.5.2        tensorA_0.36.2.1      checkmate_2.3.3      \n#&gt; [64] foreach_1.5.2         stats4_4.5.1          reformulas_0.4.1     \n#&gt; [67] distributional_0.5.0  generics_0.1.4        rprojroot_2.1.1      \n#&gt; [70] rstantools_2.5.0      scales_1.4.0          minqa_1.2.8          \n#&gt; [73] xtable_1.8-4          glue_1.8.0            emmeans_1.11.2-8     \n#&gt; [76] tools_4.5.1           lme4_1.1-37           mvtnorm_1.3-3        \n#&gt; [79] grid_4.5.1            rbibutils_2.3         QuickJSR_1.8.0       \n#&gt; [82] colorspace_2.1-1      nlme_3.1-168          cli_3.6.5            \n#&gt; [85] textshaping_1.0.3     svUnit_1.0.8          Brobdingnag_1.2-9    \n#&gt; [88] V8_7.0.0              gtable_0.3.6          digest_0.6.37        \n#&gt; [91] TH.data_1.1-4         htmlwidgets_1.6.4     farver_2.1.2         \n#&gt; [94] memoise_2.0.1         htmltools_0.5.8.1     lifecycle_1.0.4      \n#&gt; [97] mitml_0.4-5           MASS_7.3-65",
    "crumbs": [
      "R",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Usare `dplyr`</span>"
    ]
  },
  {
    "objectID": "chapters/R/05_dplyr.html#bibliografia",
    "href": "chapters/R/05_dplyr.html#bibliografia",
    "title": "11  Usare dplyr",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nIrizarry, R. A. (2024). Introduction to Data Science: Data Wrangling and Visualization with R. CRC Press.",
    "crumbs": [
      "R",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Usare `dplyr`</span>"
    ]
  },
  {
    "objectID": "chapters/R/05_dplyr.html#footnotes",
    "href": "chapters/R/05_dplyr.html#footnotes",
    "title": "11  Usare dplyr",
    "section": "",
    "text": "Per comprendere meglio il concetto di “data tidying”, possiamo rifarci a una citazione tratta dal testo di riferimento R for Data Science (2e): “Tidy datasets are all alike, but every messy dataset is messy in its own way.”↩︎",
    "crumbs": [
      "R",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Usare `dplyr`</span>"
    ]
  },
  {
    "objectID": "chapters/R/06_quarto.html",
    "href": "chapters/R/06_quarto.html",
    "title": "12  Quarto",
    "section": "",
    "text": "Introduzione\nLa crisi della riproducibilità scientifica rappresenta una delle sfide più importanti della ricerca contemporanea. Con questo termine ci si riferisce alla difficoltà, riscontrata in diverse discipline, di replicare i risultati degli studi scientifici. Sebbene le definizioni di riproducibilità varino tra i diversi ambiti, un’interpretazione ampiamente condivisa la identifica come la capacità di ottenere gli stessi risultati utilizzando i medesimi dati di input e seguendo gli stessi passaggi computazionali nei metodi e nelle analisi.\nLa pratica scientifica è profondamente radicata nella formazione accademica: ciò che viene insegnato nelle aule universitarie si riflette direttamente nel lavoro svolto nei laboratori, sul campo e nell’analisi dei dati. Riconoscendo questo stretto legame tra didattica e ricerca, molti studiosi sostengono l’importanza di integrare i metodi di riproducibilità nei corsi universitari di data science, sia a livello undergraduate che graduate (Dogucu, 2024). L’educazione alla data science che incorpora la riproducibilità nell’analisi dei dati viene infatti considerata la “controffensiva statistica” alla crisi della riproducibilità.\nIn questo contesto si inserisce Quarto, uno strumento innovativo che affronta direttamente le sfide della crisi della riproducibilità. Quarto si colloca nella tradizione del literate programming, un approccio pioneristico introdotto da Donald Knuth negli anni ’80. Questa metodologia nasce dalla visione di unificare codice e testo descrittivo in un unico documento, rendendo i programmi non solo eseguibili ma anche comprensibili agli esseri umani. L’obiettivo è superare la tradizionale separazione tra codice e documentazione, permettendo di spiegare non solo il funzionamento tecnico di un programma, ma anche le ragioni delle scelte implementative.\nQuesta filosofia risulta particolarmente pertinente nell’ambito della data science e dell’analisi statistica, dove riproducibilità e trasparenza sono requisiti imprescindibili. Quarto eccelle in questo contesto, offrendo la possibilità di integrare in codice, risultati e narrazione. La sua versatilità si manifesta nella capacità di produrre diversi tipi di output - dai report agli articoli scientifici, dalle presentazioni ai documenti tecnici - in vari formati come HTML, PDF e Word, combinando efficacemente testo interpretativo, risultati numerici e visualizzazioni grafiche.\nUn punto di forza distintivo di Quarto è la sua flessibilità nel supportare molteplici linguaggi di programmazione, tra cui R, Python e Julia. Lo strumento può essere utilizzato secondo tre modalità principali: per presentare conclusioni condividendo i risultati senza esporre il codice sottostante; per documentare il processo analitico includendo sia il codice che i risultati, garantendo così piena trasparenza e riproducibilità; e per annotare l’analisi, integrando interpretazioni e motivazioni delle decisioni prese durante il processo analitico.\nNonostante Quarto sia tecnicamente uno strumento da riga di comando (CLI), l’integrazione con RStudio ne semplifica notevolmente l’utilizzo, rendendo l’installazione e l’operatività accessibili anche agli utenti meno esperti nell’uso del terminale. Questa caratteristica, unita alle sue potenti funzionalità, rende Quarto una naturale evoluzione del literate programming, offrendo un ambiente di lavoro che bilancia efficacemente praticità d’uso e rigore scientifico. In questo modo, Quarto si configura come una risposta concreta alle sfide della riproducibilità nella ricerca contemporanea, fornendo gli strumenti necessari per una scienza più trasparente e verificabile. L’obiettivo di questo capitolo è quello di fornire un’introduzione pratica a Quarto.",
    "crumbs": [
      "R",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "chapters/R/06_quarto.html#introduzione",
    "href": "chapters/R/06_quarto.html#introduzione",
    "title": "12  Quarto",
    "section": "",
    "text": "Prerequisiti\n\n\n\n\n\n\nLeggere Reproducibility in the Classroom (Dogucu, 2024).\nLeggere An Introduction to R.\nLeggere R for Data Science (2e).\nLeggere l’?sec-apx-latex-math.\n\n\n\n\n\n\n\n\n\n\nPreparazione del Notebook\n\n\n\n\n\n\nhere::here(\"code\", \"_common.R\") |&gt; source()\n\n# Load packages\nif (!requireNamespace(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyr)\n\n\n\n\n\n12.0.1 Creare un documento Quarto\nUn file Quarto ha estensione .qmd e segue questa struttura:\n\nQuesto file include:\n\nUn’intestazione YAML (metadati del documento).\nBlocchi di codice delimitati da ```.\nTesto scritto in Markdown con formattazioni semplici come titoli (# Titolo), corsivi (*testo*), ecc.\n\n12.0.2 Editor visivo e sorgente\n\n\nEditor visivo: simile a Google Docs, offre un’interfaccia WYSIWYM (What You See Is What You Mean). Consente di inserire facilmente immagini, tabelle, citazioni e altro.\n\nEditor sorgente: consente un controllo diretto sul Markdown, utile per debug e personalizzazioni avanzate.\n\n12.0.3 Blocchi di codice\nI blocchi di codice (chiamati “chunks”) eseguono codice e visualizzano i risultati. Ogni chunk è delimitato da ``` e può includere opzioni specifiche:\n#| label: esempio\n#| echo: false\n1 + 1\nLe opzioni più comuni includono:\n\n\necho: false (nasconde il codice nel report),\n\neval: false (non esegue il codice),\n\nmessage: false e warning: false (nasconde messaggi o avvisi).\n\n12.0.4 Figure\nLe figure possono essere generate tramite codice (es. ggplot()) o inserite come file esterni. Le opzioni più comuni per il controllo delle dimensioni sono:\n\n\nfig-width e fig-height (dimensioni della figura in pollici),\n\nout-width (percentuale di larghezza del documento),\n\nfig-asp (rapporto d’aspetto, es. 0.618 per il rapporto aureo).\n\nEsempio:\n#| fig-width: 6\nggplot(data, aes(x, y)) + geom_point()\n\n12.0.5 Equazioni\nLe equazioni possono essere scritte in LaTeX, così come spiegato nell’?sec-apx-latex-math.\n\n12.0.6 Tabelle\nLe tabelle possono essere stampate direttamente o personalizzate con funzioni come knitr::kable() o pacchetti come gt:\n\nknitr::kable(head(mtcars))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.62\n16.5\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.88\n17.0\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.32\n18.6\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.21\n19.4\n1\n0\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.44\n17.0\n0\n0\n3\n2\n\n\nValiant\n18.1\n6\n225\n105\n2.76\n3.46\n20.2\n1\n0\n3\n1\n\n\n\n\n\n\n12.0.7 Caching\nPer velocizzare i documenti con calcoli complessi, Quarto supporta la memorizzazione dei risultati:\n\n\ncache: true salva i risultati di un chunk, evitando di ricalcolarli se il codice non cambia.\n\ndependson specifica dipendenze tra chunk.\n\n12.0.8 Gestione delle Citazioni e delle Bibliografie in Quarto\nQuarto offre un supporto avanzato per la generazione automatica di citazioni e bibliografie, consentendo l’applicazione di formati personalizzati come lo stile APA. Per includere riferimenti bibliografici, è necessario creare un file .bib (ad esempio, references.bib) contenente le citazioni nel formato BibTeX. Queste citazioni possono essere ottenute direttamente da Google Scholar o altri database accademici.\nEcco un esempio di una citazione in formato BibTeX:\n@article{ceccarini2024age,\n  title={Age-dependent changes in the anger superiority effect: Evidence from a visual search task},\n  author={Ceccarini, Francesco and Colpizzi, Ilaria and Caudek, Corrado},\n  journal={Psychonomic Bulletin \\& Review},\n  pages={1--10},\n  year={2024},\n  publisher={Springer}\n}\nQuesta citazione deve essere inserita in un file .bib, ad esempio, references.bib. Tale file dovrà poi essere specificato nell’intestazione del documento Quarto.\n\n12.0.8.1 Configurazione dell’Intestazione YAML\nNel file .qmd, è necessario aggiungere le seguenti righe all’intestazione YAML per collegare il file references.bib e configurare lo stile della bibliografia:\nbibliography: references.bib\nbiblio-style: apalike\ncsl: apa.csl\n\n\nbibliography: Specifica il percorso del file .bib. In questo esempio, si assume che il file si trovi nella stessa cartella del documento Quarto.\n\nbiblio-style: Imposta lo stile delle citazioni. Ad esempio, apalike è uno stile simile allo stile APA.\n\ncsl: Consente di utilizzare uno stile di citazione personalizzato, come apa.csl. Puoi scaricare facilmente questi stili dal Zotero Style Repository.\n\n12.0.8.2 Esempio Completo\nDi seguito è riportato un esempio completo di un documento Quarto che include una citazione e genera automaticamente la bibliografia:\n---\ntitle: \"Articolo di Esempio\"\nauthor: \"Autore di Esempio\"\ndate: \"2025-09-09\"\nbibliography: references.bib\nbiblio-style: apalike\ncsl: apa.csl\n---\n\n## Introduzione\n\nIn questo articolo, discutiamo i cambiamenti dipendenti dall'età nell'anger-superiority effect [@ceccarini2024age].\n\n## Risultati\n\nI risultati mostrano che...\n\n## Riferimenti\nIn questo esempio, l’identificatore @ceccarini2024age viene utilizzato per fare riferimento alla citazione contenuta nel file references.bib. Al momento della compilazione, Quarto genererà automaticamente la lista dei riferimenti bibliografici in base al formato specificato.\n\n12.0.8.3 Citazioni Inline\nAll’interno di un documento .qmd, le citazioni vengono aggiunte utilizzando il simbolo @ seguito dall’identificativo della citazione specificato nel file .bib. Ad esempio:\n... come evidenziato da @ceccarini2024age, si osserva che...\nQuarto genera automaticamente la bibliografia, includendo solo i riferimenti effettivamente citati nel documento. La bibliografia viene aggiunta alla fine del file renderizzato (ad esempio, in formato HTML o PDF).\nAd esempio, nel caso di un documento .qmd, il testo sopra sarà visualizzato così:\n\n… come evidenziato da Ceccarini et al. (2024), si osserva che…\n\nLa citazione completa sarà inclusa automaticamente nella bibliografia, posizionata alla fine della pagina web o del documento finale. Si noti che Quarto gestisce automaticamente la formattazione e la posizione della bibliografia, garantendo coerenza e precisione.\n\nEsempio 12.1 Per fare un esempio pratico, possiamo inserire la citazione @ceccarini2024age direttamente nel file .qmd di questa pagina web. Quando il documento viene compilato, Quarto renderà la citazione in modo appropriato, come mostrato qui: Ceccarini et al. (2024).\nSi noti che, in fondo a questa pagina web, è presente un riferimento bibliografico corrispondente. Questo riferimento è stato aggiunto automaticamente da Quarto in risposta all’uso della citazione @ceccarini2024age nel testo del documento. Questo processo automatizzato semplifica la gestione delle citazioni e garantisce che tutti i riferimenti siano correttamente inclusi e formattati.",
    "crumbs": [
      "R",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "chapters/R/06_quarto.html#riflessioni-conclusive",
    "href": "chapters/R/06_quarto.html#riflessioni-conclusive",
    "title": "12  Quarto",
    "section": "Riflessioni conclusive",
    "text": "Riflessioni conclusive\nQuarto è uno strumento potente per la creazione di documenti riproducibili e ben strutturati, integrando codice, risultati e testo descrittivo in un unico file. Questa introduzione dovrebbe essere sufficiente per iniziare a lavorare con Quarto, ma c’è ancora molto da imparare. Il modo migliore per rimanere aggiornati è consultare il sito ufficiale di Quarto: https://quarto.org.\nUn argomento importante che non abbiamo trattato qui riguarda i dettagli di come comunicare in modo accurato le proprie idee agli altri. Per migliorare le proprie capacità di scrittura, Wickham et al. (2023) consigliano due libri: Style: Lessons in Clarity and Grace di Joseph M. Williams & Joseph Bizup, e The Sense of Structure: Writing from the Reader’s Perspective di George Gopen. Una serie di brevi articoli sulla scrittura sono offerti da George Gopen e sono disponibili su https://www.georgegopen.com/litigation-articles.html.\n\n\n\n\n\n\nEsercizio\n\n\n\n\n\nIn questo esercizio esplorerai l’importanza della riproducibilità nella scienza dei dati e le funzionalità principali di Quarto.\n1. Concetti di base sulla riproducibilità\n\nCos’è la crisi della riproducibilità e perché è rilevante nella scienza dei dati?\nIn che modo Quarto può aiutare ad affrontare la crisi della riproducibilità?\nSpiega il concetto di literate programming e come si collega a Quarto.\n\n2. Struttura di un file Quarto\n\nQual è l’estensione di un file Quarto e quali sono le sue tre sezioni principali?\nQual è la differenza tra editor visivo ed editor sorgente in Quarto?\nQual è la funzione dell’intestazione YAML in un file .qmd?\n\n3. Blocchi di codice e opzioni\n\nCome si scrive un blocco di codice in Quarto?\nQuali opzioni puoi utilizzare nei blocchi di codice per controllare l’esecuzione e la visualizzazione del codice e dei risultati?\nScrivi un blocco di codice Quarto che calcola la media di un vettore di numeri e stampa il risultato senza mostrare il codice.\n\n4. Figure e Tabelle\n\nQuali opzioni di formattazione delle figure offre Quarto?\nCome puoi creare una tabella formattata in Quarto usando knitr::kable()?\n\n5. Citazioni e Bibliografia\n\nCome si aggiunge una citazione bibliografica in Quarto?\nQuali file devono essere inclusi per gestire una bibliografia in Quarto?\nScrivi un esempio di citazione in formato BibTeX e mostra come collegarla a un documento .qmd.\n\n6. Considerazioni Finali\n\nQuali sono i vantaggi di usare Quarto rispetto a strumenti più tradizionali come Word per la creazione di report scientifici?\n\n\n\n\n\n\n\n\n\n\nSoluzione\n\n\n\n\n\n1. Concetti di base sulla riproducibilità\n\nLa crisi della riproducibilità è il fenomeno per cui molti studi scientifici non possono essere replicati con gli stessi metodi e dati. Questo mina la fiducia nella scienza e può portare a risultati non affidabili.\nQuarto aiuta la riproducibilità integrando codice, testo e risultati in un unico documento, rendendo più semplice verificare e riprodurre le analisi.\n\nLiterate programming è un approccio introdotto da Donald Knuth che combina codice e spiegazioni testuali nello stesso file, migliorando la comprensione e documentazione delle analisi. Quarto segue questa filosofia.\n\n2. Struttura di un file Quarto\n\n\nL’estensione di un file Quarto è .qmd. Le tre sezioni principali sono:\n\nL’intestazione YAML (metadati),\nIl codice (chunks),\nIl testo scritto in Markdown.\n\n\nL’editor visivo è un’interfaccia intuitiva simile a Google Docs, mentre l’editor sorgente permette di scrivere direttamente in Markdown e codice.\nL’intestazione YAML definisce le proprietà del documento come titolo, autore, formato di output e opzioni di rendering.\n\n3. Blocchi di codice e opzioni\n\n\nUn blocco di codice in Quarto si scrive con tripli backtick (```) e un linguaggio specificato:\n#| echo: true\nprint(\"Esempio di codice in Quarto\")\n\n\nAlcune opzioni utili nei blocchi di codice sono:\n\n\necho: false per nascondere il codice,\n\neval: false per non eseguire il codice,\n\nwarning: false e message: false per nascondere messaggi e avvisi.\n\n\n\nEsempio di blocco di codice che calcola una media senza mostrare il codice:\n#| echo: false\nmean(c(1, 2, 3, 4, 5))\n\n\n\n12.1 4. Figure e Tabelle\n\n\nLe opzioni principali per le figure includono:\n\n\nfig-width e fig-height per le dimensioni,\n\nout-width per la larghezza relativa,\n\nfig-asp per il rapporto d’aspetto.\n\n\n\nPer creare una tabella formattata con knitr::kable():\nknitr::kable(head(mtcars))\n\n\n5. Citazioni e Bibliografia\n\nLe citazioni in Quarto si aggiungono usando il simbolo @ seguito dal riferimento BibTeX (es. @ceccarini2024age).\n\nPer gestire la bibliografia in Quarto servono:\n\nUn file .bib con le citazioni,\nUn’intestazione YAML che collega il file .bib e specifica lo stile (csl).\n\n\n\nEsempio di citazione BibTeX e collegamento in YAML:\n@article{ceccarini2024age,\n  title={Age-dependent changes in the anger superiority effect},\n  author={Ceccarini, Francesco et al.},\n  journal={Psychonomic Bulletin & Review},\n  year={2024}\n}\nYAML:\nbibliography: references.bib\nbiblio-style: apalike\n\n\n6. Considerazioni Finali\n\n\nI vantaggi di Quarto rispetto a Word includono:\n\nmaggiore riproducibilità e trasparenza,\npossibilità di integrare codice ed esecuzione in un unico documento,\nfacilità di gestione delle citazioni automatiche,\nsupporto per diversi formati di output (HTML, PDF, Word).\n\n\n\n\n\n\n\n\n\n\n\n\nInformazioni sull’ambiente di sviluppo\n\n\n\n\n\n\nsessionInfo()\n#&gt; R version 4.5.1 (2025-06-13)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.6.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] pillar_1.11.0         tinytable_0.13.0      patchwork_1.3.2      \n#&gt;  [4] ggdist_3.3.3          tidybayes_3.0.7       bayesplot_1.14.0     \n#&gt;  [7] ggplot2_3.5.2         reliabilitydiag_0.2.1 priorsense_1.1.1     \n#&gt; [10] posterior_1.6.1       loo_2.8.0             rstan_2.32.7         \n#&gt; [13] StanHeaders_2.32.10   brms_2.22.0           Rcpp_1.1.0           \n#&gt; [16] sessioninfo_1.2.3     conflicted_1.2.0      janitor_2.2.1        \n#&gt; [19] matrixStats_1.5.0     modelr_0.1.11         tibble_3.3.0         \n#&gt; [22] dplyr_1.1.4           tidyr_1.3.1           rio_1.2.3            \n#&gt; [25] here_1.0.1           \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;  [1] svUnit_1.0.8          tidyselect_1.2.1      farver_2.1.2         \n#&gt;  [4] fastmap_1.2.0         TH.data_1.1-4         tensorA_0.36.2.1     \n#&gt;  [7] pacman_0.5.1          digest_0.6.37         timechange_0.3.0     \n#&gt; [10] estimability_1.5.1    lifecycle_1.0.4       survival_3.8-3       \n#&gt; [13] magrittr_2.0.3        compiler_4.5.1        rlang_1.1.6          \n#&gt; [16] tools_4.5.1           knitr_1.50            bridgesampling_1.1-2 \n#&gt; [19] htmlwidgets_1.6.4     curl_7.0.0            pkgbuild_1.4.8       \n#&gt; [22] RColorBrewer_1.1-3    abind_1.4-8           multcomp_1.4-28      \n#&gt; [25] withr_3.0.2           purrr_1.1.0           grid_4.5.1           \n#&gt; [28] stats4_4.5.1          colorspace_2.1-1      xtable_1.8-4         \n#&gt; [31] inline_0.3.21         emmeans_1.11.2-8      scales_1.4.0         \n#&gt; [34] MASS_7.3-65           cli_3.6.5             mvtnorm_1.3-3        \n#&gt; [37] rmarkdown_2.29        ragg_1.5.0            generics_0.1.4       \n#&gt; [40] RcppParallel_5.1.11-1 cachem_1.1.0          stringr_1.5.1        \n#&gt; [43] splines_4.5.1         parallel_4.5.1        vctrs_0.6.5          \n#&gt; [46] V8_7.0.0              Matrix_1.7-4          sandwich_3.1-1       \n#&gt; [49] jsonlite_2.0.0        arrayhelpers_1.1-0    systemfonts_1.2.3    \n#&gt; [52] glue_1.8.0            codetools_0.2-20      distributional_0.5.0 \n#&gt; [55] lubridate_1.9.4       stringi_1.8.7         gtable_0.3.6         \n#&gt; [58] QuickJSR_1.8.0        htmltools_0.5.8.1     Brobdingnag_1.2-9    \n#&gt; [61] R6_2.6.1              textshaping_1.0.3     rprojroot_2.1.1      \n#&gt; [64] evaluate_1.0.5        lattice_0.22-7        backports_1.5.0      \n#&gt; [67] memoise_2.0.1         broom_1.0.9           snakecase_0.11.1     \n#&gt; [70] rstantools_2.5.0      coda_0.19-4.1         gridExtra_2.3        \n#&gt; [73] nlme_3.1-168          checkmate_2.3.3       xfun_0.53            \n#&gt; [76] zoo_1.8-14            pkgconfig_2.0.3",
    "crumbs": [
      "R",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "chapters/R/06_quarto.html#figure-e-tabelle",
    "href": "chapters/R/06_quarto.html#figure-e-tabelle",
    "title": "12  Quarto",
    "section": "\n12.1 4. Figure e Tabelle",
    "text": "12.1 4. Figure e Tabelle\n\n\nLe opzioni principali per le figure includono:\n\n\nfig-width e fig-height per le dimensioni,\n\nout-width per la larghezza relativa,\n\nfig-asp per il rapporto d’aspetto.\n\n\n\nPer creare una tabella formattata con knitr::kable():\nknitr::kable(head(mtcars))\n\n\n5. Citazioni e Bibliografia\n\nLe citazioni in Quarto si aggiungono usando il simbolo @ seguito dal riferimento BibTeX (es. @ceccarini2024age).\n\nPer gestire la bibliografia in Quarto servono:\n\nUn file .bib con le citazioni,\nUn’intestazione YAML che collega il file .bib e specifica lo stile (csl).\n\n\n\nEsempio di citazione BibTeX e collegamento in YAML:\n@article{ceccarini2024age,\n  title={Age-dependent changes in the anger superiority effect},\n  author={Ceccarini, Francesco et al.},\n  journal={Psychonomic Bulletin & Review},\n  year={2024}\n}\nYAML:\nbibliography: references.bib\nbiblio-style: apalike\n\n\n6. Considerazioni Finali\n\n\nI vantaggi di Quarto rispetto a Word includono:\n\nmaggiore riproducibilità e trasparenza,\npossibilità di integrare codice ed esecuzione in un unico documento,\nfacilità di gestione delle citazioni automatiche,\nsupporto per diversi formati di output (HTML, PDF, Word).",
    "crumbs": [
      "R",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "chapters/R/06_quarto.html#bibliografia",
    "href": "chapters/R/06_quarto.html#bibliografia",
    "title": "12  Quarto",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nCeccarini, F., Colpizzi, I., & Caudek, C. (2024). Age-dependent changes in the anger superiority effect: Evidence from a visual search task. Psychonomic Bulletin & Review, 1–10.\n\n\nDogucu, M. (2024). Reproducibility in the Classroom. Annual Review of Statistics and Its Application, 12.\n\n\nWickham, H., Çetinkaya-Rundel, M., & Grolemund, G. (2023). R for data science. \" O’Reilly Media, Inc.\".",
    "crumbs": [
      "R",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "chapters/R/07_environment.html",
    "href": "chapters/R/07_environment.html",
    "title": "13  L’ambiente di programmazione",
    "section": "",
    "text": "Introduzione\nProgrammare presuppone necessariamente un ambiente di programmazione. Una cattiva gestione di questo ambiente può causare il malfunzionamento degli script, rendendo fondamentale imparare a gestirlo correttamente.\nL’ambiente può influenzare persino il comportamento delle funzioni più basilari. Considera il seguente esempio:\nQuesto potrebbe produrre il seguente output:\nTuttavia, modificando l’opzione di larghezza in R, il comportamento cambia:\nIn questo caso, l’output potrebbe essere:\nLa differenza è dovuta a un’opzione dell’ambiente, width, che regola il numero massimo di caratteri visualizzati per ogni riga.",
    "crumbs": [
      "R",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>L'ambiente di programmazione</span>"
    ]
  },
  {
    "objectID": "chapters/R/07_environment.html#introduzione",
    "href": "chapters/R/07_environment.html#introduzione",
    "title": "13  L’ambiente di programmazione",
    "section": "",
    "text": "Nota: In R, il termine “ambiente” ha un significato specifico, riferendosi allo spazio di lavoro in cui gli oggetti vengono memorizzati durante una sessione. In questo capitolo, tuttavia, il termine si riferisce allo stato complessivo del tuo computer mentre programmi, inclusa l’organizzazione dei file, la versione di R che stai usando e altre impostazioni.\n\n\nprint(1:9)\n\n[1] 1 2 3 4 5 6 7 8 9\n\n# Dopo aver modificato options(width)\noptions(width = 10)\nprint(1:9)\n\n[1] 1 2 3\n[4] 4 5 6\n[7] 7 8 9\n\n\n\n\n\n\n\nPrerequisiti\n\n\n\n\n\n\nLeggere il capitolo 3, “Setting Up Your Data Science Project”, del libro Veridical Data Science (Yu & Barter, 2024).\nLeggere il ?sec-apx-shell della dispensa.\n\n\n\n\n\n\n\n\n\n\nPreparazione del Notebook\n\n\n\n\n\n\nhere::here(\"code\", \"_common.R\") |&gt; \n  source()\n\n# Load packages\nif (!requireNamespace(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyr)",
    "crumbs": [
      "R",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>L'ambiente di programmazione</span>"
    ]
  },
  {
    "objectID": "chapters/R/07_environment.html#file-system",
    "href": "chapters/R/07_environment.html#file-system",
    "title": "13  L’ambiente di programmazione",
    "section": "\n13.1 File system",
    "text": "13.1 File system\nPrima di iniziare a organizzare un progetto in R, è fondamentale seguire alcune linee guida per strutturare e nominare i file in modo efficace. Spesso si tende a sottovalutare l’importanza di una buona organizzazione, ma adottare un sistema coerente può far risparmiare tempo prezioso nella ricerca e gestione dei progetti passati. Danielle Navarro ha creato una presentazione sulla struttura dei progetti, nella quale propone tre principi fondamentali per la gestione dei file:\n\nessere gentili con le macchine;\nessere gentili con gli esseri umani;\nfacilitare l’ordinamento e la ricerca.\n\n\n13.1.1 Essere gentili con le macchine\nLe macchine possono confondersi con spazi, caratteri speciali (come ^.*?+|$\"), e lettere accentate. Per evitare problemi:\n\nusa solo lettere minuscole, numeri, trattini _ o -;\nevita caratteri speciali e spazi nei nomi dei file;\nevita le lettere accentate;\nusa estensioni coerenti, come .R per gli script R.\n\nEsempi:\n# Buono\nprogetto01_analisi_dati.R\n\n# Cattivo\nProgetto \"Analisi Dati\".R\n\n13.1.2 Essere gentili con gli umani\nGli esseri umani hanno bisogno di contesto. Evita nomi vaghi e usa descrizioni significative.\n# Buono\nanalisi01_statistiche_descrittive.R\nnote02_intro_modello.docx\n\n# Cattivo\n01.R\nappunti.docx\n\n\n\n\n\n\nEvitate categoricamente l’uso di spazi nei nomi di file, cartelle o oggetti in R. Anche se il sistema operativo potrebbe consentirlo, questa pratica può generare problemi futuri, complicare il debugging e rendere il codice meno leggibile e portabile. Per evitare questi inconvenienti, adottate sempre nomi privi di spazi, preferendo separatori come trattini bassi (_) o trattini (-).\n\n\n\n\n13.1.3 Facilitare l’ordinamento e la ricerca\nSe i nomi dei file includono date, usa sempre il formato YYYY-MM-DD per permettere un ordinamento automatico.\n# Buono\n2024-01-01_analisi.R\n2024-02-15_riassunto.docx\n\n# Cattivo\n1-gennaio-2024.R\nriassunto-15-02-2024.docx\nSe devi ordinare i file in base a qualcosa di diverso dalle date, usa numeri con lo zero iniziale per mantenere l’ordine.\nreading01_shakespeare_romeo-and-juliet.docx\nreading02_shakespeare_romeo-and-juliet.docx\n...\nreading11_shakespeare_romeo-and-juliet.docx\nnotes01_shakespeare_romeo-and-juliet.docx\n...",
    "crumbs": [
      "R",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>L'ambiente di programmazione</span>"
    ]
  },
  {
    "objectID": "chapters/R/07_environment.html#versioni-di-r-e-pacchetti",
    "href": "chapters/R/07_environment.html#versioni-di-r-e-pacchetti",
    "title": "13  L’ambiente di programmazione",
    "section": "\n13.2 Versioni di R e pacchetti",
    "text": "13.2 Versioni di R e pacchetti\nAggiornare regolarmente R e i pacchetti è essenziale per evitare bug e sfruttare le nuove funzionalità. Ecco alcune buone pratiche:\n\nEsegui update.packages() ogni poche settimane per aggiornare i pacchetti.\nAggiorna la versione di R ogni pochi mesi. Su Windows puoi usare il pacchetto installr, mentre su altri sistemi puoi scaricare l’ultima versione dal sito ufficiale di R.\nMantieni aggiornato anche il sistema operativo.",
    "crumbs": [
      "R",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>L'ambiente di programmazione</span>"
    ]
  },
  {
    "objectID": "chapters/R/07_environment.html#progetti-in-r",
    "href": "chapters/R/07_environment.html#progetti-in-r",
    "title": "13  L’ambiente di programmazione",
    "section": "\n13.3 Progetti in R",
    "text": "13.3 Progetti in R\nSe hai seguito i consigli finora, avrai creato una cartella per tutti i tuoi progetti di programmazione e la tua installazione di R sarà aggiornata. Ora è il momento di organizzare i tuoi progetti in R.\n\n13.3.1 Percorsi assoluti e relativi\nUn percorso assoluto parte dalla directory principale del tuo computer (ad esempio, / su Linux/MacOS o C:/ su Windows) e indica in modo completo e univoco la posizione di un file o di una cartella. Un percorso relativo, invece, parte dalla directory corrente del progetto o dalla directory di lavoro impostata e descrive la posizione di un file in relazione a questa.\nAd esempio, il percorso assoluto del file utilizzato per generare questa pagina HTML potrebbe essere ottenuto così:\n\nfs::path_abs(\"05_environment.qmd\")\n#&gt; /Users/corrado/_repositories/utet-eda/chapters/R/05_environment.qmd",
    "crumbs": [
      "R",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>L'ambiente di programmazione</span>"
    ]
  },
  {
    "objectID": "chapters/R/07_environment.html#funzione-here",
    "href": "chapters/R/07_environment.html#funzione-here",
    "title": "13  L’ambiente di programmazione",
    "section": "\n13.4 Funzione here()\n",
    "text": "13.4 Funzione here()\n\nSe il progetto si trova nella cartella psicometria-r, possiamo utilizzare la funzione here() del pacchetto here per indicare la posizione del file 05_environment.qmd in modo relativo. Ecco un esempio:\n\nfile.exists(here::here(\"chapters\", \"R\", \"07_environment.qmd\"))\n#&gt; [1] TRUE\n\nIn questo caso, il file 07_environment.qmd è contenuto nella cartella chapters/R, che si trova all’interno della directory principale del progetto. Grazie a here(), non è necessario specificare manualmente la posizione del progetto: questa funzione identifica automaticamente la directory principale e consente di indicare solo il percorso relativo del file rispetto ad essa. In altre parole, puoi riferirti al file 07_environment.qmd semplicemente fornendo il percorso relativo all’interno della struttura del progetto, lasciando a here() il compito di gestire il contesto globale.\n\n13.4.1 Perché preferire i percorsi relativi?\nL’utilizzo di percorsi relativi con here() offre numerosi vantaggi:\n\n\nPortabilità: Il codice diventa più semplice da condividere, poiché non dipende dalla struttura delle directory specifica del computer su cui è stato scritto.\n\nOrganizzazione: Favorisce una struttura chiara e coerente all’interno del progetto, rendendo più facile individuare e accedere ai file.\n\nAffidabilità: Riduce il rischio di errori dovuti a percorsi assoluti errati, soprattutto quando il progetto viene spostato o condiviso.\n\n13.4.2 Buone pratiche\n\n\nUsare sempre percorsi relativi: Questo assicura che il progetto sia facilmente eseguibile su altri sistemi senza necessità di modifiche ai percorsi.\n\nImpostare una struttura coerente del progetto: Organizzare i file in cartelle ben definite (ad esempio, data, scripts, outputs) facilita l’uso di percorsi relativi.\n\nIn sintesi, specificare i percorsi relativi rispetto alla directory principale del progetto è una buona pratica essenziale per garantire portabilità, organizzazione e riproducibilità del lavoro.\n\n13.4.3 Creare un progetto in R\nUn progetto R è semplicemente una cartella con un file .Rproj. Puoi crearne uno con RStudio o con il pacchetto usethis.\nIn RStudio:\n\nVai su File &gt; New Project.\nSeleziona New Directory &gt; New Project.\nDai un nome al progetto e scegli la sua posizione.\n\nCon usethis:\nusethis::create_project(\"path/alla/cartella\")\nEsempio:\nusethis::create_project(\"/Users/corrado/_repositories/psicometria-r\")\nVedremo nel Capitolo 14 come organizzare i file all’interno di un progetto.\n\n\n\n\n\n\nInformazioni sull’ambiente di sviluppo\n\n\n\n\n\n\nsessionInfo()\n#&gt; R version 4.5.1 (2025-06-13)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.6.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] pillar_1.11.0         tinytable_0.13.0      patchwork_1.3.2      \n#&gt;  [4] ggdist_3.3.3          tidybayes_3.0.7       bayesplot_1.14.0     \n#&gt;  [7] ggplot2_3.5.2         reliabilitydiag_0.2.1 priorsense_1.1.1     \n#&gt; [10] posterior_1.6.1       loo_2.8.0             rstan_2.32.7         \n#&gt; [13] StanHeaders_2.32.10   brms_2.22.0           Rcpp_1.1.0           \n#&gt; [16] sessioninfo_1.2.3     conflicted_1.2.0      janitor_2.2.1        \n#&gt; [19] matrixStats_1.5.0     modelr_0.1.11         tibble_3.3.0         \n#&gt; [22] dplyr_1.1.4           tidyr_1.3.1           rio_1.2.3            \n#&gt; [25] here_1.0.1           \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;  [1] svUnit_1.0.8          tidyselect_1.2.1      farver_2.1.2         \n#&gt;  [4] fastmap_1.2.0         TH.data_1.1-4         tensorA_0.36.2.1     \n#&gt;  [7] pacman_0.5.1          digest_0.6.37         timechange_0.3.0     \n#&gt; [10] estimability_1.5.1    lifecycle_1.0.4       survival_3.8-3       \n#&gt; [13] magrittr_2.0.3        compiler_4.5.1        rlang_1.1.6          \n#&gt; [16] tools_4.5.1           knitr_1.50            bridgesampling_1.1-2 \n#&gt; [19] htmlwidgets_1.6.4     curl_7.0.0            pkgbuild_1.4.8       \n#&gt; [22] RColorBrewer_1.1-3    abind_1.4-8           multcomp_1.4-28      \n#&gt; [25] withr_3.0.2           purrr_1.1.0           grid_4.5.1           \n#&gt; [28] stats4_4.5.1          colorspace_2.1-1      xtable_1.8-4         \n#&gt; [31] inline_0.3.21         emmeans_1.11.2-8      scales_1.4.0         \n#&gt; [34] MASS_7.3-65           cli_3.6.5             mvtnorm_1.3-3        \n#&gt; [37] crayon_1.5.3          rmarkdown_2.29        ragg_1.5.0           \n#&gt; [40] generics_0.1.4        RcppParallel_5.1.11-1 cachem_1.1.0         \n#&gt; [43] stringr_1.5.1         splines_4.5.1         parallel_4.5.1       \n#&gt; [46] vctrs_0.6.5           V8_7.0.0              Matrix_1.7-4         \n#&gt; [49] sandwich_3.1-1        jsonlite_2.0.0        arrayhelpers_1.1-0   \n#&gt; [52] systemfonts_1.2.3     glue_1.8.0            codetools_0.2-20     \n#&gt; [55] distributional_0.5.0  lubridate_1.9.4       stringi_1.8.7        \n#&gt; [58] gtable_0.3.6          QuickJSR_1.8.0        htmltools_0.5.8.1    \n#&gt; [61] Brobdingnag_1.2-9     R6_2.6.1              textshaping_1.0.3    \n#&gt; [64] rprojroot_2.1.1       evaluate_1.0.5        lattice_0.22-7       \n#&gt; [67] backports_1.5.0       memoise_2.0.1         broom_1.0.9          \n#&gt; [70] snakecase_0.11.1      rstantools_2.5.0      coda_0.19-4.1        \n#&gt; [73] gridExtra_2.3         nlme_3.1-168          checkmate_2.3.3      \n#&gt; [76] xfun_0.53             fs_1.6.6              zoo_1.8-14           \n#&gt; [79] pkgconfig_2.0.3",
    "crumbs": [
      "R",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>L'ambiente di programmazione</span>"
    ]
  },
  {
    "objectID": "chapters/R/07_environment.html#bibliografia",
    "href": "chapters/R/07_environment.html#bibliografia",
    "title": "13  L’ambiente di programmazione",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nYu, B., & Barter, R. L. (2024). Veridical data science: The practice of responsible data analysis and decision making. MIT Press.",
    "crumbs": [
      "R",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>L'ambiente di programmazione</span>"
    ]
  },
  {
    "objectID": "chapters/eda/introduction_eda.html",
    "href": "chapters/eda/introduction_eda.html",
    "title": "EDA",
    "section": "",
    "text": "Il Ruolo Fondamentale dell’EDA\nL’analisi esplorativa dei dati (Exploratory Data Analysis, EDA) è un processo attivo di scoperta, un dialogo critico con i dati volto a rivelare ciò che è inatteso. Più che un insieme di tecniche, è un approccio mentale: un modo per mettere alla prova il proprio modello mentale attraverso il confronto sistematico tra aspettative e realtà. Prima di procedere con qualsiasi tipo di visualizzazione, è utile esplicitare le proprie aspettative, per poi verificare se i dati confermano, contraddicono o complicano tali ipotesi preliminari. In questo senso, l’EDA è una forma di model checking intuitivo, in cui l’intuizione viene continuamente affinata dall’evidenza empirica.\nUna volta raccolti i dati, il primo passo dell’analisi statistica consiste nel familiarizzare con essi: esplorarne la struttura, identificarne le regolarità e le anomalie e coglierne le dinamiche sottostanti. Questo processo, formalizzato da John Tukey negli anni ’70 (Tukey et al., 1977), rappresenta un antidoto alla rigidità dei modelli preconcetti. Tukey insisteva sull’importanza di “lasciar parlare i dati” prima di incasellarli in schemi teorici, perché è proprio nell’esplorazione che emergono pattern inattesi, relazioni non lineari o errori sistematici altrimenti invisibili.\nL’EDA non è in contrapposizione all’analisi confermativa (CDA), ma ne rappresenta il naturale complemento. Mentre l’analisi confermativa (CDA) testa ipotesi precise con metodi formali (ad esempio, test di ipotesi o modelli bayesiani), l’analisi esplorativa (EDA) genera tali ipotesi, verificando preliminarmente la plausibilità delle assunzioni di base. In campi come la psicologia, dove i dati sono spesso rumorosi e multidimensionali, questa fase è cruciale per evitare conclusioni fuorvianti.",
    "crumbs": [
      "EDA"
    ]
  },
  {
    "objectID": "chapters/eda/introduction_eda.html#strumenti-e-filosofia",
    "href": "chapters/eda/introduction_eda.html#strumenti-e-filosofia",
    "title": "EDA",
    "section": "Strumenti e Filosofia",
    "text": "Strumenti e Filosofia\nSebbene l’EDA includa strumenti quantitativi (statistiche descrittive, misure di dispersione), la sua essenza si esprime appieno nella visualizzazione. Un grafico ben progettato, che si tratti di un boxplot, di uno scatterplot o di un semplice istogramma, può rivelare asimmetrie, outlier o cluster che una tabella numerica non riuscirebbe a mostrare. Tuttavia, come sottolineato da Hullman & Gelman (2021), la vera forza dell’EDA non risiede nella produzione di immagini, ma nel collegare ciò che si osserva a un modello mentale del processo di generazione dei dati. Visualizzare non significa solo “vedere”, ma anche “confrontare”: ci si chiede se la distribuzione di una variabile corrisponda alle aspettative o se una correlazione apparente resista a un’analisi critica.\nIn questa prospettiva, anche il Posterior Predictive Checking (Gelman et al., 2013) può essere considerato una forma di EDA, in cui l’adeguatezza di un modello viene verificata attraverso confronti grafici tra dati osservati e simulati. Anche senza un modello formale, l’analista compie un esercizio simile, valutando implicitamente la coerenza tra i dati e le proprie aspettative.",
    "crumbs": [
      "EDA"
    ]
  },
  {
    "objectID": "chapters/eda/introduction_eda.html#obiettivi-di-questa-sezione",
    "href": "chapters/eda/introduction_eda.html#obiettivi-di-questa-sezione",
    "title": "EDA",
    "section": "Obiettivi di questa sezione",
    "text": "Obiettivi di questa sezione\nIn questa sezione, ci proponiamo di:\n\nIntrodurre i concetti base della statistica descrittiva (media, deviazione standard, correlazione) e le loro rappresentazioni grafiche (istogrammi, grafici a violino, matrici di scatterplot).\nMostrare applicazioni pratiche in R, utilizzando dataset psicologici reali o simulati, con un focus su come tradurre domande di ricerca in visualizzazioni efficaci.\nDiscutere i limiti dell’approccio puramente descrittivo, enfatizzando la necessità di integrare l’EDA con modelli teorici o causalità.\nCollegare esplorazione e inferenza, illustrando come l’EDA possa evolvere in un controllo modellistico esplicito, specie in contesti bayesiani dove le aspettative precedenti (prior) giocano un ruolo chiave.",
    "crumbs": [
      "EDA"
    ]
  },
  {
    "objectID": "chapters/eda/introduction_eda.html#leda-come-atteggiamento-scientifico",
    "href": "chapters/eda/introduction_eda.html#leda-come-atteggiamento-scientifico",
    "title": "EDA",
    "section": "L’EDA come atteggiamento scientifico",
    "text": "L’EDA come atteggiamento scientifico\nIn conclusione, l’EDA non è un semplice preludio tecnico all’analisi “seria”, ma una componente essenziale del pensiero statistico. Ci spinge a formulare domande migliori, a interpretare le risposte con umiltà e a resistere alla tentazione di cercare conferme facili. Come scrisse Tukey: “È meglio rispondere in modo approssimativo a una domanda giusta, piuttosto che rispondere in modo esatto a una domanda sbagliata”.",
    "crumbs": [
      "EDA"
    ]
  },
  {
    "objectID": "chapters/eda/introduction_eda.html#bibliografia",
    "href": "chapters/eda/introduction_eda.html#bibliografia",
    "title": "EDA",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nGelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian Data Analysis (3rd ed.). Chapman; Hall/CRC.\n\n\nHullman, J., & Gelman, A. (2021). Challenges in Incorporating Exploratory Data Analysis Into Statistical Workflow. Harvard Data Science Review, 3(3).\n\n\nTukey, J. W. et al. (1977). Exploratory data analysis (Vol. 2). Springer.",
    "crumbs": [
      "EDA"
    ]
  },
  {
    "objectID": "chapters/eda/01_project_structure.html",
    "href": "chapters/eda/01_project_structure.html",
    "title": "14  Il ciclo di vita di un progetto di analisi dei dati",
    "section": "",
    "text": "Introduzione\nNella ricerca psicologica i dati sono il legame concreto tra le teorie e la loro verifica empirica. Gestirli con rigore non significa soltanto tenerli in ordine: è la condizione che rende le analisi affidabili, le conclusioni sensate e i risultati riproducibili.\nUn dataset solido deve avere tre caratteristiche: integrità (i dati non devono essere alterati senza traccia), chiarezza semantica (ogni variabile deve avere un significato esplicito), e tracciabilità (ogni trasformazione deve poter essere ricostruita). Ad esempio: se rinomini un item da Q5 a Ansia_Sociale, questa modifica deve comparire in uno script, non solo nella memoria di chi analizza. Senza questa trasparenza, anche l’analisi più sofisticata perde credibilità e diventa irriproducibile.\nLa riproducibilità, cardine della scienza aperta (Open Science), non è facoltativa. Documentare accuratamente dati e processi permette alla comunità scientifica — e al proprio gruppo di ricerca nel tempo — di riesaminare, validare, criticare o estendere il lavoro. La gestione dei dati non è quindi un compito tecnico secondario, ma un atto scientifico con implicazioni etiche e metodologiche profonde.\nI dati psicologici sono oggi sempre più complessi: possono essere longitudinali (misure ripetute nel tempo), multimodali (questionari, dati EMA da app, registrazioni digitali), e provenire da fonti eterogenee. Ogni dataset riflette scelte teoriche e tecniche: rendere queste scelte esplicite, tracciabili e accessibili significa trasformare una semplice elaborazione in un processo analitico trasparente e criticabile.\nIn questo capitolo vedremo principi e pratiche per costruire un progetto di analisi dati che abbia fin dalle fondamenta chiarezza, rigore e tracciabilità. L’obiettivo non è solo operativo, ma epistemologico: creare un ponte solido tra l’osservazione empirica e l’inferenza scientifica, attraverso strumenti concreti e replicabili.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Il ciclo di vita di un progetto di analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/01_project_structure.html#introduzione",
    "href": "chapters/eda/01_project_structure.html#introduzione",
    "title": "14  Il ciclo di vita di un progetto di analisi dei dati",
    "section": "",
    "text": "Panoramica del capitolo\n\nPianificazione iniziale.\nCome configurare l’ambiente R.\nCome gestire il ciclo di vita di un progetto di Data Science.\nCome garantire la riproducibilità usando Make.\n\n\n\n\n\n\n\nPrerequisiti\n\n\n\n\n\n\nLeggere Veridical Data Science (Yu & Barter, 2024) focalizzandoti sul primo capitolo, che introduce le problematiche della data science, e sul quarto capitolo, che fornisce le linee guida dettagliate sull’organizzazione di un progetto di analisi dei dati.\n\n\n\n\n\n\n\n\n\n\nPreparazione del Notebook\n\n\n\n\n\n\n# Carica il file _common.R per impostazioni di pacchetti e opzioni\nhere::here(\"code\", \"_common.R\") |&gt; \n  source()\n\n\n\n\n\n\n\n\n\n\nDomande iniziali\n\n\n\n\n\nPrima di immergerti nella lettura di questo capitolo, prenditi un momento per riflettere sulle seguenti domande. Quali risposte daresti prima di leggere il materiale?\n\nPerché è importante organizzare e documentare accuratamente i dati in un progetto di ricerca?\nQuali problemi possono emergere se si inizia a scrivere codice senza una pianificazione adeguata?\nQuali vantaggi offre R per la gestione dei dati rispetto ad altri strumenti?\nQuali strategie potrebbero migliorare la riproducibilità del tuo lavoro?\nCome struttureresti un progetto di analisi dati per mantenerlo chiaro e facilmente replicabile?\n\nOra, mentre leggi il capitolo, confronta le tue risposte con i concetti discussi.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Il ciclo di vita di un progetto di analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/01_project_structure.html#limportanza-della-pianificazione-iniziale",
    "href": "chapters/eda/01_project_structure.html#limportanza-della-pianificazione-iniziale",
    "title": "14  Il ciclo di vita di un progetto di analisi dei dati",
    "section": "\n14.1 L’importanza della pianificazione iniziale",
    "text": "14.1 L’importanza della pianificazione iniziale\nAffrontare un progetto di analisi senza una pianificazione accurata è come condurre un esperimento psicologico senza ipotesi: si procede in modo confuso, con errori, ripensamenti e soluzioni improvvisate che rendono il lavoro inefficiente e difficile da mantenere nel tempo. Scrivere codice, contrariamente a quanto si crede, è solo una piccola parte del processo. La maggior parte dello sforzo riguarda la definizione degli obiettivi, la scelta degli strumenti appropriati, la costruzione di un percorso coerente e la prevenzione dei cosiddetti debiti tecnici — scorciatoie temporanee che in seguito diventano ostacoli complessi da risolvere.\nUn approccio non strutturato può portare, ad esempio, a importare i dati in modo disordinato, a confondere i file originali con versioni intermedie o a trascurare la documentazione delle trasformazioni. In queste condizioni, anche l’analisi più brillante diventa fragile e poco trasparente, fino a compromettere la possibilità di replicare i risultati.\n\n14.1.1 Costruire un flusso di lavoro efficace\nUn progetto ben strutturato nasce da una fase di progettazione concettuale: chiarire le domande di ricerca, identificare le variabili chiave e ipotizzare i passaggi fondamentali dell’analisi. Anche uno schema molto semplice, disegnato su carta, può aiutare a visualizzare l’intero percorso: dalla pulizia dei dati all’inferenza statistica, fino alla comunicazione dei risultati. Questa rappresentazione preliminare diventa una bussola per mantenere coerenza ed evitare deviazioni inutili.\nÈ altrettanto importante suddividere il lavoro in unità gestibili, stimando in modo realistico il tempo necessario per ciascuna e formulando obiettivi specifici. Dire “fare un’analisi preliminare” è troppo vago; meglio stabilire: “esplorare la distribuzione dei punteggi tra il gruppo sperimentale e quello di controllo e produrre un grafico documentato e interpretabile”. Questo livello di precisione facilita il monitoraggio dei progressi e la divisione dei compiti nei progetti collaborativi.\nLa scelta degli strumenti merita attenzione sin dall’inizio: dedicare tempo a selezionare pacchetti R affidabili evita soluzioni improvvisate poco sostenibili. Ad esempio, janitor semplifica la pulizia dei dati e report produce automaticamente output in stile APA. Tuttavia, lo strumento più importante resta la documentazione quotidiana: annotare le motivazioni delle scelte nel codice, descrivere chiaramente le trasformazioni e integrare testo, codice ed evidenze in un documento Quarto crea un archivio trasparente e facilmente condivisibile.\nInfine, la riproducibilità è la chiave per garantire solidità nel tempo. Una struttura di progetto ordinata, con cartelle distinte per dati, script, output e report, facilita l’orientamento e la collaborazione. Anche i dettagli apparentemente minori — nomi chiari per le variabili, coerenza nell’indentazione, commenti sintetici ma informativi — contribuiscono a rendere l’analisi più leggibile, robusta e sostenibile.\n\n14.1.2 Prevenire e gestire il debito tecnico\nCon l’avanzare del progetto, è naturale accumulare complessità e dover affrontare problemi di manutenzione del codice. Per evitarne l’escalation, conviene ritagliarsi momenti periodici di revisione: semplificare strutture ridondanti, dividere gli script troppo lunghi in moduli più chiari e trasformare blocchi ripetitivi in funzioni riutilizzabili. Un codice pulito riduce il rischio di errori, rende più semplice collaborare e garantisce la possibilità di estendere il progetto senza doverlo riscrivere da capo.\nUn’altra buona pratica è mantenere allineati codice e commenti. Ogni modifica sostanziale deve essere accompagnata da un aggiornamento delle annotazioni: pochi commenti chiari valgono più di intere sezioni obsolete che confondono chi legge.\nPrima di condividere il progetto è utile effettuare una “checklist di qualità”:\n\nverificare che i dati originali siano conservati separatamente e non alterati;\ndocumentare l’ambiente di lavoro (ad esempio, salvando l’output di sessionInfo());\ncontrollare che ogni script sia leggibile, ben commentato e riproducibile dall’inizio alla fine.\n\nIn definitiva, pianificare un flusso di lavoro in R non significa complicarsi la vita: significa creare le condizioni per analisi più chiare, durature e scientificamente robuste. L’organizzazione del progetto, la cura del codice e l’adozione di buone pratiche non sono accessori, ma il fondamento di un approccio che mira a comprendere e non solo a calcolare.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Il ciclo di vita di un progetto di analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/01_project_structure.html#capacità-di-gestione-dei-dati-in-r",
    "href": "chapters/eda/01_project_structure.html#capacità-di-gestione-dei-dati-in-r",
    "title": "14  Il ciclo di vita di un progetto di analisi dei dati",
    "section": "\n14.2 Capacità di gestione dei dati in R",
    "text": "14.2 Capacità di gestione dei dati in R\nUna volta compresa l’importanza di pianificare il lavoro, il passo successivo è dotarsi di strumenti affidabili per gestire, analizzare e documentare i dati. In questo contesto, R si rivela un alleato prezioso. È un ambiente di programmazione pensato per accompagnare l’intero ciclo di vita dei dati: dall’importazione e trasformazione, all’analisi statistica, fino alla produzione di report riproducibili. La sua versatilità lo rende centrale in qualsiasi flusso di lavoro orientato alla qualità e alla trasparenza — qualità indispensabili soprattutto in psicologia, dove i dataset sono spesso complessi e richiedono al tempo stesso flessibilità e rigore.\nR permette innanzitutto di importare ed esportare dati in molti formati. Pacchetti come readr e rio semplificano l’interazione con file CSV, database o persino fonti web. Una volta acquisiti, i dati possono essere puliti e trasformati in modo rapido ed elegante grazie a strumenti come dplyr, tidyr e stringr, che offrono un linguaggio espressivo per filtrare osservazioni, ristrutturare tabelle e modificare stringhe. La fase di esplorazione può poi contare su ggplot2, che consente di visualizzare pattern e riassumere indicatori descrittivi con grafici chiari e informativi.\nLa vera forza di R, tuttavia, emerge quando si tratta di documentare l’intero processo analitico. Con R Markdown e Quarto è possibile integrare codice, testo esplicativo, tabelle e grafici in un unico documento dinamico: un report che mostra non solo i risultati finali, ma anche come sono stati ottenuti, garantendo così riproducibilità e trasparenza. A questo si aggiunge la possibilità di usare Git, integrato in RStudio, per tracciare le modifiche, collaborare in modo strutturato e mantenere un archivio verificabile del lavoro.\nL’integrazione tra strategia progettuale e strumenti tecnici rende R una piattaforma ideale per chi vuole lavorare in modo affidabile, efficiente e scientificamente solido. Nei progetti con dati complessi, questa combinazione rappresenta una garanzia di qualità e sostenibilità analitica.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Il ciclo di vita di un progetto di analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/01_project_structure.html#configurare-lambiente-r",
    "href": "chapters/eda/01_project_structure.html#configurare-lambiente-r",
    "title": "14  Il ciclo di vita di un progetto di analisi dei dati",
    "section": "\n14.3 Configurare l’ambiente R",
    "text": "14.3 Configurare l’ambiente R\nPer sfruttare al meglio R è importante partire da un ambiente di lavoro ben configurato. RStudio offre un’interfaccia potente e flessibile, che consente di controllare in dettaglio il comportamento delle sessioni.\nUn primo passo utile è disattivare il salvataggio automatico dell’ambiente di lavoro. Dal menu Tools &gt; Global Options &gt; General si consiglia di:\n\ndeselezionare l’opzione di caricamento automatico del file .RData all’avvio,\nimpostare il salvataggio del workspace su Never.\n\nIn questo modo si evitano “residui invisibili” delle sessioni precedenti e si favorisce l’uso sistematico degli script come unico riferimento per l’analisi. L’obiettivo è rendere ogni passaggio esplicito e documentato, piuttosto che affidarsi a oggetti temporanei difficili da tracciare.\nUn ambiente ben preparato deve includere anche i pacchetti fondamentali. Tra questi, il tidyverse costituisce un vero ecosistema integrato per la manipolazione (dplyr), la visualizzazione (ggplot2), l’importazione (readr) e molte altre operazioni, sempre con una sintassi coerente e leggibile. Il pacchetto here, invece, semplifica la gestione dei percorsi relativi: gli script diventano portabili e funzionano senza modifiche anche su sistemi o directory differenti.\nL’installazione si effettua una sola volta con install.packages(), mentre il caricamento va ripetuto all’inizio di ogni script. Un setup iniziale tipico potrebbe essere:\n# install.packages(c(\"here\", \"tidyverse\"))  # solo la prima volta\nlibrary(here)\nlibrary(tidyverse)\nAdottare da subito queste pratiche di configurazione significa ridurre errori, aumentare la leggibilità del codice e costruire un ambiente solido, adatto ad analisi ripetibili e condivisibili. È qui che la programmazione incontra il metodo scientifico: nella combinazione di rigore tecnico e trasparenza metodologica.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Il ciclo di vita di un progetto di analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/01_project_structure.html#gestione-dei-progetti",
    "href": "chapters/eda/01_project_structure.html#gestione-dei-progetti",
    "title": "14  Il ciclo di vita di un progetto di analisi dei dati",
    "section": "\n14.4 Gestione dei progetti",
    "text": "14.4 Gestione dei progetti\nIn RStudio, i progetti permettono di organizzare tutto il lavoro analitico all’interno di una cartella dedicata, che contiene dati, script, output e documentazione. Ogni progetto è così un’unità autonoma, con percorsi relativi stabili e indipendenti dagli altri lavori. Questo riduce il rischio di confusione e rende più semplice riprodurre le analisi anche a distanza di tempo.\nCreare un nuovo progetto è immediato: basta selezionare File &gt; New Project e indicare la directory in cui raccogliere i file. Da quel momento, aprendo il file .Rproj, si ritrova automaticamente l’ambiente configurato e pronto all’uso.\nPiù che una questione di ordine, lavorare per progetti significa adottare una struttura replicabile: ogni analisi può essere riaperta, eseguita e condivisa senza dipendere da impostazioni esterne o da file sparsi. È anche un vantaggio per la collaborazione, perché rende chiaro a tutti dove trovare dati, script e risultati.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Il ciclo di vita di un progetto di analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/01_project_structure.html#il-ciclo-di-vita-di-un-progetto-di-data-science",
    "href": "chapters/eda/01_project_structure.html#il-ciclo-di-vita-di-un-progetto-di-data-science",
    "title": "14  Il ciclo di vita di un progetto di analisi dei dati",
    "section": "\n14.5 Il ciclo di vita di un progetto di Data Science",
    "text": "14.5 Il ciclo di vita di un progetto di Data Science\nUna volta predisposto un ambiente stabile e ben organizzato, il progetto di Data Science può essere affrontato in modo sistematico. Seguendo la proposta di Yu & Barter (2024), il ciclo di vita tipico comprende cinque fasi fondamentali: formulazione della domanda di ricerca, preparazione e analisi esplorativa dei dati, eventuale modellazione inferenziale o predittiva, valutazione dei risultati e comunicazione degli esiti.\nNon tutti i progetti arrivano alla modellazione, ma tutti traggono vantaggio da un percorso ordinato che attraversi in modo coerente queste fasi. Un approccio strutturato migliora la qualità delle analisi e riduce il rischio di errori o ambiguità interpretative.\n\n14.5.1 Formulazione del problema e raccolta dei dati\nIl primo passo è definire con chiarezza gli obiettivi. In ambito applicativo ciò può significare, ad esempio, valutare l’efficacia di un intervento con un report descrittivo; in ambito accademico, ancorare la domanda alla letteratura e a un quadro teorico esplicito. In entrambi i casi, la domanda deve essere precisa e affrontabile con i dati disponibili.\nLa raccolta dei dati può basarsi su fonti esistenti (dataset pubblici, archivi di laboratorio) o su nuove rilevazioni. Pianificare in anticipo l’analisi aiuta a evitare dati inutili o non pertinenti. È inoltre fondamentale documentare le modalità di raccolta ed esplicitare eventuali limiti.\n\n14.5.2 Pulizia, preprocessing e analisi esplorativa\nUna volta acquisiti, i dati vanno importati in R in formato analizzabile, di norma come data frame. Pacchetti come rio rendono semplice l’importazione, mentre here facilita la gestione dei percorsi. I dati grezzi vanno sempre conservati separati dai dati trasformati.\nLa pulizia comprende la correzione di errori, la gestione dei valori mancanti, l’eliminazione di duplicati e l’uniformazione delle codifiche. Le trasformazioni non vanno mai fatte “a mano” ma registrate in script, per garantire tracciabilità.\nIl preprocessing adatta i dati all’analisi: ad esempio standardizzare variabili, creare indici compositi o ricodificare categorie. Ogni passo deve essere documentato, perché ha effetti diretti sull’interpretazione.\nL’analisi esplorativa (EDA) serve a conoscere i dati prima di modellarli: statistiche descrittive, distribuzioni, visualizzazioni con ggplot2. Questa fase permette di individuare pattern, anomalie e relazioni preliminari che guideranno le scelte successive.\n\n14.5.3 Analisi predittiva e inferenziale\nQuando previsto, il cuore del progetto è la modellazione. In psicologia si usano comunemente regressioni, test parametrici e non parametrici, modelli ad effetti misti o algoritmi di classificazione. L’obiettivo può essere inferenziale (trarre conclusioni su una popolazione) o predittivo (anticipare valori futuri). In ogni caso le scelte modellistiche devono essere motivate, e le ipotesi verificate.\n\n14.5.4 Valutazione dei risultati\nInterpretare i risultati richiede attenzione su due livelli:\n\n\nstatistico, attraverso indici di bontà del modello, intervalli di confidenza o posteriori credibili,\n\nconcettuale, verificando la coerenza con la teoria di riferimento o con il contesto applicativo.\n\nRicollegarsi sempre agli obiettivi iniziali aiuta a distinguere ciò che il modello davvero mostra da ciò che resta incerto.\n\n14.5.5 Comunicazione dei risultati\nLa fase finale è la comunicazione, che può assumere forme diverse: articolo scientifico, report tecnico, presentazione a un pubblico non specialistico. In ogni caso serve un linguaggio chiaro e motivato, supportato da visualizzazioni curate. Un buon risultato analitico non è completo finché non viene presentato in modo comprensibile e utile a chi lo riceve.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Il ciclo di vita di un progetto di analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/01_project_structure.html#organizzazione-del-progetto",
    "href": "chapters/eda/01_project_structure.html#organizzazione-del-progetto",
    "title": "14  Il ciclo di vita di un progetto di analisi dei dati",
    "section": "\n14.6 Organizzazione del progetto",
    "text": "14.6 Organizzazione del progetto\nPer garantire riproducibilità e gestione a lungo termine, è utile adottare una struttura chiara e strumenti che automatizzino i passaggi chiave. Un esempio concreto è documentato nel repository ccaudek/make-tutorial, che mostra come integrare R, Quarto e GNU Make in un flusso unico e completamente ricostruibile.\n\n14.6.1 Perché Make\nMake è uno strumento nato per compilare software, ma perfetto anche per la ricerca: invece di un “master script” che lancia tutto in sequenza, Make lavora con dipendenze dichiarative. Ogni output (un file pulito, un grafico, un report) viene associato agli input necessari e al comando che lo genera. Così, se modifichi un solo script, Make ricostruisce solo ciò che dipende da quel file, senza ricalcolare inutilmente il resto.\nTre principi fondamentali:\n\n\nModularità: ogni passaggio (pulizia, scoring, analisi, report) è uno script autonomo.\n\nDichiaratività: il Makefile descrive cosa dipende da cosa, non come eseguirlo manualmente.\n\nEfficienza: make all ricostruisce l’intero progetto, make clean lo riporta allo stato iniziale.\n\n14.6.2 Struttura del progetto\nmake-tutorial/\n├─ data/\n│  ├─ raw/         # input \"sorgente\" o simulati\n│  └─ processed/   # output intermedi (clean, scored)\n├─ scripts/        # step atomici della pipeline (Rscript)\n├─ stan/           # modelli Stan (se usati)\n├─ fits/           # oggetti dei fit (rds/csv)\n├─ figs/           # figure prodotte dagli script\n├─ report/         # report .qmd e .html\n├─ Makefile        # regole e dipendenze\n└─ README.md       # istruzioni d'uso\n\n14.6.3 Il cuore del progetto: il Makefile essenziale\nQuando un progetto di analisi dati cresce, tenere traccia di tutti i file, script e delle loro dipendenze può diventare complicato. GNU Make è uno strumento che automatizza il processo di costruzione (o “build”) di un progetto. La sua logica è semplice ma potente: definisci cosa vuoi produrre (i file-obiettivo), da cosa dipende (i prerequisiti) e come produrlo (i comandi). Make si occuperà di eseguire i comandi nell’ordine corretto, solo quando necessario. Un file che contiene queste istruzioni si chiama Makefile.\n\n14.6.3.1 Anatomia di una regola\nLa struttura fondamentale di un Makefile è la regola. Ogni regola è composta da tre elementi:\ntarget: prerequisito1 prerequisito2 ...\n    comando\n\n\nTarget: il file che vuoi costruire (ad esempio, un dataset pulito, un grafico, un report HTML).\n\nPrerequisiti: i file di cui il target ha bisogno per essere costruito (ad esempio, gli script R e i dati grezzi). Sono le dipendenze del target.\n\nComando: la ricetta, ossia la riga di comando che, eseguita, trasforma i prerequisiti nel target. È fondamentale che questa riga inizi con un carattere di tabulazione (TAB), non con spazi.\n\nCome funziona Make? Quando gli chiedi di costruire un target (ad esempio, make report.html), Make fa questo:\n\n\nControlla le dipendenze: guarda tutti i prerequisiti del target.\n\nVerifica la necessità: controlla se il target esiste già e se è più recente di tutti i suoi prerequisiti.\n\nSe il target non esiste o è più vecchio di anche solo uno dei suoi prerequisiti, Make esegue il comando per aggiornarlo.\nSe il target esiste ed è più recente di tutti i prerequisiti, Make sa che non c’è nulla da fare e salta la ricetta, risparmiando tempo.\n\n\n\nSi propaga: applica questa stessa logica ricorsivamente a tutti i prerequisiti che sono a loro volta target di altre regole.\n\nQuesto meccanismo intelligente assicura che tutto il tuo progetto venga ricostruito in modo efficiente e sempre aggiornato.\n\n14.6.3.2 Spiegazione del nostro Makefile\nAnalizziamo ora il Makefile utilizzato in ccaudek/make-tutorial pezzo per pezzo.\n1. Impostare l’obiettivo predefinito\n# Esegui la pipeline completa se non passi argomenti a 'make'\n.DEFAULT_GOAL := all\nQuesta riga dice a Make: “Se l’utente digita semplicemente make senza specificare un target, esegui di default il target chiamato all”. È un’utile convenzione per avviare l’intera pipeline con un comando minimo.\n2. Generazione dei dati (DAG: da raw a processed)\ndata/raw/rosenberg_raw.csv: scripts/00_simulate.R\n    Rscript $&lt;\n\n\nTarget: data/raw/rosenberg_raw.csv (i dati simulati grezzi).\n\nPrerequisito: scripts/00_simulate.R (lo script che genera i dati).\n\nComando: Rscript $&lt;\n\n\n$&lt; è una variabile automatica di Make. Viene sostituita con il nome del primo prerequisito della regola. In questo caso, $&lt; diventa scripts/00_simulate.R.\nQuindi, il comando eseguito è Rscript scripts/00_simulate.R.\n\n\n\ndata/processed/clean.csv: scripts/01_clean.R data/raw/rosenberg_raw.csv\n    Rscript $&lt;\n\n\nTarget: data/processed/clean.csv (i dati puliti).\n\nPrerequisiti: Lo script di pulizia (01_clean.R) e i dati grezzi. Se i dati grezzi cambiano, anche i dati puliti devono essere rigenerati.\n\nComando: Rscript $&lt; (dove $&lt; è scripts/01_clean.R).\n\n3. Creazione delle figure (EDA)\nfigs/dist_by_gender.png figs/scatter_rosenberg_vs_outcome.png: scripts/03_eda.R data/processed/scored.csv\n    Rscript scripts/03_eda.R\nQuesta regola è interessante perché ha due target. Significa che eseguendo un unico comando (Rscript scripts/03_eda.R) vengono generati entrambi i file figura. Se uno dei due file PNG o lo script EDA viene modificato, Make rigenererà entrambe le figure.\n4. Esecuzione delle analisi\nfits/freq_done.flag: scripts/10_freq_tests.R data/processed/scored.csv\n    Rscript scripts/10_freq_tests.R && touch $@\n\n\nTarget: fits/freq_done.flag. Questo è un file flag (o sentinella). A volte un’analisi non produce un file di output vero e proprio, o lo produce con un nome imprevedibile. Il file flag è un file vuoto il cui unico scopo è registrare quando l’analisi è stata eseguita con successo per l’ultima volta.\n\nComando: Rscript scripts/10_freq_tests.R && touch $@\n\n\n&& significa “esegui il comando successivo solo se il primo è terminato con successo”.\n\n$@ è un’altra variabile automatica. Viene sostituita con il nome del target della regola. In questo caso, $@ diventa fits/freq_done.flag.\nQuindi, il comando esegue prima lo script R e, se ha successo, crea/tocca il file flag.\n\n\n\nfits/brms_fit.rds: scripts/11_bayes_brms.R data/processed/scored.csv\n    Rscript $&lt;\n\n\nTarget: fits/brms_fit.rds (un file binario R che contiene il modello fitted).\nIl comando usa $&lt; per riferirsi al primo prerequisito, lo script.\n\n5. Generazione del report finale\nreport/report.html: report/report.qmd figs/dist_by_gender.png figs/scatter_rosenberg_vs_outcome.png fits/freq_done.flag fits/brms_fit.rds fits/cmdstan_fit.rds\n    quarto render $&lt;\n\n\nTarget: Il report HTML finale.\n\nPrerequisiti: Tutto ciò di cui il report ha bisogno: il file Quarto source (.qmd), tutte le figure e i risultati di tutte le analisi (compreso il file flag).\n\nComando: quarto render $&lt; (dove $&lt; è report/report.qmd). Questo comando compila il documento Quarto in HTML, incorporando figure e risultati.\n\n6. Target “fittizi” (phony) e pulizia\n.PHONY: all clean\nall: report/report.html\n\nclean:\n    rm -f data/raw/rosenberg_raw.csv \\\n          data/processed/*.csv \\\n          figs/*.png \\\n          fits/*.rds fits/*.flag \\\n          report/report.html\n\n\n.PHONY: Questa è una direttiva speciale che dice a Make che i target all e clean non sono nomi di file. Sono comandi, azioni. Senza .PHONY, se per caso esistesse un file chiamato all o clean nella cartella, Make penserebbe che quel target è già aggiornato e non eseguirebbe il comando, il che non è ciò che vogliamo.\n\nall: report/report.html: Definisce il target all come un semplice collegamento (alias) al nostro report finale. È il target che costruisce l’intero progetto.\n\nclean: Questo target ha un comando molto importante: elimina tutti i file generati. È utile per ricominciare da zero con una pulizia profonda. Nota la continuazione di riga con il carattere \\.\n\n14.6.3.3 Riepilogo delle Variabili Automatiche\n\n\n$&lt;: Il nome del primo prerequisito.\n\n$@: Il nome del target.\n\n14.6.3.4 Come usare questo Makefile\nApri un terminale nella cartella del progetto contenente il Makefile e digita:\n\n\nmake o make all: costruisce l’intero progetto (dati, figure, analisi, report).\n\nmake data/processed/clean.csv: costruisce solo il dataset pulito e tutto ciò da cui dipende (dati grezzi).\n\nmake figs/dist_by_gender.png: genera solo quella figura specifica e le sue dipendenze.\n\nmake clean: cancella tutti i file prodotti, lasciando solo il codice sorgente e il Makefile.\n\nmake -n target: esegue una simulazione (“dry run”). Make mostrerà i comandi che eseguirebbe senza eseguirli realmente. Ottimo per debug.\n\nIl Makefile contenuto nel repository ccaudek/make-tutorial trasforma il progetto di analisi dei dati da una collezione di script in una pipeline di dati riproducibile e automatizzata.\n\n14.6.4 DAG della pipeline\n\nflowchart LR\n    A[\"raw.csv\"] --&gt; B[\"clean.csv\"]\n    B --&gt; C[\"scored.csv\"]\n    C --&gt; D1[\"EDA figs\"]\n    C --&gt; D2[\"freq tests\"]\n    C --&gt; D3[\"bayes (brms)\"]\n    C --&gt; D4[\"bayes (cmdstanr)\"]\n    D1 --&gt; RPT[\"report.html\"]\n    D2 --&gt; RPT\n    D3 --&gt; RPT\n    D4 --&gt; RPT\n\n\n\n\nflowchart LR\n    A[\"raw.csv\"] --&gt; B[\"clean.csv\"]\n    B --&gt; C[\"scored.csv\"]\n    C --&gt; D1[\"EDA figs\"]\n    C --&gt; D2[\"freq tests\"]\n    C --&gt; D3[\"bayes (brms)\"]\n    C --&gt; D4[\"bayes (cmdstanr)\"]\n    D1 --&gt; RPT[\"report.html\"]\n    D2 --&gt; RPT\n    D3 --&gt; RPT\n    D4 --&gt; RPT\n\n\n\n\n\n\n\n14.6.5 Buone pratiche operative\n\n\nShebang facoltativo negli script (#!/usr/bin/env Rscript) + chmod +x per lanciarli anche come eseguibili. Nei Makefile va benissimo Rscript script.R.\n\nPercorsi stabili: nel report .qmd, usa percorsi relativi o abilita embed-resources: true per incorporare le immagini nell’HTML.\n\nCmdStanR e file CSV: gli oggetti CmdStanMCMC salvati con saveRDS() puntano ai CSV generati dal sampler. Salvali in una cartella persistente (es. fits/cmdstan_csv/ con output_dir=) oppure salva direttamente un summary (fit$summary()) in RDS per evitare dipendenza dai CSV temporanei.\n\nIdempotenza: ogni regola dovrebbe essere deterministica dato il suo input (se simuli dati, fissa il seed negli script).\n\nPulizia selettiva: clean rimuove solo artefatti ricostruibili. Se vuoi mantenere determinati risultati (es. fit costosi), non includerli nel clean.\n\n14.6.6 Vantaggi pratici\n\n\nAutomatizzazione del flusso di lavoro: elimina la necessità di orchestrazione manuale degli script, sostituendo il classico “master script” con un grafo di dipendenze esplicito ed eseguibile.\n\nEsecuzione incrementale: modifiche a singoli componenti innescano automaticamente solo il riprocessamento degli output dipendenti, ottimizzando significativamente i tempi di esecuzione.\n\nDocumentazione attiva: il Makefile stesso costituisce una documentazione sempre aggiornata e verificabile della pipeline analitica, garantendo trasparenza e riproducibilità.\n\nPortabilità cross-piattaforma: il comando make all produce risultati consistenti attraverso macOS, Linux e Windows (quest’ultimo tramite ambienti come WSL o strumenti GNU), astraendo dalle specificità del sistema operativo.\n\nSe vuoi estendere l’esempio (più analisi, cross-validation, modelli addizionali), basta aggiungere nuovi target con prerequisiti appropriati e collegarli al report. Make farà il resto.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Il ciclo di vita di un progetto di analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/01_project_structure.html#riflessioni-conclusive",
    "href": "chapters/eda/01_project_structure.html#riflessioni-conclusive",
    "title": "14  Il ciclo di vita di un progetto di analisi dei dati",
    "section": "Riflessioni conclusive",
    "text": "Riflessioni conclusive\nLa riproducibilità computazionale rappresenta un fondamento metodologico essenziale, che trascende la mera esecuzione tecnica per diventare garanzia di integrità scientifica. In psicologia, dove la complessità dei dati e la natura iterativa dell’analisi richiedono tracciabilità completa, un flusso di lavoro codificato e ben documentato non è opzionale ma necessario.\nI vantaggi si manifestano su tre livelli:\n\n\nPersonale: la capacità di riattivare, correggere o estendere un’analisi a distanza di tempo senza perdita di informazione\n\nCollettivo: la possibilità per altri ricercatori di verificare, adattare e costruire sulle procedure condivise\n\nEpistemologico: la costruzione di una conoscenza cumulativa e criticabile, basata su processi analitici trasparenti\n\nStrumenti come il versioning (git), la strutturazione coerente dei progetti e l’automazione dei flussi (make) non sono mere questioni organizzative, ma abilitatori concreti di rigore metodologico. Un’analisi riproducibile non è solo “ordinata” — è intrinsecamente più robusta, in quanto costringe a esplicitare assunzioni, scelte e trasformazioni.\nInvestire in pratiche riproducibili significa quindi investire nella qualità del proprio lavoro: non un vincolo “burocratico”, ma una disciplina che affina il pensiero critico e trasforma l’analisi dati da attività esecutiva a processo scientifico pienamente consapevole.\n\n\n\n\n\n\nUn problema cruciale della psicologia contemporanea è la crisi di replicabilità, che evidenzia come molti risultati di ricerca non siano replicabili (Collaboration, 2015). La riproducibilità computazionale, pur avendo un obiettivo più ristretto, si concentra sulla possibilità di ottenere gli stessi risultati applicando lo stesso codice agli stessi dati. Questo approccio, sebbene non risolva completamente la crisi, rappresenta un passo fondamentale verso una scienza più trasparente e rigorosa.\n\n\n\n\n\n\n\n\n\nRisposte alle Domande Iniziali\n\n\n\n\n\nOra che hai completato il capitolo, confrontiamo le risposte alle domande iniziali con quanto appreso:\n\n\nPerché è importante organizzare e documentare accuratamente i dati in un progetto di ricerca?\n\nUna gestione strutturata dei dati riduce il rischio di errori, facilita l’analisi e migliora la riproducibilità, rendendo il lavoro scientifico più affidabile e trasparente.\n\n\n\nQuali problemi possono emergere se si inizia a scrivere codice senza una pianificazione adeguata?\n\nSenza una pianificazione strategica si rischia di incorrere nel “debito tecnico”, accumulando codice disordinato che richiede correzioni costose in termini di tempo e risorse. Inoltre, si potrebbero fare scelte subottimali che compromettono la scalabilità e manutenibilità del progetto.\n\n\n\nQuali vantaggi offre R per la gestione dei dati rispetto ad altri strumenti?\n\nR fornisce strumenti avanzati per importazione, pulizia, analisi e visualizzazione dei dati, oltre a supportare la documentazione dinamica e il controllo delle versioni con Git. La sua ampia comunità e la disponibilità di pacchetti specializzati lo rendono particolarmente adatto per l’analisi statistica e la ricerca.\n\n\n\nQuali strategie potrebbero migliorare la riproducibilità del tuo lavoro?\n\nUtilizzare strumenti come Quarto per documentare il codice e le analisi, adottare percorsi relativi con il pacchetto here, strutturare i dati in cartelle ben organizzate e integrare il controllo delle versioni con Git.\n\n\n\nCome struttureresti un progetto di analisi dati per mantenerlo chiaro e facilmente replicabile?\n\n\nAdottando una struttura chiara, ad esempio:\nnome_progetto/\n├── nome_progetto.Rproj\n├── data/\n│   ├── raw/\n│   │   └── my_data.csv\n│   ├── processed/\n├── dslc_documentation/\n│   ├── 01_data_cleaning.qmd\n│   ├── 02_analysis.qmd\n│   └── functions/\n└── README.md\n\nQuesta organizzazione separa i dati grezzi da quelli elaborati, include documentazione chiara e facilita la riproducibilità.\n\n\n\n\n\nConclusione: Riflettere in anticipo sui problemi e sulle strategie di gestione dei dati aiuta a costruire workflow più efficienti e affidabili. Se le tue risposte iniziali differivano da queste, quali nuovi spunti hai appreso da questo capitolo?",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Il ciclo di vita di un progetto di analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/01_project_structure.html#esercizi",
    "href": "chapters/eda/01_project_structure.html#esercizi",
    "title": "14  Il ciclo di vita di un progetto di analisi dei dati",
    "section": "Esercizi",
    "text": "Esercizi\n\n\n\n\n\n\nEsercizio\n\n\n\n\n\nL’obiettivo di questo esercizio è comprendere il ciclo di vita di un progetto di analisi dei dati, l’organizzazione del progetto e la gestione della riproducibilità.\n\n\nGestione del progetto di analisi\n\nQuali sono le fasi principali di un progetto di analisi dei dati secondo Yu (2024)?\nSpiega il ruolo della fase di formulazione del problema e raccolta dei dati.\n\n\n\nOrganizzazione del workspace in R\n\nQuali impostazioni devono essere modificate in RStudio per favorire la riproducibilità?\nPerché è importante usare percorsi relativi nei progetti in RStudio?\nDescrivi il ruolo del pacchetto here nella gestione dei percorsi dei file.\n\n\n\nStruttura dei progetti in R\n\nQuali sono i vantaggi dell’utilizzo dei progetti in RStudio?\nQuali sono le cartelle principali in una struttura organizzata di un progetto?\nPerché è utile separare i dati grezzi dai dati processati?\n\n\n\nImportazione ed esportazione dei dati\n\nQuali pacchetti di R possono essere utilizzati per importare ed esportare dati?\n\nScrivi un esempio di codice per importare un file CSV usando rio e il pacchetto here.\n\nCome puoi esportare un dataset modificato in una cartella dedicata ai dati processati?\n\n\n\nPulizia e preprocessing dei dati\n\nQual è la differenza tra pulizia e preprocessing dei dati?\n\nQuali strumenti di dplyr sono comunemente usati per pulire e trasformare i dati?\n\n\n\nAnalisi esplorativa dei dati (EDA)\n\nQuali sono alcuni strumenti utilizzati in R per effettuare un’analisi esplorativa dei dati?\n\nScrivi un breve esempio di codice in R per calcolare statistiche descrittive di base su un dataset.\n\n\n\nRiproducibilità e comunicazione dei risultati\n\nPerché la riproducibilità è un elemento chiave nella scienza dei dati?\n\nQuali strumenti offre Quarto per la documentazione e la condivisione dei risultati di un’analisi?\n\n\n\n\n\n\n\n\n\n\n\n\nSoluzione\n\n\n\n\n\n1. Gestione del progetto di analisi\n\n\nFasi principali del progetto di analisi dei dati (Yu, 2024):\n\nFormulazione del problema e raccolta dei dati\n\nPulizia, preprocessing e analisi esplorativa\n\nAnalisi predittiva e/o inferenziale (se applicabile)\n\nValutazione dei risultati\n\nComunicazione dei risultati\n\n\n\nRuolo della fase di formulazione del problema:\nAiuta a definire gli obiettivi dell’analisi e a selezionare le fonti di dati adeguate. Una domanda di ricerca ben definita garantisce che i dati siano pertinenti e che le analisi siano mirate.\n\n2. Organizzazione del workspace in R\n\n\nImpostazioni da modificare in RStudio:\n\nDisabilitare Restore .RData into workspace at startup\n\nImpostare Save workspace to .RData on exit su “Never”\n\n\nImportanza dei percorsi relativi:\nPermettono di rendere il progetto portabile e riproducibile, evitando problemi di percorsi assoluti specifici per un computer.\nRuolo del pacchetto here\nAiuta a gestire i percorsi relativi all’interno del progetto senza dover specificare percorsi assoluti.\n\n3. Struttura dei progetti in R\n\nVantaggi dell’uso dei progetti in RStudio:\nMantengono ambienti separati, organizzano i file e facilitano la riproducibilità.\n\nCartelle principali in una struttura organizzata:\n\n\ndata/raw/ → Dati grezzi\n\n\ndata/processed/ → Dati elaborati\n\n\ndslc_documentation/ → Documentazione e script\n\n\nfunctions/ → Funzioni personalizzate\n\n\nSeparare dati grezzi da dati processati:\nEvita di modificare accidentalmente i dati originali, garantendo riproducibilità.\n\n4. Importazione ed esportazione dei dati\n\n\nPacchetti per importazione/esportazione:\n\n\nrio: unifica funzioni di import/export\n\n\nreadr: specifico per CSV e altri formati di testo\n\n\nhere: gestisce percorsi relativi\n\n\n\nEsempio di codice per importare dati CSV:\nlibrary(here)\nlibrary(rio)\ndf &lt;- rio::import(here(\"data\", \"raw\", \"my_data.csv\"))\n\n\nEsportare dati modificati:\nrio::export(df, here(\"data\", \"processed\", \"my_data_processed.csv\"))\n\n\n5. Pulizia e preprocessing dei dati\n\n\nDifferenza tra pulizia e preprocessing:\n\nPulizia: rimozione di errori, gestione dei dati mancanti, formattazione\n\nPreprocessing: trasformazione dei dati per adattarli a modelli specifici\n\n\n\nStrumenti di dplyr per pulizia e trasformazione:\n\n\nmutate(), filter(), select(), rename(), relocate()\n\n\n\n\n6. Analisi esplorativa dei dati (EDA)\n\n\nStrumenti comuni:\n\n\nsummary(), str(), glimpse(), ggplot2 per visualizzazione\n\n\n\nEsempio di codice per statistiche descrittive:\nsummary(df)\n\n\n7. Riproducibilità e comunicazione dei risultati\n\n\nImportanza della riproducibilità:\n\nFacilita la verifica e il miglioramento degli studi\n\nPreviene errori accidentali\n\nConsente a terzi di replicare e costruire su ricerche precedenti\n\n\n\nStrumenti di Quarto:\n\nPermette di combinare testo, codice e output in documenti riproducibili\n\nSupporta citazioni automatiche e gestione delle bibliografie\n\n\n\n\n\n\n\n\n\n\n\n\n\nInformazioni sull’ambiente di sviluppo\n\n\n\n\n\n\nsessionInfo()\n#&gt; R version 4.5.1 (2025-06-13)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.6.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] pillar_1.11.0         tinytable_0.13.0      patchwork_1.3.2      \n#&gt;  [4] ggdist_3.3.3          tidybayes_3.0.7       bayesplot_1.14.0     \n#&gt;  [7] ggplot2_3.5.2         reliabilitydiag_0.2.1 priorsense_1.1.1     \n#&gt; [10] posterior_1.6.1       loo_2.8.0             rstan_2.32.7         \n#&gt; [13] StanHeaders_2.32.10   brms_2.22.0           Rcpp_1.1.0           \n#&gt; [16] sessioninfo_1.2.3     conflicted_1.2.0      janitor_2.2.1        \n#&gt; [19] matrixStats_1.5.0     modelr_0.1.11         tibble_3.3.0         \n#&gt; [22] dplyr_1.1.4           tidyr_1.3.1           rio_1.2.3            \n#&gt; [25] here_1.0.1           \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;  [1] svUnit_1.0.8          tidyselect_1.2.1      farver_2.1.2         \n#&gt;  [4] fastmap_1.2.0         TH.data_1.1-4         tensorA_0.36.2.1     \n#&gt;  [7] digest_0.6.37         timechange_0.3.0      estimability_1.5.1   \n#&gt; [10] lifecycle_1.0.4       survival_3.8-3        magrittr_2.0.3       \n#&gt; [13] compiler_4.5.1        rlang_1.1.6           tools_4.5.1          \n#&gt; [16] knitr_1.50            bridgesampling_1.1-2  htmlwidgets_1.6.4    \n#&gt; [19] curl_7.0.0            pkgbuild_1.4.8        RColorBrewer_1.1-3   \n#&gt; [22] abind_1.4-8           multcomp_1.4-28       withr_3.0.2          \n#&gt; [25] purrr_1.1.0           grid_4.5.1            stats4_4.5.1         \n#&gt; [28] colorspace_2.1-1      xtable_1.8-4          inline_0.3.21        \n#&gt; [31] emmeans_1.11.2-8      scales_1.4.0          MASS_7.3-65          \n#&gt; [34] cli_3.6.5             mvtnorm_1.3-3         rmarkdown_2.29       \n#&gt; [37] ragg_1.5.0            generics_0.1.4        RcppParallel_5.1.11-1\n#&gt; [40] cachem_1.1.0          stringr_1.5.1         splines_4.5.1        \n#&gt; [43] parallel_4.5.1        vctrs_0.6.5           V8_7.0.0             \n#&gt; [46] Matrix_1.7-4          sandwich_3.1-1        jsonlite_2.0.0       \n#&gt; [49] arrayhelpers_1.1-0    systemfonts_1.2.3     glue_1.8.0           \n#&gt; [52] codetools_0.2-20      distributional_0.5.0  lubridate_1.9.4      \n#&gt; [55] stringi_1.8.7         gtable_0.3.6          QuickJSR_1.8.0       \n#&gt; [58] htmltools_0.5.8.1     Brobdingnag_1.2-9     R6_2.6.1             \n#&gt; [61] textshaping_1.0.3     rprojroot_2.1.1       evaluate_1.0.5       \n#&gt; [64] lattice_0.22-7        backports_1.5.0       memoise_2.0.1        \n#&gt; [67] broom_1.0.9           snakecase_0.11.1      rstantools_2.5.0     \n#&gt; [70] coda_0.19-4.1         gridExtra_2.3         nlme_3.1-168         \n#&gt; [73] checkmate_2.3.3       xfun_0.53             zoo_1.8-14           \n#&gt; [76] pkgconfig_2.0.3",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Il ciclo di vita di un progetto di analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/01_project_structure.html#bibliografia",
    "href": "chapters/eda/01_project_structure.html#bibliografia",
    "title": "14  Il ciclo di vita di un progetto di analisi dei dati",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nCollaboration, O. S. (2015). Estimating the reproducibility of psychological science. Science, 349(6251), aac4716.\n\n\nYu, B., & Barter, R. L. (2024). Veridical data science: The practice of responsible data analysis and decision making. MIT Press.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Il ciclo di vita di un progetto di analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html",
    "href": "chapters/eda/02_data_cleaning.html",
    "title": "15  Flusso di lavoro per la pulizia dei dati",
    "section": "",
    "text": "Introduzione\nNonostante la fase più interessante di un progetto di analisi dei dati sia quella in cui si riesce a rispondere alla domanda che ha dato avvio all’indagine, gran parte del tempo di un analista è in realtà dedicata a una fase preliminare: la pulizia e il preprocessing dei dati, operazioni che vengono svolte ancor prima dell’analisi esplorativa.\nIn questo capitolo, esamineremo un caso concreto di data cleaning e preprocessing, seguendo il tutorial di Crystal Lewis. Il problema viene presentato come segue:\nCrystal Lewis elenca i seguenti passaggi da seguire nel processo di data cleaning:\nSebbene l’ordine di questi passaggi sia flessibile e possa essere adattato alle esigenze specifiche, c’è un passaggio che non dovrebbe mai essere saltato: il primo, ovvero la revisione dei dati. Senza una revisione preliminare, l’analista rischia di sprecare ore a pulire i dati per poi scoprire che mancano dei partecipanti, che i dati non sono organizzati come previsto o, peggio ancora, che si sta lavorando con i dati sbagliati.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#introduzione",
    "href": "chapters/eda/02_data_cleaning.html#introduzione",
    "title": "15  Flusso di lavoro per la pulizia dei dati",
    "section": "",
    "text": "I am managing data for a longitudinal randomized controlled trial (RCT) study. For this RCT, schools are randomized to either a treatment or control group. Students who are in a treatment school receive a program to boost their math self-efficacy. Data is collected on all students in two waves (wave 1 is in the fall of a school year, and wave 2 is collected in the spring). At this point in time, we have collected wave 1 of our student survey on a paper form and we set up a data entry database for staff to enter the information into. Data has been double-entered, checked for entry errors, and has been exported in a csv format (“w1_mathproj_stu_svy_raw.csv”) to a folder (called “data”) where it is waiting to be cleaned.\n\n\n\nRevisione dei dati.\nRegolazione del numero di casi.\nDe-identificazione dei dati.\nEliminazione delle colonne irrilevanti.\nDivisione delle colonne, se necessario.\nRidenominazione delle variabili.\nTrasformazione/normalizzazione delle variabili.\nStandardizzazione delle variabili.\nAggiornamento dei tipi di variabili, se necessario.\nRicodifica delle variabili.\nCreazione di eventuali variabili necessarie.\nGestione dei valori mancanti, se necessario.\nAggiunta di metadati, se necessario.\nValidazione dei dati.\nFusione e/o unione dei dati, se necessario.\nTrasformazione dei dati, se necessario.\nSalvataggio dei dati puliti.\n\n\nPanoramica del capitolo\n\nVerificare, pulire e trasformare i dati per l’analisi.\nDocumentare il dataset con un dizionario e note esplicative.\nAssicurare validità, unicità e organizzazione dei dati.\nApplicare regole coerenti per denominazione e codifica.\n\n\n\n\n\n\n\nPrerequisiti\n\n\n\n\n\n\nLeggere Cleaning sample data in standardized way di Crystal Lewis.\nLeggere Getting Started Creating Data Dictionaries: How to Create a Shareable Data Set di Buchanan et al. (2021).\nConsultare il capitolo Documentation di Data Management in Large-Scale Education Research.\nConsultare How to Make a Data Dictionary.\nConsultare data dictionary template.\n\n\n\n\n\n\n\n\n\n\nPreparazione del Notebook\n\n\n\n\n\n\nhere::here(\"code\", \"_common.R\") |&gt; \n  source()\n\n# Load packages\nif (!requireNamespace(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(mice, labelled, haven, pointblank, mice, purrr, knitr)",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#tutorial",
    "href": "chapters/eda/02_data_cleaning.html#tutorial",
    "title": "15  Flusso di lavoro per la pulizia dei dati",
    "section": "\n15.1 Tutorial",
    "text": "15.1 Tutorial\nQuesto tutorial segue i passaggi descritti da Crystal Lewis per illustrare le buone pratiche nella gestione e pulizia dei dati.\n\n15.1.1 Organizzazione dei dati\nUn principio fondamentale nella gestione dei dati è preservare l’integrità dei dati grezzi. I dati originali non devono mai essere modificati direttamente. È quindi consigliabile strutturare i dati in una directory denominata data, suddivisa in due sottocartelle:\n\n\nraw: contiene i dati originali, mantenuti inalterati.\n\n\nprocessed: destinata ai dati ripuliti e preprocessati.\n\nAd esempio, per avviare il processo di pulizia, importiamo i dati da un file denominato w1_mathproj_stu_svy_raw.csv. Tutte le operazioni dovranno essere effettuate utilizzando percorsi relativi alla home directory del progetto, che definiremo come primo passo.\n\n15.1.2 Passaggi del tutorial\n\n15.1.2.1 Importare e esaminare i dati\nImportiamo i dati utilizzando la funzione import() della libreria rio e visualizziamo i primi valori di ciascuna colonna per verificarne la corretta importazione:\n\n# Importa i dati\nsvy &lt;- rio::import(here::here(\"data\", \"w1_mathproj_stu_svy_raw.csv\"))\n\n# Esamina la struttura del dataset\nglimpse(svy)\n#&gt; Rows: 6\n#&gt; Columns: 7\n#&gt; $ stu_id      &lt;int&gt; 1347, 1368, 1377, 1387, 1347, 1399\n#&gt; $ svy_date    &lt;IDate&gt; 2023-02-13, 2023-02-13, 2023-02-13, 2023-02-13, 2023-02-14…\n#&gt; $ grade_level &lt;int&gt; 9, 10, 9, 11, 9, 12\n#&gt; $ math1       &lt;int&gt; 2, 3, 4, 3, 2, 4\n#&gt; $ math2       &lt;chr&gt; \"1\", \"2\", \"\\n4\", \"3\", \"2\", \"1\"\n#&gt; $ math3       &lt;int&gt; 3, 2, 4, NA, 4, 3\n#&gt; $ math4       &lt;int&gt; 3, 2, 4, NA, 2, 1\n\nPer controllare visivamente i dati, possiamo esaminare le prime e le ultime righe del data frame:\n\n# Visualizza le prime righe\nsvy |&gt; \n  head()\n#&gt;   stu_id   svy_date grade_level math1 math2 math3 math4\n#&gt; 1   1347 2023-02-13           9     2     1     3     3\n#&gt; 2   1368 2023-02-13          10     3     2     2     2\n#&gt; 3   1377 2023-02-13           9     4   \\n4     4     4\n#&gt; 4   1387 2023-02-13          11     3     3    NA    NA\n#&gt; 5   1347 2023-02-14           9     2     2     4     2\n#&gt; 6   1399 2023-02-14          12     4     1     3     1\n\n# Visualizza le ultime righe\nsvy |&gt; \n  tail()\n#&gt;   stu_id   svy_date grade_level math1 math2 math3 math4\n#&gt; 1   1347 2023-02-13           9     2     1     3     3\n#&gt; 2   1368 2023-02-13          10     3     2     2     2\n#&gt; 3   1377 2023-02-13           9     4   \\n4     4     4\n#&gt; 4   1387 2023-02-13          11     3     3    NA    NA\n#&gt; 5   1347 2023-02-14           9     2     2     4     2\n#&gt; 6   1399 2023-02-14          12     4     1     3     1\n\n\n15.1.2.2 Individuare e rimuovere i duplicati\nIn questa fase, eseguiamo alcune modifiche necessarie al data frame, come rimuovere duplicati e ordinare i dati:\n\n\nVerifica duplicati: controlliamo i record duplicati nel dataset.\n\nRimuovi duplicati: manteniamo solo la prima occorrenza.\n\nOrdina per data: organizziamo i record in ordine crescente rispetto alla variabile svy_date.\n\nEsamina i dati puliti: controlliamo il risultato delle modifiche.\n\n\n# Identifica i duplicati basati su 'stu_id'\nduplicates &lt;- \n  svy[duplicated(svy$stu_id) | duplicated(svy$stu_id, fromLast = TRUE), ]\n\n\n# Visualizza i duplicati trovati\nduplicates\n#&gt;   stu_id   svy_date grade_level math1 math2 math3 math4\n#&gt; 1   1347 2023-02-13           9     2     1     3     3\n#&gt; 5   1347 2023-02-14           9     2     2     4     2\n\n\n# Ordina per 'svy_date' in ordine crescente\nsvy &lt;- svy[order(svy$svy_date), ]\n\n\n# Rimuove i duplicati mantenendo la prima occorrenza\nsvy &lt;- svy[!duplicated(svy$stu_id), ]\n\n\n# Esamina il dataset finale\nprint(svy)\n#&gt;   stu_id   svy_date grade_level math1 math2 math3 math4\n#&gt; 1   1347 2023-02-13           9     2     1     3     3\n#&gt; 2   1368 2023-02-13          10     3     2     2     2\n#&gt; 3   1377 2023-02-13           9     4   \\n4     4     4\n#&gt; 4   1387 2023-02-13          11     3     3    NA    NA\n#&gt; 6   1399 2023-02-14          12     4     1     3     1\n\nVerifichiamo le dimensioni del dataset pulito per assicurarci che le operazioni siano state eseguite correttamente:\n\n# Controlla il numero di righe e colonne\nsvy |&gt; \n  dim()\n#&gt; [1] 5 7\n\n\n15.1.2.3 De-identificazione dei dati\n\n# Rimuovi la colonna 'svy_date'\nsvy &lt;- svy |&gt;\n  dplyr::select(-svy_date)\n\n# Mostra i nomi delle colonne rimaste\nnames(svy)\n#&gt; [1] \"stu_id\"      \"grade_level\" \"math1\"       \"math2\"       \"math3\"      \n#&gt; [6] \"math4\"\n\n\n15.1.2.4 Rimuovere le colonne non necessarie\nNel caso presente, la rimozione di colonne non è necessaria. Tuttavia, in molti progetti di analisi dei dati, soprattutto quando i dati vengono raccolti utilizzando software di terze parti o strumenti specifici per esperimenti psicologici, è comune trovarsi con colonne che non sono pertinenti allo studio in corso.\nQueste colonne possono includere dati come identificatori interni, timestamp generati automaticamente, informazioni di debug, o variabili che non sono rilevanti per l’analisi che si intende condurre. Quando tali colonne sono irrilevanti per la ricerca, possono essere rimosse per semplificare il dataset e ridurre il rischio di confusione o errori durante l’analisi. Rimuovere le colonne non necessarie non solo rende il dataset più gestibile, ma aiuta anche a focalizzare l’analisi sulle variabili che realmente importano per rispondere alle domande di ricerca.\n\n15.1.2.5 Dividere le colonne secondo necessità\nNel caso presente, questa operazione non è necessaria. Tuttavia, se si lavora con un dataset che include una colonna chiamata “NomeCompleto”, contenente sia il nome che il cognome di uno studente, per esempio, è buona pratica separare questa colonna in due colonne distinte, “Nome” e “Cognome”. Questa suddivisione facilita l’analisi e la manipolazione dei dati, rendendoli più organizzati e accessibili.\n\n15.1.2.6 Rinominare le colonne\nÈ importante assegnare nomi chiari alle colonne del dataset. Utilizzare nomi di variabili comprensibili aiuta a rendere l’analisi dei dati più intuitiva e a ridurre il rischio di errori interpretativi.\nEsempi di buone pratiche:\n\nEvita nomi di colonne come “x” o acronimi incomprensibili. Questi possono creare confusione durante l’analisi, specialmente se il dataset viene condiviso con altri ricercatori o se viene ripreso dopo un lungo periodo di tempo.\nInvece, cerca di utilizzare nomi di variabili che descrivano chiaramente il contenuto della colonna. Ad esempio, invece di “x1” o “VAR123”, un nome come “ansia_base” o “liv_autoefficacia” è molto più comprensibile e immediato.\nPer i nomi composti, utilizza un separatore come il trattino basso _. Ad esempio, se stai lavorando con dati relativi a un test psicologico, potresti avere colonne chiamate “test_ansia_pre” e “test_ansia_post” per indicare i risultati del test di ansia prima e dopo un intervento.\n\nEsempi di nomi di colonne ben scelti:\n\n\nNome generico: TS, AE\n\n\nNome migliore: tempo_studio, auto_efficacia\n\n\n\n\nNome generico: S1, S2\n\n\nNome migliore: stress_situazione1, stress_situazione2\n\n\n\n\nNome generico: Q1, Q2\n\n\nNome migliore: qualità_sonno_sett1, qualità_sonno_sett2\n\n\n\n\n15.1.2.7 Trasformare le variabili\nNel caso presente non si applica, ma è un passo importante in molte analisi dei dati.\nEsempi di trasformazione delle variabili:\n\nLogaritmo di una variabile: Immaginiamo di avere una variabile che misura i tempi di reazione dei partecipanti a un esperimento. Se i tempi di reazione hanno una distribuzione fortemente asimmetrica (con alcuni valori molto elevati), potrebbe essere utile applicare una trasformazione logaritmica per rendere la distribuzione più simmetrica e migliorare l’interpretabilità dei risultati.\nCodifica delle variabili categoriche: Se è presente una variabile categorica come il “tipo di intervento” con valori come “cognitivo”, “comportamentale” e “farmacologico”, potrebbe essere necessario trasformare questa variabile in variabili dummy (ad esempio, intervento_cognitivo, intervento_comportamentale, intervento_farmacologico), dove ogni variabile assume il valore 0 o 1 a seconda della presenza o meno di quel tipo di intervento. Questo è utile quando si utilizzano tecniche di regressione.\n\n15.1.2.8 Standardizzazione delle variabili\nLa standardizzazione è utile quando si desidera rendere comparabili variabili misurate su scale diverse, ad esempio per confronti tra gruppi o per inclusione in modelli di regressione.\nEsempio. Supponiamo di avere una variabile che misura il livello di ansia su una scala da 0 a 100. Per standardizzarla:\n\n\nSottrai la media del campione dalla variabile.\n\n\nDividi per la deviazione standard.\n\nIl risultato è una variabile con media pari a 0 e deviazione standard pari a 1.\nVantaggi della standardizzazione:\n\nfacilita l’interpretazione dei coefficienti in un modello di regressione;\npermette un confronto diretto tra variabili che hanno unità di misura diverse.\n\n15.1.2.9 Normalizzazione delle variabili\nLa normalizzazione consiste nel ridimensionare i dati su una scala predefinita, spesso compresa tra 0 e 1. Questo processo è particolarmente utile in analisi multivariate, dove variabili con scale molto diverse potrebbero influenzare in modo sproporzionato i risultati.\nEsempio. Hai dati su:\n\nOre di sonno (misurate in ore, da 0 a 24).\n\nLivello di stress (misurato su una scala da 1 a 50).\n\nAuto-efficacia (misurata su una scala da 0 a 100).\n\nPer garantire che ogni variabile abbia lo stesso peso nell’analisi, puoi normalizzarle usando una formula come:\n\\[\nx_{\\text{norm}} = \\frac{x - \\text{min}(x)}{\\text{max}(x) - \\text{min}(x)} .\n\\]\nPerché Standardizzare o Normalizzare?\nTrasformare le variabili è cruciale per:\n\n\nGestire scale diverse: Riduce il rischio che variabili con valori numericamente più grandi dominino i risultati.\n\n\nGarantire validità e interpretabilità: Facilita il confronto tra dati provenienti da fonti o gruppi diversi.\n\n\nEvitare problemi numerici nei modelli statistici: Alcuni algoritmi di machine learning o tecniche di ottimizzazione richiedono che i dati siano su scale simili per funzionare correttamente.\n\nIn conclusione, la scelta tra standardizzazione e normalizzazione dipende dal contesto dell’analisi e dagli obiettivi specifici. Entrambi i processi sono strumenti indispensabili per garantire che i dati siano trattati in modo adeguato, portando a risultati robusti e facilmente interpretabili.\n\n15.1.2.10 Aggiornare i tipi delle variabili\nNel caso presente non è necessario. Supponiamo invece di avere una colonna in un dataset psicologico che contiene punteggi di un questionario, ma i dati sono stati importati come stringhe (testo) invece che come numeri. Per eseguire calcoli statistici, sarà necessario convertire questa colonna da stringa a numerico.\nIn R, si potrebbe usare il seguente codice:\n\n# Supponiamo di avere un data frame chiamato 'df' con una colonna 'punteggio' importata come carattere\ndf$punteggio &lt;- as.numeric(df$punteggio)\n\n# Ora la colonna 'punteggio' è stata convertita in un tipo numerico ed è possibile eseguire calcoli su di essa\n\nIn questo esempio, la funzione as.numeric() viene utilizzata per convertire la colonna punteggio in un formato numerico, permettendo di eseguire analisi quantitative sui dati.\nUn altro caso molto comune si verifica quando si importano dati da file Excel. Spesso capita che, all’interno di una cella di una colonna che dovrebbe contenere solo valori numerici, venga inserito erroneamente uno o più caratteri alfanumerici. Di conseguenza, l’intera colonna viene interpretata come di tipo alfanumerico, anche se i valori dovrebbero essere numerici. In questi casi, è fondamentale individuare la cella problematica, correggere manualmente il valore errato, e poi riconvertire l’intera colonna da alfanumerica a numerica.\n\n15.1.2.11 Ricodificare le variabili\nAnche se in questo caso non è necessario, la ricodifica delle variabili è una pratica molto comune nelle analisi dei dati psicologici.\nPer esempio, consideriamo una variabile categoriale con modalità descritte da stringhe poco comprensibili, che vengono ricodificate con nomi più chiari e comprensibili.\nSupponiamo di avere un DataFrame chiamato df con una colonna tipo_intervento che contiene le modalità \"CT\", \"BT\", e \"MT\" per rappresentare rispettivamente “Terapia Cognitiva”, “Terapia Comportamentale” e “Terapia Mista”. Queste abbreviazioni potrebbero non essere immediatamente chiare a chiunque analizzi i dati, quindi decidiamo di ricodificarle con nomi più espliciti. Ecco come farlo in R:\n\n# Tibble chiamato 'df' con una colonna 'tipo_intervento'\ndf &lt;- tibble(tipo_intervento = c(\"CT\", \"BT\", \"MT\", \"CT\", \"BT\"))\ndf\n#&gt; # A tibble: 5 × 1\n#&gt;   tipo_intervento\n#&gt;   &lt;chr&gt;          \n#&gt; 1 CT             \n#&gt; 2 BT             \n#&gt; 3 MT             \n#&gt; 4 CT             \n#&gt; 5 BT\n\n\n# Ricodifica delle modalità della variabile 'tipo_intervento' in nomi più comprensibili\ndf &lt;- df %&gt;%\n  mutate(tipo_intervento_ricodificato = dplyr::recode(\n    tipo_intervento,\n    \"CT\" = \"Terapia Cognitiva\",\n    \"BT\" = \"Terapia Comportamentale\",\n    \"MT\" = \"Terapia Mista\"\n  ))\n\n# Mostra il tibble con la nuova colonna ricodificata\ndf\n#&gt; # A tibble: 5 × 2\n#&gt;   tipo_intervento tipo_intervento_ricodificato\n#&gt;   &lt;chr&gt;           &lt;chr&gt;                       \n#&gt; 1 CT              Terapia Cognitiva           \n#&gt; 2 BT              Terapia Comportamentale     \n#&gt; 3 MT              Terapia Mista               \n#&gt; 4 CT              Terapia Cognitiva           \n#&gt; 5 BT              Terapia Comportamentale\n\n\n15.1.2.12 Aggiungere nuove variabili nel data frame\nNel caso presente non è richiesto, ma aggiungere nuove variabili a un DataFrame è un’operazione comune durante l’analisi dei dati. Un esempio è il calcolo dell’indice di massa corporea (BMI).\nSupponiamo di avere un DataFrame chiamato df che contiene le colonne peso_kg (peso in chilogrammi) e altezza_m (altezza in metri) per ciascun partecipante a uno studio psicologico. Per arricchire il dataset, possiamo calcolare il BMI per ogni partecipante e aggiungerlo come una nuova variabile.\nIl BMI si calcola con la formula:\n\\[ \\text{BMI} = \\frac{\\text{peso in kg}}{\\text{altezza in metri}^2} .\\]\nEcco come aggiungere la nuova colonna.\n\n# Supponiamo di avere un tibble chiamato 'df' con le colonne 'peso_kg' e 'altezza_m'\ndf &lt;- tibble(\n  peso_kg = c(70, 85, 60, 95),\n  altezza_m = c(1.75, 1.80, 1.65, 1.90)\n)\ndf\n#&gt; # A tibble: 4 × 2\n#&gt;   peso_kg altezza_m\n#&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1      70      1.75\n#&gt; 2      85      1.8 \n#&gt; 3      60      1.65\n#&gt; 4      95      1.9\n\n\n# Calcola il BMI e aggiungilo come una nuova colonna 'BMI'\ndf &lt;- df %&gt;%\n  mutate(BMI = peso_kg / (altezza_m^2))\n\n# Mostra il tibble con la nuova variabile aggiunta\ndf\n#&gt; # A tibble: 4 × 3\n#&gt;   peso_kg altezza_m   BMI\n#&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1      70      1.75  22.9\n#&gt; 2      85      1.8   26.2\n#&gt; 3      60      1.65  22.0\n#&gt; 4      95      1.9   26.3\n\n\n15.1.3 Affrontare il problema dei dati mancanti\nI dati mancanti sono un problema comune nelle ricerche psicologiche e in molte altre discipline. Quando mancano delle informazioni in un dataset, possono verificarsi gravi problemi per l’analisi statistica, come risultati distorti, riduzione della precisione delle stime o, in alcuni casi, l’impossibilità di applicare alcuni algoritmi.\n\n15.1.3.1 Perché i dati mancanti sono un problema?\nImmagina di voler capire il rendimento medio di una classe in un test, ma alcuni studenti hanno lasciato delle risposte in bianco. Se ignoriamo le risposte mancanti o eliminiamo gli studenti con dati incompleti, rischiamo di ottenere una stima che non rappresenta correttamente la realtà. Questo succede perché:\n\n\nBias dei risultati: Se i dati mancanti non sono casuali (ad esempio, i dati mancanti sono presenti più spesso in studenti con basso rendimento), le conclusioni possono essere errate.\n\nRiduzione della potenza statistica: Eliminare dati incompleti riduce il numero totale di osservazioni, rendendo più difficile trovare risultati significativi.\n\nImpossibilità di applicare alcuni metodi: Molti algoritmi statistici richiedono dati completi e non funzionano con valori mancanti.\n\n15.1.3.2 Come gestire i dati mancanti?\nCi sono diversi modi per affrontare i dati mancanti. Vediamo prima i metodi più semplici e poi quelli più avanzati.\nEsaminiamo il data frame iniziale:\n\nsvy |&gt; \n  summary()\n#&gt;      stu_id      grade_level       math1        math2               math3     \n#&gt;  Min.   :1347   Min.   : 9.0   Min.   :2.0   Length:5           Min.   :2.00  \n#&gt;  1st Qu.:1368   1st Qu.: 9.0   1st Qu.:3.0   Class :character   1st Qu.:2.75  \n#&gt;  Median :1377   Median :10.0   Median :3.0   Mode  :character   Median :3.00  \n#&gt;  Mean   :1376   Mean   :10.2   Mean   :3.2                      Mean   :3.00  \n#&gt;  3rd Qu.:1387   3rd Qu.:11.0   3rd Qu.:4.0                      3rd Qu.:3.25  \n#&gt;  Max.   :1399   Max.   :12.0   Max.   :4.0                      Max.   :4.00  \n#&gt;                                                                 NA's   :1     \n#&gt;      math4     \n#&gt;  Min.   :1.00  \n#&gt;  1st Qu.:1.75  \n#&gt;  Median :2.50  \n#&gt;  Mean   :2.50  \n#&gt;  3rd Qu.:3.25  \n#&gt;  Max.   :4.00  \n#&gt;  NA's   :1\n\nSi noti la presenza di dati mancanti sulle variabili math3 e math4.\n\ndim(svy)\n#&gt; [1] 5 6\n\n\n\nEsclusione dei Casi Incompleti (Complete Case Analysis).\nUn approccio comune, ma spesso non ideale, è quello di analizzare solo i casi completi, eliminando tutte le righe con valori mancanti. In R, questo si può fare con il seguente comando:\n\nsvy_comp &lt;- svy |&gt;\n  drop_na()\n\nIn questo modo abbiamo escluso tutte le righe nelle quali sono presenti dei dati mancanti (in questo caso, una sola riga).\n\ndim(svy_comp)\n#&gt; [1] 4 6\n\nLimite: Questo metodo può introdurre bias se i dati mancanti non sono casuali e riduce il campione, compromettendo l’affidabilità delle analisi.\n\n\nImputazione Semplice\nSostituire i valori mancanti con stime semplici:\n\n\nMedia o mediana: Per le variabili numeriche, sostituire i valori mancanti con la media o la mediana. Questo metodo è facile da implementare, ma può ridurre la variabilità nei dati.\n\nModa: Per le variabili categoriche, sostituire i valori mancanti con il valore più frequente. Tuttavia, può introdurre distorsioni se i dati sono molto eterogenei.\n\n\n\nImputazione Multipla\nUn approccio più avanzato è l’imputazione multipla, che utilizza modelli statistici per stimare i valori mancanti in modo iterativo. L’idea di base è semplice: ogni valore mancante viene stimato tenendo conto delle relazioni con tutte le altre variabili.\nVantaggi:\n\nMantiene la variabilità dei dati.\nPreserva le relazioni tra le variabili.\nRiduce il rischio di bias rispetto ai metodi semplici.\n\n\n\n15.1.3.3 Applicazione pratica: imputazione multipla con mice in R\nSupponiamo di avere un dataset con alcune colonne numeriche che contengono valori mancanti. Possiamo utilizzare il pacchetto mice per imputare i dati mancanti.\nSelezioniamo le colonne numeriche da imputare.\n\nnumeric_columns &lt;- c(\"math1\", \"math2\", \"math3\", \"math4\")\nsvy &lt;- svy %&gt;%\n  mutate(across(all_of(numeric_columns), as.numeric))\n\nEseguiamo l’imputazione multipla.\n\nimputed &lt;- mice(\n  svy[numeric_columns], \n  m = 1, \n  maxit = 10, \n  method = \"norm.predict\", \n  seed = 123\n)\n#&gt; \n#&gt;  iter imp variable\n#&gt;   1   1  math3  math4\n#&gt;   2   1  math3  math4\n#&gt;   3   1  math3  math4\n#&gt;   4   1  math3  math4\n#&gt;   5   1  math3  math4\n#&gt;   6   1  math3  math4\n#&gt;   7   1  math3  math4\n#&gt;   8   1  math3  math4\n#&gt;   9   1  math3  math4\n#&gt;   10   1  math3  math4\n\nOttieniamo il dataset con i valori imputati.\n\nsvy_imputed &lt;- complete(imputed)\n\nArrotondiamo i valori imputati (se necessario).\n\nsvy_imputed &lt;- svy_imputed %&gt;%\n  mutate(across(everything(), round))\n\nSostituiamo i valori imputati nel dataset originale.\n\nsvy[numeric_columns] &lt;- svy_imputed\n\nEsaminiamo il risultato ottenuto.\n\nsvy |&gt; \n  summary()\n#&gt;      stu_id      grade_level       math1         math2         math3    \n#&gt;  Min.   :1347   Min.   : 9.0   Min.   :2.0   Min.   :1.0   Min.   :2.0  \n#&gt;  1st Qu.:1368   1st Qu.: 9.0   1st Qu.:3.0   1st Qu.:1.0   1st Qu.:3.0  \n#&gt;  Median :1377   Median :10.0   Median :3.0   Median :2.0   Median :3.0  \n#&gt;  Mean   :1376   Mean   :10.2   Mean   :3.2   Mean   :2.2   Mean   :3.2  \n#&gt;  3rd Qu.:1387   3rd Qu.:11.0   3rd Qu.:4.0   3rd Qu.:3.0   3rd Qu.:4.0  \n#&gt;  Max.   :1399   Max.   :12.0   Max.   :4.0   Max.   :4.0   Max.   :4.0  \n#&gt;      math4    \n#&gt;  Min.   :1.0  \n#&gt;  1st Qu.:2.0  \n#&gt;  Median :3.0  \n#&gt;  Mean   :2.8  \n#&gt;  3rd Qu.:4.0  \n#&gt;  Max.   :4.0\n\n\n15.1.3.4 Come funziona l’imputazione multipla?\n\nOgni variabile con valori mancanti viene modellata come funzione delle altre variabili.\nI valori mancanti vengono stimati iterativamente. In ogni iterazione, si utilizza l’output precedente come input per migliorare le stime.\nDopo un numero sufficiente di iterazioni, le stime si stabilizzano (convergenza).\n\nIn conclusione, l’imputazione multipla è una tecnica avanzata che permette di gestire i dati mancanti preservando la qualità delle analisi. Rispetto ai metodi semplici, consente di mantenere la variabilità e ridurre il rischio di bias, rendendola una scelta ideale per analisi psicologiche e di ricerca.\n\n15.1.3.5 Aggiungere i metadati\nI metadati sono informazioni che descrivono i dati stessi, come etichette di variabili, etichette di valori, informazioni sull’origine dei dati, unità di misura e altro ancora. Questi metadati sono essenziali per comprendere, documentare e condividere correttamente un dataset.\nIn R, i metadati sono gestiti in modo molto dettagliato e strutturato attraverso pacchetti come haven, labelled, e Hmisc. Questi pacchetti consentono di associare etichette ai dati, come etichette di variabili e di valori, e persino di gestire i valori mancanti con etichette specifiche.\n\n\nEtichette di variabili: Si possono aggiungere direttamente alle colonne di un DataFrame usando funzioni come labelled::set_variable_labels().\n\nEtichette di valori: Possono essere aggiunte a variabili categoriali utilizzando labelled::labelled().\n\nValori mancanti: In R, è possibile etichettare specifici valori come mancanti usando labelled::na_values&lt;-.\n\nQuesti strumenti rendono molto facile documentare un dataset all’interno del processo di analisi, assicurando che tutte le informazioni critiche sui dati siano facilmente accessibili e ben documentate.\nEsaminiamo un esempio pratico. Consideriamo nuovamente il data set svy:\n\nglimpse(svy)\n#&gt; Rows: 5\n#&gt; Columns: 6\n#&gt; $ stu_id      &lt;int&gt; 1347, 1368, 1377, 1387, 1399\n#&gt; $ grade_level &lt;int&gt; 9, 10, 9, 11, 12\n#&gt; $ math1       &lt;dbl&gt; 2, 3, 4, 3, 4\n#&gt; $ math2       &lt;dbl&gt; 1, 2, 4, 3, 1\n#&gt; $ math3       &lt;dbl&gt; 3, 2, 4, 4, 3\n#&gt; $ math4       &lt;dbl&gt; 3, 2, 4, 4, 1\n\nSi noti che la variabile math2 contiene un valore inamissibile, probabilmente un errore di battitura. Questo fa in modo che math2 sia di tipo char mentre dovrebbe essere una variabile numerica. Correggiamo.\n\n# Correzione di `math2`: Rimuovi valori non validi e converti in numerico\nsvy$math2 &lt;- gsub(\"\\\\n\", \"\", svy$math2)  # Rimuovi caratteri non validi come '\\n'\nsvy$math2 &lt;- as.numeric(svy$math2)      # Converte la variabile in numerico\n\n# Visualizzazione del dataset corretto\nprint(svy)\n#&gt;   stu_id grade_level math1 math2 math3 math4\n#&gt; 1   1347           9     2     1     3     3\n#&gt; 2   1368          10     3     2     2     2\n#&gt; 3   1377           9     4     4     4     4\n#&gt; 4   1387          11     3     3     4     4\n#&gt; 6   1399          12     4     1     3     1\n\nDefiniamo le etichette di valore per le variabili math1:math4.\n\nvalue_labels_math &lt;- set_names(\n  as.numeric(names(c(\n    `1` = \"strongly disagree\",\n    `2` = \"disagree\",\n    `3` = \"agree\",\n    `4` = \"strongly agree\"\n  ))),\n  c(\"strongly disagree\", \"disagree\", \"agree\", \"strongly agree\")\n)\n\nAggiungiamo le etichette di valore alle colonne math1:math4.\n\nsvy &lt;- svy %&gt;%\n  mutate(across(starts_with(\"math\"), ~ labelled(., labels = value_labels_math)))\n\nVerifica delle etichette.\n\nval_labels(svy$math1)\n#&gt; strongly disagree          disagree             agree    strongly agree \n#&gt;                 1                 2                 3                 4\n\n\nsvy\n#&gt;   stu_id grade_level math1 math2 math3 math4\n#&gt; 1   1347           9     2     1     3     3\n#&gt; 2   1368          10     3     2     2     2\n#&gt; 3   1377           9     4     4     4     4\n#&gt; 4   1387          11     3     3     4     4\n#&gt; 6   1399          12     4     1     3     1\n\n\n15.1.3.5.1 Utilizzo delle etichette in R con variabili numeriche\nLe etichette dei valori (value labels) vengono utilizzate per rendere più leggibili e interpretabili le variabili numeriche, associando ad ogni valore un’etichetta descrittiva. Questo approccio è particolarmente utile in ambiti come la ricerca psicologica, dove le risposte ai questionari sono spesso codificate con numeri (ad esempio, 1 = “Strongly Disagree”, 2 = “Disagree”, ecc.) ma rappresentano concetti qualitativi.\nVantaggi dell’Uso delle Etichette\n\n\nChiarezza nelle Analisi: Le etichette descrittive rendono i dati più facilmente comprensibili senza dover ricordare il significato numerico di ciascun valore.\n\n\nDocumentazione Integrata: Permettono di incorporare metadati direttamente nelle variabili, migliorando la trasparenza e riducendo il rischio di interpretazioni errate.\n\nCompatibilità con Software Statistici: Molti strumenti (ad esempio, SPSS o Stata) utilizzano etichette di valori. Il pacchetto haven in R consente di gestire facilmente i dati etichettati esportati/importati da questi software.\n\nManipolazione di Variabili Etichettate\nAnche se una variabile numerica è etichettata con labelled (ad esempio, tramite il pacchetto haven), essa conserva la sua natura numerica e può essere utilizzata in calcoli, modelli statistici, e trasformazioni. Le etichette non alterano il valore sottostante, ma lo arricchiscono con informazioni aggiuntive.\nIn conclusione, le etichette dei valori migliorano l’interpretabilità dei dati senza comprometterne la manipolabilità. Questo approccio è ideale per mantenere le variabili numeriche pienamente funzionali per analisi statistiche, mentre le etichette descrittive forniscono un contesto chiaro e leggibile.\n\n15.1.3.6 Validazione dei dati\nLa validazione dei dati è un passaggio fondamentale per garantire che il dataset soddisfi i criteri previsti e sia pronto per le analisi successive. Questo processo include il controllo della coerenza e della correttezza dei dati in base a specifiche regole definite dal dizionario dei dati. Alcune verifiche comuni includono:\n\n\nUnicità delle righe: Assicurarsi che ogni riga sia unica, verificando l’assenza di ID duplicati.\n\nValidità degli ID: Controllare che gli ID rientrino in un intervallo previsto (es. numerico).\n\nValori accettabili nelle variabili categoriali: Verificare che variabili come grade_level, int e le colonne math contengano esclusivamente valori appartenenti a un set di valori validi.\n\nIl pacchetto pointblank fornisce strumenti flessibili e intuitivi per eseguire verifiche di validazione e generare report dettagliati. Questo pacchetto consente di:\n\n\nDefinire le regole di validazione: Specificare controlli come unicità, intervalli di valori e appartenenza a insiemi predefiniti.\n\nEseguire i controlli: Applicare le regole di validazione su un dataset per identificare eventuali discrepanze.\n\nGenerare report interattivi: Creare un riepilogo chiaro e visivo dei controlli, evidenziando eventuali errori o anomalie.\n\nCon pointblank, è possibile integrare la validazione dei dati come parte di un workflow strutturato, garantendo la qualità dei dati in modo sistematico e ripetibile.\ncreate_agent(svy) %&gt;%\n  rows_distinct(columns = vars(stu_id)) %&gt;%\n  col_vals_between(columns = c(stu_id), \n                   left = 1300, right = 1400, na_pass = TRUE) %&gt;%\n  col_vals_in_set(columns = c(grade_level), \n                  set = c(9, 10, 11, 12, NA)) %&gt;%\n  col_vals_in_set(columns = c(int),\n                  set = c(0, 1, NA)) %&gt;%\n  col_vals_in_set(columns = c(math1:math4),\n                  set = c(1, 2, 3, 4, NA)) %&gt;%\n  interrogate()\nIl dataset ripulito soddisfa tutte le aspettative delineate da Crystal Lewis.\n\n\nCompleto: Tutti i dati raccolti sono stati inseriti e/o recuperati. Non dovrebbero esserci dati estranei che non appartengono al dataset (come duplicati o partecipanti non autorizzati).\n\nValido: Le variabili rispettano i vincoli definiti nel tuo dizionario dei dati. Ricorda che il dizionario dei dati specifica i nomi delle variabili, i tipi, i range, le categorie e altre informazioni attese.\n\nAccurato: Sebbene non sia sempre possibile determinare l’accuratezza dei valori durante il processo di pulizia dei dati (ovvero, se un valore è realmente corretto o meno), in alcuni casi è possibile valutarla sulla base della conoscenza pregressa riguardante quel partecipante o caso specifico.\n\nCoerente: I valori sono allineati tra le varie fonti. Ad esempio, la data di nascita raccolta attraverso un sondaggio studentesco dovrebbe avere un formato corrispondere alla data di nascita raccolta dal distretto scolastico.\n\nUniforme: I dati sono standardizzati attraverso i moduli e nel tempo. Ad esempio, lo stato di partecipazione ai programmi di pranzo gratuito o a prezzo ridotto è sempre fornito come una variabile numerica con la stessa rappresentazione, oppure il nome della scuola è sempre scritto in modo coerente in tutto il dataset.\n\nDe-identificato: Tutte le informazioni personali identificabili (PII) sono state rimosse dal dataset per proteggere la riservatezza dei partecipanti (se richiesto dal comitato etico/consenso informato).\n\nInterpretabile: I dati hanno nomi di variabili leggibili sia da umani che dal computer, e sono presenti etichette di variabili e valori laddove necessario per facilitare l’interpretazione.\n\nAnalizzabile: Il dataset è in un formato rettangolare (righe e colonne), leggibile dal computer e conforme alle regole di base della struttura dei dati.\n\nUna volta completati i 14 passaggi precedenti, è possibile esportare questo dataset ripulito nella cartella processed per le successive analisi statistiche.\n\n15.1.3.7 Unire e/o aggiungere dati se necessario\nIn questo passaggio, è possibile unire o aggiungere colonne o righe presenti in file diversi. È importante eseguire nuovamente i controlli di validazione dopo l’unione/aggiunta di nuovi dati.\n\n15.1.3.8 Trasformare i dati se necessario\nEsistono vari motivi per cui potrebbe essere utile memorizzare i dati in formato long o wide. In questo passaggio, è possibile ristrutturare i dati secondo le esigenze.\n\n15.1.3.9 Salvare il dataset pulito finale\nL’ultimo passaggio del processo di pulizia consiste nell’esportare o salvare il dataset pulito. Come accennato in precedenza, può essere utile esportare/salvare il dataset in più di un formato di file (ad esempio, un file .csv e un file .parquet).",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#organizzazione-dei-file-e-informazioni-aggiuntive",
    "href": "chapters/eda/02_data_cleaning.html#organizzazione-dei-file-e-informazioni-aggiuntive",
    "title": "15  Flusso di lavoro per la pulizia dei dati",
    "section": "\n15.2 Organizzazione dei file e informazioni aggiuntive",
    "text": "15.2 Organizzazione dei file e informazioni aggiuntive\nInfine, è essenziale includere una documentazione adeguata per garantire che le informazioni siano interpretate correttamente, sia da altri utenti che da te stesso, se dovessi tornare a lavorare su questo progetto in futuro. La documentazione minima da fornire dovrebbe includere:\n\n\nDocumentazione a livello di progetto: Questa sezione fornisce informazioni contestuali sul perché e come i dati sono stati raccolti. È utile per chiunque voglia comprendere lo scopo e la metodologia del progetto.\n\nMetadati a livello di progetto: Se condividi i dati in un repository pubblico o privato, è importante includere metadati a livello di progetto. Questi metadati forniscono informazioni dettagliate che facilitano la ricerca, la comprensione e la consultabilità dei dati. I metadati a livello di progetto possono includere descrizioni generali del progetto, parole chiave, e riferimenti bibliografici.\n\nDizionario dei dati: Un documento che descrive tutte le variabili presenti nel dataset, inclusi i loro nomi, tipi, range di valori, categorie e qualsiasi altra informazione rilevante. Questo strumento è fondamentale per chiunque voglia comprendere o analizzare i dati.\n\nREADME: Un file che fornisce una panoramica rapida dei file inclusi nel progetto, spiegando cosa contengono e come sono interconnessi. Il README è spesso il primo documento consultato e serve a orientare l’utente tra i vari file e risorse del progetto. Questa documentazione non solo aiuta a mantenere il progetto organizzato, ma è anche cruciale per facilitare la collaborazione e l’archiviazione a lungo termine.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#dizionario-dei-dati",
    "href": "chapters/eda/02_data_cleaning.html#dizionario-dei-dati",
    "title": "15  Flusso di lavoro per la pulizia dei dati",
    "section": "\n15.3 Dizionario dei dati",
    "text": "15.3 Dizionario dei dati\nApprofondiamo qui il problema della creazione del Dizionario dei dati.\nUn dizionario dei dati è un documento che descrive le caratteristiche di ciascuna variabile in un dataset. Include informazioni come il nome della variabile, il tipo di dato, il range di valori, le categorie (per le variabili categoriche), e altre informazioni rilevanti. Questo strumento è essenziale per comprendere e analizzare correttamente il dataset.\nSi presti particolare attenzione alle guide di stile per la denominazione delle variabili e la codifica dei valori delle risposte.\n\n15.3.1 Esempio in R\n\nEcco come tradurre i passi per creare un dizionario dei dati in R, utilizzando il pacchetto tibble per creare il dizionario e writexl o readr per esportarlo in formato .xlsx o .csv.\n\n\nIdentificare le variabili: Elencare tutte le variabili presenti nel dataset.\n\nDescrivere ogni variabile: Per ciascuna variabile, definire il tipo (ad esempio, integer, numeric, character), il range di valori accettabili o le categorie, e fornire una descrizione chiara.\n\nSalvare il dizionario dei dati: Il dizionario può essere salvato in un file .csv o .xlsx per una facile consultazione.\n\nsvy\nCreeremo un dizionario dei dati per un dataset di esempio e lo salveremo sia in formato CSV che Excel.\nlibrary(tibble)\nlibrary(readr)\nlibrary(writexl)\n\n# Creazione del Dizionario dei Dati\ndata_dict &lt;- tibble(\n  `Variable Name` = c(\n    \"stu_id\",\n    \"svy_date\",\n    \"grade_level\",\n    \"math1\",\n    \"math2\",\n    \"math3\",\n    \"math4\"\n  ),\n  `Type` = c(\n    \"integer\",\n    \"datetime\",\n    \"integer\",\n    \"integer\",\n    \"integer\",\n    \"numeric\",\n    \"numeric\"\n  ),\n  `Description` = c(\n    \"Student ID\",\n    \"Survey Date\",\n    \"Grade Level\",\n    \"Math Response 1 (1: Strongly Disagree, 4: Strongly Agree)\",\n    \"Math Response 2 (1: Strongly Disagree, 4: Strongly Agree)\",\n    \"Math Response 3 (1: Strongly Disagree, 4: Strongly Agree)\",\n    \"Math Response 4 (1: Strongly Disagree, 4: Strongly Agree)\"\n  ),\n  `Range/Values` = c(\n    \"1347-1399\",\n    \"2023-02-13 to 2023-02-14\",\n    \"9-12\",\n    \"1-4\",\n    \"1-4\",\n    \"1.0-4.0 (NA allowed)\",\n    \"1.0-4.0 (NA allowed)\"\n  )\n)\n\n# Visualizza il Dizionario dei Dati\nprint(data_dict)\n\n# Salva il Dizionario dei Dati in un file CSV\nwrite_csv(data_dict, here::here(\"data\", \"processed\", \"data_dictionary.csv\"))\n\n# Salva il Dizionario dei Dati in un file Excel\nwrite_xlsx(data_dict, here::here(\"data\", \"processed\", \"data_dictionary.xlsx\"))\nOutput Atteso: file CSV (data_dictionary.csv).\ndata_dict &lt;- rio::import(\n  here::here(\"data\", \"processed\", \"data_dictionary.csv\")\n)\nprint(data_dict)\n\n15.3.1.1 Uso del pacchetto dataMeta\n\nIl pacchetto dataMeta è progettato per generare metadati e dizionari dei dati in modo strutturato.\n\nlibrary(dataMeta)\nlibrary(tibble)\n\n# Descrizioni delle variabili\nvariable_descriptions &lt;- c(\n  \"Student ID\",\n  \"Grade Level\",\n  \"Math Response 1 (1: Strongly Disagree, 4: Strongly Agree)\",\n  \"Math Response 2 (1: Strongly Disagree, 4: Strongly Agree)\",\n  \"Math Response 3 (1: Strongly Disagree, 4: Strongly Agree)\",\n  \"Math Response 4 (1: Strongly Disagree, 4: Strongly Agree)\"\n)\n\nvar_type &lt;- c(1, 0, 0, 0, 0, 0)\n\nlinker &lt;- build_linker(\n  svy, \n  variable_description = variable_descriptions, \n  variable_type = var_type\n)\n\ndict &lt;- build_dict(\n  my.data = svy, \n  linker = linker, \n  option_description = NULL, \n  prompt_varopts = FALSE\n)\n\nkable(dict, format = \"html\", caption = \"Data dictionary for original dataset\")\n\n\nData dictionary for original dataset\n\nvariable_name\nvariable_description\nvariable_options\n\n\n\ngrade_level\nGrade Level\n9 to 12\n\n\nmath1\nMath Response 1 (1: Strongly Disagree, 4: Strongly Agree)\n2 to 4\n\n\nmath2\nMath Response 2 (1: Strongly Disagree, 4: Strongly Agree)\n1 to 4\n\n\nmath3\nMath Response 3 (1: Strongly Disagree, 4: Strongly Agree)\n2 to 4\n\n\nmath4\nMath Response 4 (1: Strongly Disagree, 4: Strongly Agree)\n1 to 4\n\n\nstu_id\nStudent ID\n1347\n\n\n\n\n1368\n\n\n\n\n1377\n\n\n\n\n1387\n\n\n\n\n1399\n\n\n\n\n\n\n15.3.1.2 Uso del pacchetto skimr\n\nIl pacchetto skimr è utile per generare riassunti dettagliati delle variabili, che possono essere utilizzati come base per un dizionario.\n\nlibrary(skimr)\n\n# Riassunto del dataset\nskim_dict &lt;- skim(svy)\n\nkable(skim_dict, format = \"html\", caption = \"Data dictionary for original dataset\")\n\n\nData dictionary for original dataset\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\nnumeric\nstu_id\n0\n1\n1375.6\n19.718\n1347\n1368\n1377\n1387\n1399\n▃▁▇▃▃\n\n\nnumeric\ngrade_level\n0\n1\n10.2\n1.304\n9\n9\n10\n11\n12\n▇▃▁▃▃\n\n\nnumeric\nmath1\n0\n1\n3.2\n0.837\n2\n3\n3\n4\n4\n▃▁▇▁▇\n\n\nnumeric\nmath2\n0\n1\n2.2\n1.304\n1\n1\n2\n3\n4\n▇▃▁▃▃\n\n\nnumeric\nmath3\n0\n1\n3.2\n0.837\n2\n3\n3\n4\n4\n▃▁▇▁▇\n\n\nnumeric\nmath4\n0\n1\n2.8\n1.304\n1\n2\n3\n4\n4\n▃▃▁▃▇",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#riflessioni-conclusive",
    "href": "chapters/eda/02_data_cleaning.html#riflessioni-conclusive",
    "title": "15  Flusso di lavoro per la pulizia dei dati",
    "section": "Riflessioni conclusive",
    "text": "Riflessioni conclusive\nNel processo di analisi dei dati, la fase di pulizia e pre-elaborazione è cruciale per garantire la qualità e l’integrità dei risultati finali. Sebbene questa fase possa sembrare meno interessante rispetto all’analisi vera e propria, essa costituisce la base su cui si costruiscono tutte le successive elaborazioni e interpretazioni. Attraverso una serie di passaggi strutturati, come quelli illustrati in questo capitolo, è possibile trasformare dati grezzi e disordinati in un dataset pulito, coerente e pronto per l’analisi. La cura nella gestione dei dati, dalla rimozione di duplicati alla creazione di un dizionario dei dati, è fondamentale per ottenere risultati affidabili e riproducibili.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#esercizi",
    "href": "chapters/eda/02_data_cleaning.html#esercizi",
    "title": "15  Flusso di lavoro per la pulizia dei dati",
    "section": "Esercizi",
    "text": "Esercizi\n\n\n\n\n\n\nEsercizio\n\n\n\n\n\nIn questo esercizio, applicherai le tecniche di pulizia e preprocessing dei dati utilizzando il dataset SWLS. Il tuo compito è seguire i passaggi descritti per trasformare il dataset in una forma pronta per l’analisi.\nIstruzioni\n\n\nImporta i dati SWLS (Survey of Life Satisfaction Scale). Introduci almeno due dati mancanti nei dati e almeno un duplicato.\n\nControlla i dati: verifica la struttura e individua eventuali anomalie.\n\nPulisci i dati:\n\nRimuovi eventuali duplicati.\nGestisci i valori mancanti in modo appropriato.\nRinomina le variabili per una maggiore chiarezza.\nStandardizza alcune variabili per l’analisi.\n\n\n\nDocumenta le modifiche effettuate.\n\nEsporta il dataset pulito.\n\nConsegna\n\nSalva il tuo file Quarto con il nome swls_cleaning.qmd.\n\nUsa questo header YAML:\n---\ntitle: \"Assegnamento: Pulizia e Preprocessing dei Dati SWLS\"\nauthor: \"Nome Studente\"\ndate: \"2025-09-09\"\nformat: html\n---\n\nAssicurati che il codice sia commentato e spiegato chiaramente.\nEsporta il dataset pulito e allegalo alla consegna.\n\n\n\n\n\n\n\n\n\n\nInformazioni sull’ambiente di sviluppo\n\n\n\n\n\n\nsessionInfo()\n#&gt; R version 4.5.1 (2025-06-13)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.6.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] skimr_2.2.1           dataMeta_0.1.1        knitr_1.50           \n#&gt;  [4] purrr_1.1.0           pointblank_0.12.2     haven_2.5.5          \n#&gt;  [7] labelled_2.14.1       mice_3.18.0           pillar_1.11.0        \n#&gt; [10] tinytable_0.13.0      patchwork_1.3.2       ggdist_3.3.3         \n#&gt; [13] tidybayes_3.0.7       bayesplot_1.14.0      ggplot2_3.5.2        \n#&gt; [16] reliabilitydiag_0.2.1 priorsense_1.1.1      posterior_1.6.1      \n#&gt; [19] loo_2.8.0             rstan_2.32.7          StanHeaders_2.32.10  \n#&gt; [22] brms_2.22.0           Rcpp_1.1.0            sessioninfo_1.2.3    \n#&gt; [25] conflicted_1.2.0      janitor_2.2.1         matrixStats_1.5.0    \n#&gt; [28] modelr_0.1.11         tibble_3.3.0          dplyr_1.1.4          \n#&gt; [31] tidyr_1.3.1           rio_1.2.3             here_1.0.1           \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] RColorBrewer_1.1-3    tensorA_0.36.2.1      jsonlite_2.0.0       \n#&gt;   [4] shape_1.4.6.1         magrittr_2.0.3        TH.data_1.1-4        \n#&gt;   [7] estimability_1.5.1    jomo_2.7-6            farver_2.1.2         \n#&gt;  [10] nloptr_2.2.1          rmarkdown_2.29        ragg_1.5.0           \n#&gt;  [13] vctrs_0.6.5           memoise_2.0.1         minqa_1.2.8          \n#&gt;  [16] base64enc_0.1-3       htmltools_0.5.8.1     forcats_1.0.0        \n#&gt;  [19] distributional_0.5.0  curl_7.0.0            broom_1.0.9          \n#&gt;  [22] mitml_0.4-5           htmlwidgets_1.6.4     sandwich_3.1-1       \n#&gt;  [25] emmeans_1.11.2-8      zoo_1.8-14            lubridate_1.9.4      \n#&gt;  [28] cachem_1.1.0          lifecycle_1.0.4       iterators_1.0.14     \n#&gt;  [31] pkgconfig_2.0.3       Matrix_1.7-4          R6_2.6.1             \n#&gt;  [34] fastmap_1.2.0         rbibutils_2.3         snakecase_0.11.1     \n#&gt;  [37] digest_0.6.37         colorspace_2.1-1      rprojroot_2.1.1      \n#&gt;  [40] textshaping_1.0.3     timechange_0.3.0      abind_1.4-8          \n#&gt;  [43] compiler_4.5.1        withr_3.0.2           backports_1.5.0      \n#&gt;  [46] inline_0.3.21         QuickJSR_1.8.0        pkgbuild_1.4.8       \n#&gt;  [49] R.utils_2.13.0        pan_1.9               MASS_7.3-65          \n#&gt;  [52] tools_4.5.1           nnet_7.3-20           R.oo_1.27.1          \n#&gt;  [55] glue_1.8.0            nlme_3.1-168          grid_4.5.1           \n#&gt;  [58] checkmate_2.3.3       generics_0.1.4        gtable_0.3.6         \n#&gt;  [61] R.methodsS3_1.8.2     data.table_1.17.8     hms_1.1.3            \n#&gt;  [64] utf8_1.2.6            foreach_1.5.2         stringr_1.5.1        \n#&gt;  [67] splines_4.5.1         lattice_0.22-7        survival_3.8-3       \n#&gt;  [70] tidyselect_1.2.1      reformulas_0.4.1      arrayhelpers_1.1-0   \n#&gt;  [73] gridExtra_2.3         V8_7.0.0              stats4_4.5.1         \n#&gt;  [76] xfun_0.53             bridgesampling_1.1-2  stringi_1.8.7        \n#&gt;  [79] pacman_0.5.1          boot_1.3-32           evaluate_1.0.5       \n#&gt;  [82] codetools_0.2-20      cli_3.6.5             blastula_0.3.6       \n#&gt;  [85] RcppParallel_5.1.11-1 rpart_4.1.24          xtable_1.8-4         \n#&gt;  [88] systemfonts_1.2.3     Rdpack_2.6.4          repr_1.1.7           \n#&gt;  [91] coda_0.19-4.1         svUnit_1.0.8          parallel_4.5.1       \n#&gt;  [94] rstantools_2.5.0      Brobdingnag_1.2-9     lme4_1.1-37          \n#&gt;  [97] glmnet_4.1-10         mvtnorm_1.3-3         scales_1.4.0         \n#&gt; [100] rlang_1.1.6           multcomp_1.4-28",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#bibliografia",
    "href": "chapters/eda/02_data_cleaning.html#bibliografia",
    "title": "15  Flusso di lavoro per la pulizia dei dati",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nBuchanan, E. M., Crain, S. E., Cunningham, A. L., Johnson, H. R., Stash, H., Papadatou-Pastou, M., Isager, P. M., Carlsson, R., & Aczel, B. (2021). Getting started creating data dictionaries: How to create a shareable data set. Advances in Methods and Practices in Psychological Science, 4(1), 2515245920928007.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/03_exploring_qualitative_data.html",
    "href": "chapters/eda/03_exploring_qualitative_data.html",
    "title": "16  Esplorare i dati qualitativi",
    "section": "",
    "text": "Introduzione\nIn questo capitolo ci concentreremo sull’analisi esplorativa dei dati (Exploratory Data Analysis, EDA) applicata ai dati qualitativi e categoriali (Tukey et al., 1977). In psicologia la raccolta di dati qualitativi e categoriali è estremamente frequente: si pensi alle variabili sociodemografiche (genere, stato civile, livello di istruzione), alle risposte a item a scelta multipla nei questionari, alle diagnosi cliniche, o alle categorie di comportamento osservato in laboratorio. Prima di procedere a modelli complessi, è fondamentale esplorare questi dati per comprenderne la struttura, individuare pattern ricorrenti e riconoscere eventuali anomalie.\nL’EDA rappresenta il primo passo di ogni studio empirico. Con grafici e tabelle di frequenza possiamo farci un’idea immediata della distribuzione delle risposte, verificare se i dati sono equilibrati tra categorie, e osservare come certe caratteristiche si combinano. Ad esempio, in un’indagine clinica potremmo voler capire se i livelli di ansia riportati dai pazienti variano in base al genere o alla fascia di età; in uno studio sperimentale, se il successo in un compito dipende dalla condizione a cui il partecipante è stato assegnato.\nIn questo capitolo impareremo dunque a esplorare e visualizzare i dati qualitativi, passando da strumenti descrittivi come le tabelle di frequenza e le percentuali cumulative, fino a grafici più sofisticati (barplot, mosaic plot) che consentono di cogliere a colpo d’occhio le relazioni tra più variabili. L’obiettivo non è soltanto acquisire dimestichezza con le tecniche, ma anche sviluppare un atteggiamento critico, capace di riconoscere i limiti e le potenzialità delle rappresentazioni grafiche quando applicate a dati che riflettono la complessità dell’esperienza psicologica.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Esplorare i dati qualitativi</span>"
    ]
  },
  {
    "objectID": "chapters/eda/03_exploring_qualitative_data.html#introduzione",
    "href": "chapters/eda/03_exploring_qualitative_data.html#introduzione",
    "title": "16  Esplorare i dati qualitativi",
    "section": "",
    "text": "Panoramica del capitolo\n\nCalcolare proporzioni e organizzare i dati in tabelle di contingenza.\nCostruire grafici a barre per rappresentare dati qualitativi.\nCreare visualizzazioni per esplorare le relazioni tra due o più variabili qualitative.\n\n\n\n\n\n\n\nPrerequisiti\n\n\n\n\n\n\nLeggere il capitolo Exploring categorical data di Introduction to Modern Statistics (2e) di Mine Çetinkaya-Rundel e Johanna Hardin.\n\n\n\n\n\n\n\n\n\n\nPreparazione del Notebook\n\n\n\n\n\n\nhere::here(\"code\", \"_common.R\") |&gt; \n  source()\n\n# Load packages\nif (!requireNamespace(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyr, viridis, vcd, janitor)",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Esplorare i dati qualitativi</span>"
    ]
  },
  {
    "objectID": "chapters/eda/03_exploring_qualitative_data.html#il-dataset-penguins",
    "href": "chapters/eda/03_exploring_qualitative_data.html#il-dataset-penguins",
    "title": "16  Esplorare i dati qualitativi",
    "section": "\n16.1 Il dataset penguins\n",
    "text": "16.1 Il dataset penguins\n\nPer fornire esempi pratici, in questo capitolo utilizzeremo il dataset palmerpenguins, messo a disposizione da Allison Horst. I dati sono stati raccolti e resi disponibili da Dr. Kristen Gorman e dalla Palmer Station, parte del programma di ricerca ecologica a lungo termine Long Term Ecological Research Network. Il dataset contiene informazioni su 344 pinguini, appartenenti a 3 diverse specie, raccolte su 3 isole dell’arcipelago di Palmer, in Antartide. Per semplicità, i dati sono organizzati nel file penguins.csv.\nPossiamo caricare i dati grezzi dal file penguins.csv in un data frame con il seguente comando:\n\nd &lt;- rio::import(here::here(\"data\", \"penguins.csv\"))\n\nEsaminiamo i dati.\n\nglimpse(d)\n#&gt; Rows: 344\n#&gt; Columns: 8\n#&gt; $ species           &lt;chr&gt; \"Adelie\", \"Adelie\", \"Adelie\", \"Adelie\", \"Adelie\", \"A…\n#&gt; $ island            &lt;chr&gt; \"Torgersen\", \"Torgersen\", \"Torgersen\", \"Torgersen\", …\n#&gt; $ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n#&gt; $ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n#&gt; $ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n#&gt; $ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n#&gt; $ sex               &lt;chr&gt; \"male\", \"female\", \"female\", NA, \"female\", \"male\", \"f…\n#&gt; $ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\nPer semplicità, rimuoviamo le righe con valori mancanti con la seguente istruzione:\n\ndf &lt;- d |&gt;\n  drop_na()",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Esplorare i dati qualitativi</span>"
    ]
  },
  {
    "objectID": "chapters/eda/03_exploring_qualitative_data.html#tabelle-di-contingenza",
    "href": "chapters/eda/03_exploring_qualitative_data.html#tabelle-di-contingenza",
    "title": "16  Esplorare i dati qualitativi",
    "section": "\n16.2 Tabelle di contingenza",
    "text": "16.2 Tabelle di contingenza\nUna tabella di contingenza è uno strumento che riassume i dati di due variabili categoriali, cioè variabili qualitative che assumono valori in un numero finito di categorie. Ogni cella della tabella indica quante osservazioni ricadono in una specifica combinazione di categorie delle due variabili.\nPer esempio, supponiamo di avere due variabili del dataset df:\n\n\nisland: l’isola di provenienza dei pinguini,\n\nspecies: la specie di appartenenza (Adelie, Chinstrap, Gentoo).\n\nCon la funzione tabyl() del pacchetto janitor possiamo costruire una tabella che mostra quante osservazioni appartengono a ciascuna combinazione di isola e specie:\n\ndf |&gt; \n  tabyl(island, species) |&gt; \n  adorn_totals(c(\"row\", \"col\")) \n#&gt;     island Adelie Chinstrap Gentoo Total\n#&gt;     Biscoe     44         0    119   163\n#&gt;      Dream     55        68      0   123\n#&gt;  Torgersen     47         0      0    47\n#&gt;      Total    146        68    119   333\n\nLa tabella risultante riporta la distribuzione di tre specie di pinguini (Adelie, Chinstrap, Gentoo) rispetto a tre isole (Biscoe, Dream, Torgersen):\n\n\nIsola Biscoe: sono presenti 44 pinguini Adelie e 119 Gentoo. Nessun esemplare Chinstrap.\n\nIsola Dream: ospita 55 pinguini Adelie e 68 Chinstrap, ma nessun Gentoo.\n\nIsola Torgersen: conta solo 47 pinguini Adelie, senza esemplari delle altre specie.\n\nOsservazioni:\n\nLa specie Adelie è l’unica distribuita su tutte e tre le isole (44 su Biscoe, 55 su Dream, 47 su Torgersen).\nLa specie Chinstrap compare esclusivamente su Dream (68 esemplari).\nLa specie Gentoo si trova soltanto su Biscoe (119 esemplari).\n\nQuesta tabella evidenzia che la distribuzione delle specie non è uniforme: alcune sono presenti solo in determinate isole, mentre altre (come gli Adelie) sono più diffuse. In termini di analisi esplorativa, le tabelle di contingenza permettono quindi di individuare pattern e differenze tra categorie, fornendo una prima descrizione della relazione tra due variabili qualitative.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Esplorare i dati qualitativi</span>"
    ]
  },
  {
    "objectID": "chapters/eda/03_exploring_qualitative_data.html#proporzioni-di-riga-e-di-colonna",
    "href": "chapters/eda/03_exploring_qualitative_data.html#proporzioni-di-riga-e-di-colonna",
    "title": "16  Esplorare i dati qualitativi",
    "section": "\n16.3 Proporzioni di riga e di colonna",
    "text": "16.3 Proporzioni di riga e di colonna\nFinora ci siamo concentrati sui conteggi assoluti. Tuttavia, in molti casi è più informativo osservare le proporzioni relative.\n\nLe proporzioni di riga descrivono come si distribuiscono le categorie di una variabile all’interno di ciascun gruppo dell’altra.\nLe proporzioni di colonna, al contrario, mostrano la distribuzione di una variabile rispetto alle categorie dell’altra.\n\nQuesti calcoli si ottengono facilmente a partire dalla tabella di contingenza.\n\n16.3.1 Proporzioni di riga (specie per isola)\n\ndf %&gt;%\n  tabyl(island, species) %&gt;%   # Tabella di contingenza\n  adorn_percentages(\"row\") %&gt;% # Proporzioni rispetto a ciascuna isola\n  adorn_totals(\"col\") %&gt;%      # Totali di riga\n  adorn_pct_formatting(digits = 2)\n#&gt;     island  Adelie Chinstrap Gentoo   Total\n#&gt;     Biscoe  26.99%     0.00% 73.01% 100.00%\n#&gt;      Dream  44.72%    55.28%  0.00% 100.00%\n#&gt;  Torgersen 100.00%     0.00%  0.00% 100.00%\n\nQuesto output mostra, per ogni isola, la percentuale di pinguini appartenenti a ciascuna specie.\n\n16.3.2 Proporzioni di colonna (isole per specie)\n\ndf |&gt; \n  tabyl(island, species) |&gt; \n  adorn_percentages(\"col\") |&gt;  # Proporzioni rispetto a ciascuna specie\n  adorn_totals(\"row\") |&gt; \n  adorn_pct_formatting(digits = 2)\n#&gt;     island  Adelie Chinstrap  Gentoo\n#&gt;     Biscoe  30.14%     0.00% 100.00%\n#&gt;      Dream  37.67%   100.00%   0.00%\n#&gt;  Torgersen  32.19%     0.00%   0.00%\n#&gt;      Total 100.00%   100.00% 100.00%\n\nQui vediamo, per ciascuna specie, su quali isole si distribuiscono i pinguini e con quale proporzione.\nIn sintesi, le proporzioni di riga e colonna forniscono un dettaglio numerico che aiuta a interpretare meglio le relazioni tra le variabili categoriali.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Esplorare i dati qualitativi</span>"
    ]
  },
  {
    "objectID": "chapters/eda/03_exploring_qualitative_data.html#grafici-a-barre",
    "href": "chapters/eda/03_exploring_qualitative_data.html#grafici-a-barre",
    "title": "16  Esplorare i dati qualitativi",
    "section": "\n16.4 Grafici a barre",
    "text": "16.4 Grafici a barre\nI grafici a barre sono uno degli strumenti più utilizzati per rappresentare visivamente i dati categoriali. Essi consentono di confrontare le frequenze delle categorie in modo immediato, mostrando chiaramente quali valori sono più comuni o più rari nel campione.\n\n16.4.1 Grafico a barre con una variabile\nNel caso più semplice, un grafico a barre rappresenta una sola variabile categoriale. Le categorie sono riportate lungo un asse (di solito l’asse orizzontale) e la lunghezza o altezza delle barre è proporzionale al numero di osservazioni per ciascuna categoria.\nAd esempio, per i dati sui pinguini possiamo visualizzare il numero totale di esemplari osservati in ciascuna isola:\n\nggplot(df, aes(x = island)) +\n  geom_bar() +\n  xlab(\"Isola\") +\n  ylab(\"Numero di pinguini\") \n\n\n\n\n\n\n\nAllo stesso modo, possiamo costruire un grafico a barre che mostra la distribuzione delle specie:\n\nggplot(df, aes(x = species)) +\n  geom_bar() +\n  xlab(\"Specie\") +\n  ylab(\"Numero di pinguini\")\n\n\n\n\n\n\n\nQuesti grafici permettono di confrontare rapidamente le frequenze delle categorie, mettendo in evidenza quali specie o quali isole sono più rappresentate.\n\n16.4.2 Grafico a barre con due variabili\nUn grafico a barre può essere esteso per visualizzare simultaneamente due variabili categoriali. In questo caso, una variabile viene posta sull’asse delle ascisse, mentre la seconda è distinta tramite colori diversi o barre impilate.\nAd esempio, possiamo osservare come le diverse specie di pinguini si distribuiscono sulle isole:\n\nggplot(df, aes(x = island, fill = species)) +\n  geom_bar(position = \"stack\") +\n  xlab(\"Isola\") +\n  ylab(\"Numero di pinguini\") +\n  labs(fill = \"Specie\") \n\n\n\n\n\n\n\nOppure, invertendo i ruoli delle due variabili, possiamo rappresentare le specie sull’asse delle ascisse e distinguere le isole tramite colori:\n\nggplot(df, aes(x = species, fill = island)) +\n  geom_bar(position = \"stack\") +\n  xlab(\"Specie\") +\n  ylab(\"Numero di pinguini\") +\n  labs(fill = \"Isola\")\n\n\n\n\n\n\n\nQuesti grafici permettono di esplorare visivamente l’associazione tra due variabili qualitative.\n\n16.4.3 Frequenze relative\nOltre alle frequenze assolute, è spesso utile rappresentare le frequenze relative (cioè le proporzioni). Questo approccio elimina l’effetto del numero totale di osservazioni, rendendo più facile confrontare le distribuzioni tra gruppi di dimensioni diverse.\nAd esempio, il grafico seguente mostra la composizione relativa delle specie per ogni isola:\n\nggplot(df, aes(x = island, fill = species)) +\n  geom_bar(position = \"fill\") +\n  xlab(\"Isola\") +\n  ylab(\"Proporzione\") +\n  labs(fill = \"Specie\")\n\n\n\n\n\n\n\nQui ogni barra è normalizzata a 1: l’altezza di ciascun segmento rappresenta la proporzione di una specie all’interno dell’isola. In questo modo è più facile capire quale specie prevale in ciascun contesto, indipendentemente dal numero complessivo di pinguini osservati.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Esplorare i dati qualitativi</span>"
    ]
  },
  {
    "objectID": "chapters/eda/03_exploring_qualitative_data.html#mosaic-plot",
    "href": "chapters/eda/03_exploring_qualitative_data.html#mosaic-plot",
    "title": "16  Esplorare i dati qualitativi",
    "section": "\n16.5 Mosaic plot",
    "text": "16.5 Mosaic plot\nIl Mosaic plot è una tecnica di visualizzazione particolarmente adatta per rappresentare le tabelle di contingenza. A differenza di un semplice grafico a barre impilate, questo tipo di grafico mostra contemporaneamente:\n\nla distribuzione interna delle categorie di una variabile,\nla dimensione relativa complessiva dei gruppi della variabile principale.\n\nIn pratica, non ci dice solo come le categorie si suddividono all’interno dei gruppi, ma anche quanto grandi sono i gruppi stessi.\n\nmosaic(\n  ~ species + island, \n  data = df, \n  main = \"Mosaic Plot of Species and Island\",\n  shade = TRUE\n)\n\n\n\n\n\n\n\nCome leggere un mosaic plot.\n\n\nDimensioni dei rettangoli\n\nLa larghezza rappresenta la dimensione relativa dei gruppi della variabile principale (island).\nL’altezza indica la proporzione delle categorie della variabile secondaria (species) all’interno di ciascun gruppo.\n\n\n\nColori (opzione shade = TRUE)\n\nI colori evidenziano deviazioni dalle frequenze attese in caso di indipendenza statistica.\nUn colore scuro segnala che in quel gruppo la frequenza osservata è molto diversa da quella attesa sotto indipendenza.\n\n\n\nInterpretazione pratica\n\nUn rettangolo largo e alto segnala una categoria numerosa in un gruppo consistente.\nUn rettangolo stretto o sottile indica una categoria rara o assente in quel gruppo.\n\n\n\nIn questo esempio, vediamo chiaramente che:\n\ngli Adelie sono presenti in tutte le isole,\ni Chinstrap compaiono solo a Dream,\ni Gentoo solo a Biscoe.\n\nIl Mosaic plot è quindi utile per cogliere schemi di associazione tra variabili categoriali e valutare rapidamente quali combinazioni sono predominanti o assenti.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Esplorare i dati qualitativi</span>"
    ]
  },
  {
    "objectID": "chapters/eda/03_exploring_qualitative_data.html#confronto-tra-gruppi",
    "href": "chapters/eda/03_exploring_qualitative_data.html#confronto-tra-gruppi",
    "title": "16  Esplorare i dati qualitativi",
    "section": "\n16.6 Confronto tra gruppi",
    "text": "16.6 Confronto tra gruppi\nUn aspetto centrale dell’analisi esplorativa consiste nel confrontare gruppi diversi. Questo ci permette di mettere in evidenza differenze e somiglianze, osservare variazioni e individuare tendenze. Il confronto può riguardare:\n\n\nvariabili categoriali tra loro (ad esempio distribuzione di genere nelle diverse specie), oppure\n\nvariabili numeriche rispetto a categorie (ad esempio come varia il peso corporeo tra specie e generi).\n\n\n16.6.1 Confronto tra variabili categoriali\nPer confrontare due variabili qualitative, possiamo utilizzare un grafico a barre con suddivisione per gruppi. Ad esempio, vediamo come si distribuisce il genere dei pinguini (maschio/femmina) all’interno delle tre specie:\n\nggplot(df, aes(x = species, fill = sex)) +\n  geom_bar(position = \"dodge\") +\n  xlab(\"Specie\") +\n  ylab(\"Conteggio\")\n\n\n\n\n\n\n\nIn questo grafico le barre affiancate permettono di confrontare facilmente, per ciascuna specie, il numero di maschi e femmine.\n\n16.6.2 Confronto tra variabili numeriche e categorie\nSpesso è ancora più interessante osservare come una variabile numerica varia tra gruppi. Questo approccio ci consente di capire se gruppi diversi tendono ad avere valori simili o differenti. Prendiamo come esempio la variabile body_mass_g (peso corporeo in grammi), e confrontiamola in base a specie e genere.\n\nggplot(df, aes(x = species, y = body_mass_g, fill = sex)) +\n  geom_violin(\n    position = position_dodge(width = 0.9), alpha = 0.5\n  ) +\n  geom_boxplot(\n    position = position_dodge(width = 0.9), width = 0.2, alpha = 0.8\n  ) +\n  xlab(\"Specie\") +\n  ylab(\"Massa corporea (g)\") +\n  labs(fill = \"Genere\")\n\n\n\n\n\n\n\nQuesto grafico combina due livelli di informazione:\n\n\nGrafico a violino (aree colorate)\n\nMostra l’intera distribuzione dei pesi per ciascun gruppo (specie × genere).\nPiù l’area è larga in un punto, più pinguini hanno un peso vicino a quel valore.\n\n\n\nBoxplot (linee centrali)\n\nRiassume visivamente i dati, mostrando mediana, quartili e variabilità.\nAiuta a confrontare rapidamente i livelli tipici e la dispersione tra gruppi.\n\n\n\nDa questo tipo di grafico possiamo osservare:\n\n\nDifferenze tra generi all’interno di una specie: ad esempio, se i maschi tendono a essere più pesanti delle femmine.\n\nDifferenze tra specie: quali specie hanno in generale pinguini più pesanti o più leggeri.\n\nSovrapposizioni: se i pesi di maschi e femmine si distinguono nettamente o se i due gruppi hanno valori simili.\n\nIn questo modo, possiamo individuare sia differenze sistematiche sia aree di variabilità comune tra i gruppi.\n\n\n\n\n\n\nProblemi\n\n\n\n\n\nIn questo esercizio analizzerai i dati qualitativi raccolti mediante la scala SWLS, concentrandoti su due variabili categoriali:\n\n\nGenere (gender): maschio / femmina.\n\nTipo di scuola superiore (school_type): liceo classico o scientifico vs tutto il resto.\n\nDovrai creare tabelle di contingenza e rappresentazioni grafiche per esplorare le relazioni tra queste variabili.\nImportazione dei dati\nImporta i dati da un file CSV e visualizza la loro struttura.\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(here)\n\n# Importa i dati\nswls_data &lt;- read_csv(here(\"data\", \"swls_students.csv\"))\n\n# Esamina i dati\nglimpse(swls_data)\nTabelle di Contingenza\n\nCrea una tabella di contingenza tra gender e school_type.\nCalcola le proporzioni di riga e colonna.\n\n# Tabella di contingenza\nswls_data |&gt; \n  tabyl(gender, school_type) |&gt; \n  adorn_totals(c(\"row\", \"col\"))\nCalcola ora le proporzioni relative.\n# Proporzioni di riga\nswls_data |&gt; \n  tabyl(gender, school_type) |&gt; \n  adorn_percentages(\"row\") |&gt; \n  adorn_pct_formatting(digits = 2)\n# Proporzioni di colonna\nswls_data |&gt; \n  tabyl(gender, school_type) |&gt; \n  adorn_percentages(\"col\") |&gt; \n  adorn_pct_formatting(digits = 2)\nVisualizzazione Grafica\n\nCrea un grafico a barre per visualizzare il numero di studenti per tipo di scuola.\nCrea un grafico a barre per la distribuzione del genere per tipo di scuola.\n\nggplot(swls_data, aes(x = school_type)) +\n  geom_bar() +\n  ggtitle(\"Numero di studenti per tipo di scuola\") +\n  xlab(\"Tipo di scuola\") +\n  ylab(\"Numero di studenti\")\nggplot(swls_data, aes(x = school_type, fill = gender)) +\n  geom_bar(position = \"dodge\") +\n  ggtitle(\"Distribuzione del genere per tipo di scuola\") +\n  xlab(\"Tipo di scuola\") +\n  ylab(\"Numero di studenti\") +\n  labs(fill = \"Genere\")\nDomande per la riflessione\n\nQuale tipo di scuola ha il maggior numero di studenti?\nCi sono differenze nella distribuzione del genere tra i tipi di scuola?\n\nConsegna\n\nCompila il file .qmd con il tuo codice e commenti.\nEsporta il documento in formato HTML o PDF.\nCarica il file su Moodle.\n\n\n\n\n\n\n\n\n\n\nInformazioni sull’ambiente di sviluppo\n\n\n\n\n\n\nsessionInfo()\n#&gt; R version 4.5.1 (2025-06-13)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.6.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] grid      stats     graphics  grDevices utils     datasets  methods  \n#&gt; [8] base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] vcd_1.4-13            viridis_0.6.5         viridisLite_0.4.2    \n#&gt;  [4] pillar_1.11.0         tinytable_0.13.0      patchwork_1.3.2      \n#&gt;  [7] ggdist_3.3.3          tidybayes_3.0.7       bayesplot_1.14.0     \n#&gt; [10] ggplot2_3.5.2         reliabilitydiag_0.2.1 priorsense_1.1.1     \n#&gt; [13] posterior_1.6.1       loo_2.8.0             rstan_2.32.7         \n#&gt; [16] StanHeaders_2.32.10   brms_2.22.0           Rcpp_1.1.0           \n#&gt; [19] sessioninfo_1.2.3     conflicted_1.2.0      janitor_2.2.1        \n#&gt; [22] matrixStats_1.5.0     modelr_0.1.11         tibble_3.3.0         \n#&gt; [25] dplyr_1.1.4           tidyr_1.3.1           rio_1.2.3            \n#&gt; [28] here_1.0.1           \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;  [1] gridExtra_2.3         inline_0.3.21         sandwich_3.1-1       \n#&gt;  [4] rlang_1.1.6           magrittr_2.0.3        multcomp_1.4-28      \n#&gt;  [7] snakecase_0.11.1      compiler_4.5.1        systemfonts_1.2.3    \n#&gt; [10] vctrs_0.6.5           stringr_1.5.1         pkgconfig_2.0.3      \n#&gt; [13] arrayhelpers_1.1-0    fastmap_1.2.0         backports_1.5.0      \n#&gt; [16] labeling_0.4.3        rmarkdown_2.29        ragg_1.5.0           \n#&gt; [19] purrr_1.1.0           xfun_0.53             cachem_1.1.0         \n#&gt; [22] jsonlite_2.0.0        broom_1.0.9           parallel_4.5.1       \n#&gt; [25] R6_2.6.1              stringi_1.8.7         RColorBrewer_1.1-3   \n#&gt; [28] lmtest_0.9-40         lubridate_1.9.4       estimability_1.5.1   \n#&gt; [31] knitr_1.50            zoo_1.8-14            R.utils_2.13.0       \n#&gt; [34] pacman_0.5.1          Matrix_1.7-4          splines_4.5.1        \n#&gt; [37] timechange_0.3.0      tidyselect_1.2.1      abind_1.4-8          \n#&gt; [40] yaml_2.3.10           codetools_0.2-20      curl_7.0.0           \n#&gt; [43] pkgbuild_1.4.8        lattice_0.22-7        withr_3.0.2          \n#&gt; [46] bridgesampling_1.1-2  coda_0.19-4.1         evaluate_1.0.5       \n#&gt; [49] survival_3.8-3        RcppParallel_5.1.11-1 tensorA_0.36.2.1     \n#&gt; [52] checkmate_2.3.3       stats4_4.5.1          distributional_0.5.0 \n#&gt; [55] generics_0.1.4        rprojroot_2.1.1       rstantools_2.5.0     \n#&gt; [58] scales_1.4.0          xtable_1.8-4          glue_1.8.0           \n#&gt; [61] emmeans_1.11.2-8      tools_4.5.1           data.table_1.17.8    \n#&gt; [64] mvtnorm_1.3-3         QuickJSR_1.8.0        colorspace_2.1-1     \n#&gt; [67] nlme_3.1-168          cli_3.6.5             textshaping_1.0.3    \n#&gt; [70] svUnit_1.0.8          Brobdingnag_1.2-9     V8_7.0.0             \n#&gt; [73] gtable_0.3.6          R.methodsS3_1.8.2     digest_0.6.37        \n#&gt; [76] TH.data_1.1-4         htmlwidgets_1.6.4     farver_2.1.2         \n#&gt; [79] R.oo_1.27.1           memoise_2.0.1         htmltools_0.5.8.1    \n#&gt; [82] lifecycle_1.0.4       MASS_7.3-65",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Esplorare i dati qualitativi</span>"
    ]
  },
  {
    "objectID": "chapters/eda/03_exploring_qualitative_data.html#bibliografia",
    "href": "chapters/eda/03_exploring_qualitative_data.html#bibliografia",
    "title": "16  Esplorare i dati qualitativi",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nTukey, J. W. et al. (1977). Exploratory data analysis (Vol. 2). Springer.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Esplorare i dati qualitativi</span>"
    ]
  },
  {
    "objectID": "chapters/eda/04_exploring_numeric_data.html",
    "href": "chapters/eda/04_exploring_numeric_data.html",
    "title": "17  Esplorare i dati numerici",
    "section": "",
    "text": "Introduzione\nIn questo capitolo ci concentreremo sull’analisi dei dati numerici. In particolare, esamineremo le distribuzioni di frequenza e i quantili, insieme alle tecniche di visualizzazione più comuni, come l’istogramma, l’istogramma smussato e il box-plot. Tratteremo sia gli aspetti computazionali che quelli interpretativi di queste misure, fornendo strumenti utili non solo per una comprensione personale, ma anche per la comunicazione efficace dei risultati, in particolare con chi utilizza questi dati per prendere decisioni pratiche nel mondo reale.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Esplorare i dati numerici</span>"
    ]
  },
  {
    "objectID": "chapters/eda/04_exploring_numeric_data.html#introduzione",
    "href": "chapters/eda/04_exploring_numeric_data.html#introduzione",
    "title": "17  Esplorare i dati numerici",
    "section": "",
    "text": "Panoramica del capitolo\n\ncostruire e interpretare distribuzioni di frequenza e cumulative;\n\ncomprendere e confrontare istogrammi e stime di densità kernel;\n\nutilizzare boxplot e violin plot per individuare differenze tra gruppi;\n\nriconoscere la forma di una distribuzione e i suoi indici di posizione;\n\ncomunicare i risultati con grafici chiari ed efficaci.\n\n\n\n\n\n\n\nPrerequisiti\n\n\n\n\n\n\nLeggere l’Appendice ?sec-apx-sums prima di procedere con la lettura di questo capitolo.\nLeggere il capitolo Exploring numerical data di Introduction to Modern Statistics (2e) di Mine Çetinkaya-Rundel e Johanna Hardin.\n\n\n\n\n\n\n\n\n\n\nPreparazione del Notebook\n\n\n\n\n\n\nhere::here(\"code\", \"_common.R\") |&gt; \n  source()\n\n# Load packages\nif (!requireNamespace(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(ggbeeswarm, dslabs, gridExtra, ggpubr, cowplot, viridis)",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Esplorare i dati numerici</span>"
    ]
  },
  {
    "objectID": "chapters/eda/04_exploring_numeric_data.html#le-aspettative-negative-nella-depressione",
    "href": "chapters/eda/04_exploring_numeric_data.html#le-aspettative-negative-nella-depressione",
    "title": "17  Esplorare i dati numerici",
    "section": "\n17.1 Le aspettative negative nella depressione",
    "text": "17.1 Le aspettative negative nella depressione\nConsideriamo i dati relativi alle aspettative negative, individuate come un meccanismo chiave nel mantenimento della depressione (Zetsche et al., 2019). Supponiamo di voler analizzare la distribuzione di una singola variabile quantitativa.\nImportiamo i dati:\n\ndf &lt;- rio::import(here::here(\"data\", \"data.mood.csv\"))\n\n\n17.1.1 Data Wrangling\nPer questo esercizio, ci concentreremo sulle variabili esm_id (il codice del soggetto), group (il gruppo) e bdi (il valore BDI-II).\n\ndf &lt;- df |&gt; \n  dplyr::select(\"esm_id\", \"group\", \"bdi\")\ndf |&gt; \n  head()\n#&gt;   esm_id group bdi\n#&gt; 1     10   mdd  25\n#&gt; 2     10   mdd  25\n#&gt; 3     10   mdd  25\n#&gt; 4     10   mdd  25\n#&gt; 5     10   mdd  25\n#&gt; 6     10   mdd  25\n\nSe elenchiamo le modalità presenti in group utilizzando il metodo unique(), scopriamo che corrispondono a mdd (pazienti) e ctl (controlli sani).\n\ndf$group |&gt; \n  unique()\n#&gt; [1] \"mdd\" \"ctl\"\n\nRimuoviamo i duplicati per ottenere un unico valore BDI-II per ogni soggetto:\n\ndf &lt;- df[!duplicated(df), ]\n\nVerifichiamo di avere ottenuto il risultato desiderato.\n\ndim(df)\n#&gt; [1] 67  3\n\n\nhead(df)\n#&gt;    esm_id group bdi\n#&gt; 1      10   mdd  25\n#&gt; 15      9   mdd  30\n#&gt; 30      6   mdd  26\n#&gt; 46      7   mdd  35\n#&gt; 65     12   mdd  44\n#&gt; 83     16   mdd  30\n\nSi noti che il nuovo DataFrame (con 67 righe) conserva il “nome” delle righe (ovvero, l’indice di riga) del DataFrame originario (con 1188 righe). Per esempio, il secondo soggetto (con codice identificativo 9) si trova sulla seconda riga del DataFrame, ma il suo indice di riga è 15. Questo non ha nessuna conseguenza perché non useremo l’indice di riga nelle analisi seguenti.\nEliminiamo eventuali valori mancanti:\n\ndf &lt;- df[!is.na(df$bdi), ]\n\nOtteniamo così il DataFrame finale per gli scopi presenti (66 righe e 3 colonne):\n\ndim(df)\n#&gt; [1] 66  3\n\n\n17.1.2 Anteprima dei dati\nPrima di approfondire l’analisi, è fondamentale esaminare una anteprima dei dati per comprenderne struttura, formati e potenziali anomalie.\nLa funzione glimpse() fornisce una panoramica compatta del dataset: numero di righe/colonne, tipo di variabili (es. chr, num, dbl) ed esempi di valori. Utile per identificare rapidamente formati errati o colonne non attese:\n\nglimpse(df)  \n#&gt; Rows: 66\n#&gt; Columns: 3\n#&gt; $ esm_id &lt;int&gt; 10, 9, 6, 7, 12, 16, 21, 18, 20, 22, 23, 25, 24, 26, 41, 31, 27…\n#&gt; $ group  &lt;chr&gt; \"mdd\", \"mdd\", \"mdd\", \"mdd\", \"mdd\", \"mdd\", \"mdd\", \"mdd\", \"mdd\", …\n#&gt; $ bdi    &lt;int&gt; 25, 30, 26, 35, 44, 30, 22, 33, 43, 43, 24, 39, 19, 3, 0, 25, 0…\n\nLa funzione summary() genera statistiche descrittive per ogni colonna:\n\nper variabili numeriche: media, mediana, quartili, min/max;\nper variabili categoriche: frequenza dei livelli;\nsegnala valori mancanti (NA), aiutando a valutare la qualità dei dati.\n\n\nsummary(df)  \n#&gt;      esm_id         group                bdi       \n#&gt;  Min.   :  6.0   Length:66          Min.   : 0.00  \n#&gt;  1st Qu.: 30.2   Class :character   1st Qu.: 0.25  \n#&gt;  Median : 46.5   Mode  :character   Median : 6.00  \n#&gt;  Mean   : 51.6                      Mean   :14.94  \n#&gt;  3rd Qu.: 76.8                      3rd Qu.:29.50  \n#&gt;  Max.   :104.0                      Max.   :44.00\n\nI comandi head() e tail() ci permettono di visualizzare le prime o le ultime righe di un dataset:\n\nhead(df)\n#&gt;    esm_id group bdi\n#&gt; 1      10   mdd  25\n#&gt; 15      9   mdd  30\n#&gt; 30      6   mdd  26\n#&gt; 46      7   mdd  35\n#&gt; 65     12   mdd  44\n#&gt; 83     16   mdd  30\n\n\ntail(df)\n#&gt;      esm_id group bdi\n#&gt; 1087    101   ctl   9\n#&gt; 1105     99   ctl   0\n#&gt; 1121    100   ctl   2\n#&gt; 1133    104   ctl   0\n#&gt; 1152    103   ctl   0\n#&gt; 1171    102   ctl   1\n\n\n17.1.3 Conversione da char a factor\n\nIn R, i tipi di dato character e factor rappresentano informazioni testuali, ma hanno utilizzi distinti:\n\n\ncharacter: è una semplice stringa di testo;\n\nfactor: è una variabile categoriale, ideale per rappresentare dati con un numero finito di categorie (livelli). I dati in formato factor sono utili per analisi statistiche, poiché trattano i valori come categorie discrete.\n\nNel seguente esempio, convertiamo una variabile group da character a factor, in modo da poterla utilizzare come variabile categoriale:\n\ndf$group &lt;- as.factor(df$group)  # Converte 'group' in un factor\n\nSuccessivamente, il comando summary() fornisce un riepilogo della variabile categoriale, mostrando il conteggio dei valori per ciascun livello:\n\nsummary(df$group)\n#&gt; ctl mdd \n#&gt;  36  30",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Esplorare i dati numerici</span>"
    ]
  },
  {
    "objectID": "chapters/eda/04_exploring_numeric_data.html#distribuzioni-di-frequenza",
    "href": "chapters/eda/04_exploring_numeric_data.html#distribuzioni-di-frequenza",
    "title": "17  Esplorare i dati numerici",
    "section": "\n17.2 Distribuzioni di frequenza",
    "text": "17.2 Distribuzioni di frequenza\nLe distribuzioni di frequenza sono strumenti essenziali per visualizzare e comprendere la variabilità di una variabile. In questo capitolo verrà illustrato come costruire una distribuzione di frequenza e, successivamente, come generare in R una distribuzione cumulativa empirica, un istogramma, un Kernel Density Plot e un boxplot.\nA titolo esemplificativo, consideriamo i punteggi del BDI-II. Iniziamo ordinando i dati in ordine crescente:\n\ndf$bdi |&gt; sort()\n#&gt;  [1]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  1  1  1  1  1  1  1\n#&gt; [26]  2  2  2  2  3  3  3  5  7  9 12 19 22 22 24 25 25 26 26 26 27 27 28 28 30\n#&gt; [51] 30 30 31 31 33 33 34 35 35 35 36 39 41 43 43 44\n\nUna distribuzione di frequenza evidenzia quante volte i valori di una variabile ricorrono in determinati intervalli. Ad esempio, per i punteggi del BDI-II è consuetudine raggruppare i dati nelle seguenti classi:\n\n0–13: depressione minima;\n14–19: depressione lieve-moderata;\n20–28: depressione moderata-severa;\n29–63: depressione severa.\n\nDefinendo ciascuna classe, indicata con \\(\\Delta_i\\), come un intervallo \\([a_i, b_i)\\) o \\((a_i, b_i]\\), possiamo calcolare le seguenti misure:\n\n\nFrequenza assoluta (\\(n_i\\)): numero di osservazioni in \\(\\Delta_i\\). La somma delle frequenze assolute corrisponde al totale delle osservazioni, \\(n\\).\n\nFrequenza relativa (\\(f_i\\)): proporzione di osservazioni in \\(\\Delta_i\\), calcolata come \\(f_i = n_i/n\\); la somma delle frequenze relative è pari a 1.\n\nFrequenza cumulata (\\(N_i\\)): somma delle frequenze assolute fino alla classe \\(i\\), ovvero \\(N_i = \\sum_{j=1}^i n_j\\).\n\nFrequenza cumulata relativa (\\(F_i\\)): somma delle frequenze relative fino alla classe \\(i\\), data da \\(F_i = \\sum_{j=1}^i f_j\\).\n\nQueste misure consentono di sintetizzare la distribuzione dei punteggi, facilitando l’interpretazione delle caratteristiche del campione.\n\n17.2.1 Frequenze assolute e relative\nPer analizzare la distribuzione dei punteggi BDI-II nel dataset di Zetsche et al. (2019), è utile creare una variabile categoriale che classifichi ogni osservazione in una delle quattro classi di gravità della depressione. A tal fine, utilizziamo la funzione cut(), che permette di suddividere il vettore dei punteggi (bdi) in intervalli definiti.\nNel comando seguente, il parametro breaks specifica i limiti degli intervalli, mentre include.lowest = TRUE garantisce che il valore minimo sia incluso nel primo intervallo:\n\n# Creazione della variabile categoriale per i livelli di depressione\ndf &lt;- df %&gt;% \n  mutate(\n    bdi_class = cut(\n      bdi, \n      breaks = c(0, 13.5, 19.5, 28.5, 63),\n      include.lowest = TRUE\n    )\n  )\n\nI punteggi vengono suddivisi nelle seguenti classi:\n\n0–13: depressione minima;\n14–19: depressione lieve-moderata;\n20–28: depressione moderata-;\n29–63: depressione severa.\n\nUna volta creata la variabile bdi_class, possiamo calcolare le frequenze assolute e relative.\n\n17.2.1.1 Frequenze assolute\nUtilizzando la funzione table(), si ottiene il numero di osservazioni in ciascuna classe:\n\ntable(df$bdi_class)\n#&gt; \n#&gt;    [0,13.5] (13.5,19.5] (19.5,28.5]   (28.5,63] \n#&gt;          36           1          12          17\n\n\n17.2.1.2 Frequenze relative\nCon prop.table() è possibile determinare la proporzione di osservazioni per ogni classe:\n\nprop.table(table(df$bdi_class))\n#&gt; \n#&gt;    [0,13.5] (13.5,19.5] (19.5,28.5]   (28.5,63] \n#&gt;      0.5455      0.0152      0.1818      0.2576\n\n\n17.2.2 Distribuzioni congiunte\nLe distribuzioni congiunte di frequenze permettono di analizzare la relazione tra due variabili, considerando tutte le possibili combinazioni dei loro valori. Ad esempio, se analizziamo le variabili bdi_class e group, la tabella congiunta mostrerà la frequenza (assoluta o relativa) per ogni coppia di valori.\nPer ottenere la distribuzione congiunta relativa, utilizziamo:\n\nprop.table(table(df$bdi_class, df$group))\n#&gt;              \n#&gt;                  ctl    mdd\n#&gt;   [0,13.5]    0.5455 0.0000\n#&gt;   (13.5,19.5] 0.0000 0.0152\n#&gt;   (19.5,28.5] 0.0000 0.1818\n#&gt;   (28.5,63]   0.0000 0.2576\n\nIn questo modo, possiamo esaminare come le classi di punteggi BDI-II si distribuiscono all’interno dei diversi gruppi, con la somma complessiva delle frequenze relative pari a 1.\n\n17.2.3 La distribuzione cumulativa empirica\nLa distribuzione cumulativa empirica (eCDF, empirical Cumulative Distribution Function) è un modo utile per rappresentare la distribuzione di dati numerici. Questa funzione indica la proporzione di dati che sono inferiori o uguali a un certo valore \\(a\\), per tutti i possibili valori di \\(a\\). Matematicamente, la eCDF è definita come:\n\\[\nF(a) = \\text{Proporzione dei dati con valore} \\leq a.\n\\]\nIn altre parole, la eCDF ci dice quale frazione dei dati osservati è minore o uguale a un determinato valore \\(a\\). Questo è particolarmente utile per comprendere come i dati sono distribuiti e per identificare pattern o caratteristiche specifiche della distribuzione, come la presenza di bimodalità (cioè, due picchi distinti nella distribuzione).\n\n17.2.3.1 Esempio con i dati di Zetsche et al. (2019)\n\nNel contesto dei dati di Zetsche et al. (2019), possiamo utilizzare la eCDF per visualizzare la distribuzione dei punteggi BDI-II. Ecco come viene rappresentata la eCDF per l’intero dataset:\n\ndf |&gt; \n  ggplot(aes(bdi)) + \n  stat_ecdf() +\n  labs(x = \"BDI\", y = \"F(BDI)\")\n\n\n\n\n\n\n\nIn questo grafico:\n\nl’asse \\(x\\) rappresenta i valori del BDI-II,\nl’asse \\(y\\) rappresenta la proporzione cumulativa dei dati, cioè \\(F(a)\\).\n\n17.2.3.2 Interpretazione del grafico\n\n\nCrescita della curva: La curva della eCDF parte da 0 (nessun dato è inferiore al valore minimo osservato) e cresce gradualmente fino a 1 (tutti i dati sono inferiori o uguali al valore massimo osservato).\n\nBimodalità: Se la curva presenta dei “gradini” o delle aree con una pendenza più ripida, questo può indicare la presenza di bimodalità, ovvero due gruppi distinti di dati con caratteristiche diverse. Nel caso dei dati BDI-II, la bimodalità potrebbe riflettere la presenza di due sottogruppi di partecipanti con livelli di depressione diversi.\n\n17.2.3.3 Filtrare i dati per il campione clinico\nSe vogliamo analizzare solo i dati relativi al campione clinico (ad esempio, i pazienti con depressione maggiore), possiamo filtrare i dati e rappresentare la eCDF solo per questo gruppo:\n\ndf |&gt; dplyr::filter(group == \"mdd\") |&gt; \n  ggplot(aes(bdi)) + \n  stat_ecdf() +\n  labs(x = \"a\", y = \"F(a)\")\n\n\n\n\n\n\n\nIn questo caso, la eCDF ci mostrerà come i punteggi BDI-II sono distribuiti tra i pazienti con depressione maggiore, permettendoci di identificare eventuali pattern specifici per questo gruppo.\nIn sintesi, la eCDF è uno strumento potente per analizzare e visualizzare la distribuzione di dati numerici, specialmente quando si vogliono identificare pattern specifici o confrontare distribuzioni tra diversi gruppi.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Esplorare i dati numerici</span>"
    ]
  },
  {
    "objectID": "chapters/eda/04_exploring_numeric_data.html#istogramma",
    "href": "chapters/eda/04_exploring_numeric_data.html#istogramma",
    "title": "17  Esplorare i dati numerici",
    "section": "\n17.3 Istogramma",
    "text": "17.3 Istogramma\nSebbene il concetto di Funzione di Distribuzione Empirica Cumulativa (eCDF) venga ampiamente discusso nei testi di statistica, in pratica tale rappresentazione non è molto diffusa. Il motivo principale è che l’eCDF non rende immediatamente visibili alcune caratteristiche fondamentali della distribuzione, come il valore intorno al quale essa è centrata, se la distribuzione sia simmetrica o quali intervalli contengano il 95% dei dati, ad esempio. Gli istogrammi, invece, sono molto più utilizzati perché facilitano notevolmente la comprensione di queste proprietà, sacrificando solo un po’ di informazione per fornire una rappresentazione più intuitiva.\nUn istogramma è un grafico che rappresenta la distribuzione delle frequenze di una variabile. Sull’asse orizzontale (ascisse) vengono indicati i limiti delle classi \\(\\Delta_i\\), mentre sull’asse verticale (ordinate) si riporta la densità della frequenza relativa della variabile \\(X\\) per ciascuna classe \\(\\Delta_i\\).\nPer descrivere formalmente la densità della frequenza relativa, si utilizza una funzione costante a tratti definita come:\n\\[\n\\varphi_n(x) = \\frac{f_i}{b_i - a_i},\n\\]\ndove:\n\n\n\\(f_i\\) è la frequenza relativa della classe \\(\\Delta_i\\),\n\n\\(b_i - a_i\\) è l’ampiezza della classe \\(\\Delta_i\\).\n\nIn questo modo, l’area del rettangolo corrispondente a \\(\\Delta_i\\) in un istogramma risulta proporzionale alla frequenza relativa \\(f_i\\). Poiché la somma delle frequenze relative deve essere pari a 1, l’area totale di un istogramma delle frequenze relative risulta anch’essa uguale a 1, corrispondendo alla somma delle aree di tutti i rettangoli.\nGli istogrammi costituiscono quindi uno strumento essenziale per visualizzare e comprendere le principali caratteristiche di una distribuzione, agevolando l’analisi della sua forma, della sua tendenza centrale e della sua dispersione.\nPer fare un esempio, costruiamo un istogramma per i valori BDI-II di Zetsche et al. (2019). Con i quattro intervalli individuati dai cut-off del BDI-II creiamo una prima versione dell’istogramma – si notino le frequenze assolute sull’asse delle ordinate.\n\nggplot(df, aes(x = bdi)) +\n  geom_histogram(\n    breaks = c(0, 13.5, 19.5, 28.5, 63),\n    aes(y = after_stat(density)),  # oppure after_stat(count / sum(count))\n    linewidth = 0.5\n  ) +\n  labs(\n    x = \"BDI-II\", \n    y = \"Densità\"\n  ) \n\n\n\n\n\n\n\nAnche se nel caso presente è sensato usare ampiezze diverse per gli intervalli delle classi, in generale gli istogrammi si costruiscono utilizzando intervalli riportati sulle ascisse con un’ampiezza uguale.\n\nggplot(df, aes(x = bdi)) +\n  geom_histogram(\n    aes(y = after_stat(density))\n  ) +\n  labs(\n    x = \"BDI-II\", \n    y = \"Densità\"\n  )",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Esplorare i dati numerici</span>"
    ]
  },
  {
    "objectID": "chapters/eda/04_exploring_numeric_data.html#kernel-density-plot",
    "href": "chapters/eda/04_exploring_numeric_data.html#kernel-density-plot",
    "title": "17  Esplorare i dati numerici",
    "section": "\n17.4 Kernel Density Plot",
    "text": "17.4 Kernel Density Plot\nUn limite evidente degli istogrammi è che la loro forma dipende da scelte arbitrarie: il numero e l’ampiezza delle classi (o bin) può infatti influenzare in modo sostanziale l’aspetto finale del grafico, rendendo più difficile l’interpretazione della distribuzione dei dati. Una soluzione a questo problema è offerta dalla stima della densità kernel (Kernel Density Estimation, KDE), un metodo che fornisce un profilo continuo e smussato della distribuzione, meno condizionato dall’arbitrarietà delle classi.\n\n17.4.1 Differenza tra istogramma e KDE\nNell’istogramma, dividiamo l’asse orizzontale in intervalli di ampiezza prefissata (i bin) e costruiamo rettangoli la cui altezza è proporzionale alla frequenza (o densità) dei dati che ricadono in ciascun intervallo. Se cambiamo il numero o la larghezza dei bin, la forma dell’istogramma può variare sensibilmente.\nLa KDE, invece, non suddivide i dati in intervalli fissi. Al contrario, “appoggia” una piccola curva (il kernel) su ogni singola osservazione. Le curve utilizzate (ad esempio di tipo gaussiano) hanno una larghezza, detta bandwidth, che controlla il grado di smussamento: con un bandwidth molto piccolo, la stima segue da vicino le singole osservazioni, generando un profilo più frastagliato; con un bandwidth più ampio, la curva risultante è più liscia, ma rischia di nascondere dettagli importanti.\nPer comprendere in modo intuitivo il concetto di KDE, possiamo partire da un esempio semplice. Immaginiamo di costruire un istogramma con classi di ampiezza sempre più piccola. Se avessimo a disposizione un numero enorme di dati (ad esempio, un milione di misurazioni dell’altezza di individui) e li rappresentassimo con bin sempre più stretti (0.1, 0.01, ecc.), l’istogramma diventerebbe sempre più levigato, avvicinandosi a una curva continua. Questo processo illustra l’idea alla base della KDE, che approssima la distribuzione dei dati in modo fluido e continuo.\nLa KDE, tuttavia, opera in modo più elegante e senza richiedere un numero enorme di punti: posiziona un piccolo “dosso di campana” (o un altro tipo di kernel) su ciascun punto dati e somma tutte queste curve in un’unica curva finale.\n\n\n\n\n\n\nChe cosa vuol dire “dosso di campana”?\n\n\n\nPossiamo immaginarlo come una piccola curva gaussiana: una curva simile alla forma di una campana che si innalza e poi discende dolcemente.\n\nOgni singolo dato viene “coperto” da questa mini-campana.\nL’ampiezza (o “larghezza”) della campana è regolata dal bandwidth, che stabilisce se la curva sarà più o meno “distesa” sul grafico.\nSommando tutte le piccole campane (una per ogni osservazione), otteniamo una curva di densità liscia e continua che rappresenta la distribuzione dei dati senza i “salti” tipici dell’istogramma.\n\n\n\nIl risultato è una curva di densità che:\n\n\nÈ continua: a differenza degli istogrammi, non presenta bruschi salti di altezza tra i bin: la curva scorre in modo uniforme lungo tutto l’asse orizzontale.\n\nMostra la proporzione di dati in ogni intervallo: l’area sotto la curva in un determinato range corrisponde alla percentuale (o probabilità) di dati che cadono in quell’intervallo.\n\nDipende dal bandwidth:\n\nUn bandwidth piccolo produce una curva più ondulata e “frastagliata” (poiché segue da vicino ogni singolo dato).\nUn bandwidth grande genera una curva più liscia e arrotondata, ma rischia di “coprire” troppi dettagli della distribuzione originaria.\n\n\n\nSi noti che la stima della densità kernel introduce, tuttavia, un’ipotesi di fondo: che la distribuzione dei dati “reali” sia “liscia” e non presenti discontinuità improvvise. Questo è spesso ragionevole (ad esempio per dati fisiologici come l’altezza), ma in altri casi potrebbe non esserlo. È quindi importante scegliere un bandwidth che rifletta adeguatamente il livello di dettaglio che vogliamo mostrare.\nInoltre, l’asse delle ordinate (l’asse y) rappresenta la densità, non la frequenza assoluta. È possibile costruire un istogramma in cui l’altezza dei rettangoli mostra quante osservazioni ricadono in ciascun bin. Nella KDE, l’altezza della curva è tale che l’area totale sotto di essa sia pari a 1, rispecchiando la natura di una funzione di densità di probabilità.\nDi seguito esaminiamo un esempio che mostra la costruzione passo dopo passo di istogrammi con diversi valori di binwidth, fino a passare a una stima di densità. Consideriemo un dataset con un numero di osservazioni molto elevato (i valori di altezza, heights, riportati da 1050 partecipanti, estratti dal pacchetto dslabs), suddiviso in due gruppi: maschi e femmine. Ecco come potremmo prima costruire un istogramma di altezze per i maschi, per poi tracciare una curva di densità smussata:\n\n# Istogramma con bin di ampiezza 1\nggplot(heights |&gt; dplyr::filter(sex == \"Male\"), aes(height)) +\n  geom_histogram(\n    binwidth = 1\n  )\n\n\n\n\n\n\n\n\n# Aggiunta della curva di densità sopra l'istogramma\nggplot(heights |&gt; dplyr::filter(sex == \"Male\"), aes(height)) +\n  geom_histogram(\n    aes(y = after_stat(density)), \n    binwidth = 1\n  ) +\n  geom_line(stat = 'density')\n\n\n\n\n\n\n\nVariando il parametro di regolazione (adjust o bandwidth) nella funzione geom_density(), possiamo modificare il livello di smussamento:\n\n# Istogramma base con alpha e colore coerente\np &lt;- ggplot(heights |&gt; filter(sex == \"Male\"), aes(x = height)) +\n  geom_histogram(\n    aes(y = after_stat(density)),\n    binwidth = 1\n  ) +\n  labs(\n    x = \"Altezza\",\n    y = \"Densità\"\n  )\n\n# Più ondulato (banda piccola)\np1 &lt;- p +\n  geom_line(stat = \"density\", adjust = 0.5) +\n  labs(subtitle = \"Smoothing accentuato (adjust = 0.5)\")\n\n# Più liscio (banda larga)\np2 &lt;- p +\n  geom_line(stat = \"density\", adjust = 2) +\n  labs(subtitle = \"Smoothing attenuato (adjust = 2)\")\n\n\np1\n\n\n\n\n\n\n\n\np2\n\n\n\n\n\n\n\nPer illustrare ulteriormente l’uso della KDE, ora consideriamo i punteggi BDI-II di Zetsche et al. (2019). Con il codice seguente creiamo due curve di densità, una per ogni gruppo:\n\nggplot(df, aes(x = bdi, fill = group)) +\n  geom_density() +\n  labs(\n    x = \"BDI-II\",\n    y = \"Densità\"\n  )\n\n\n\n\n\n\n\nQui, la sovrapposizione delle due curve ci consente di confrontare la distribuzione dei punteggi BDI-II tra i due gruppi in maniera molto più fluida e intuitiva rispetto a quanto faremmo con due istogrammi separati o con un istogramma combinato. Inoltre, non siamo più vincolati alla scelta dei bin: l’aspetto delle curve dipende soltanto dalla funzione kernel utilizzata e dal parametro di smussamento.\nIn conclusione,\n\nl’istogramma rimane uno strumento rapido e intuitivo, privo di assunzioni, ma sensibile alla scelta di numero e ampiezza dei bin;\nla stima della densità kernel (KDE) offre una rappresentazione continua della distribuzione dei dati, fornendo un quadro più “morbido” e spesso più informativo. Tuttavia, introduce alcune assunzioni e richiede la scelta del bandwidth ottimale.\n\nIn definitiva, è consigliabile usare entrambe le tecniche per ottenere una panoramica completa dei propri dati: l’istogramma permette di dare un primo sguardo alla loro distribuzione “grezza” (senza presupposti), mentre la KDE aiuta a comprenderne l’eventuale struttura “liscia” di fondo.\n\n17.4.2 Area sottesa alla curva di densità: un’interpretazione probabilistica\nQuando si lavora con una curva di densità, è importante capire che l’area totale sotto la curva rappresenta la probabilità totale, che è sempre pari a 1 (o 100%). Questo significa che l’area sotto la curva in un determinato intervallo corrisponde alla probabilità che un dato valore cada in quell’intervallo.\n\n17.4.2.1 Come interpretare l’asse Y\nL’asse y di un grafico di densità non rappresenta direttamente la probabilità, ma è scalato in modo che l’area totale sotto la curva sia uguale a 1. Se immaginiamo di creare un “bin” (un intervallo) con una base di 1 unità di lunghezza, il valore sull’asse y ci indica la proporzione di valori che cadono in quel bin. Tuttavia, questa interpretazione è valida solo per bin di dimensione 1. Per intervalli di altre dimensioni, il modo migliore per determinare la proporzione di dati in quell’intervallo è calcolare la proporzione dell’area totale sotto la curva che cade in quell’intervallo.\n\n17.4.2.2 Esempio pratico\nConsideriamo un esempio con i dati delle altezze degli uomini. Supponiamo di voler sapere quale proporzione di uomini ha un’altezza compresa tra 65 e 68 pollici. Per farlo, calcoliamo l’area sotto la curva di densità in quell’intervallo.\nEcco come appare graficamente:\n\n\n\n\n\n\n\n\nL’area evidenziata in azzurro rappresenta la proporzione di uomini con altezza tra 65 e 68 pollici. Calcolando questa area, troviamo che circa il 0.3 (ovvero il 30% degli uomini ha un’altezza in questo intervallo.\n\n17.4.2.3 Utilizzo della curva di densità come riepilogo\nComprendendo questo concetto, possiamo utilizzare la curva di densità come un efficace strumento di riepilogo. Per questo dataset, l’assunzione di smoothness (lisciatura) della curva è ragionevole, e possiamo condividere questa rappresentazione grafica per comunicare in modo chiaro e intuitivo la distribuzione delle altezze degli uomini.\nEcco un esempio di come appare la curva di densità smooth per le altezze degli uomini:\n\n\n\n\n\n\n\n\nIn sintesi, l’area sotto la curva di densità in un determinato intervallo rappresenta la probabilità che un valore casuale cada in quell’intervallo, rendendo la curva di densità uno strumento potente per comprendere e comunicare la distribuzione dei dati.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Esplorare i dati numerici</span>"
    ]
  },
  {
    "objectID": "chapters/eda/04_exploring_numeric_data.html#forma-di-una-distribuzione",
    "href": "chapters/eda/04_exploring_numeric_data.html#forma-di-una-distribuzione",
    "title": "17  Esplorare i dati numerici",
    "section": "\n17.5 Forma di una distribuzione",
    "text": "17.5 Forma di una distribuzione\nIn statistica, la forma di una distribuzione descrive come i dati sono distribuiti intorno ai valori centrali. Si distingue tra distribuzioni simmetriche e asimmetriche, e tra distribuzioni unimodali e multimodali. Un’illustrazione grafica è fornita nella figura seguente. Nel pannello 1, la distribuzione è unimodale con asimmetria negativa; nel pannello 2, la distribuzione è unimodale con asimmetria positiva; nel pannello 3, la distribuzione è simmetrica e unimodale; nel pannello 4, la distribuzione è bimodale.\n\n\nDistribuzioni\n\nIl grafico della densità di kernel (Kernel Density Plot) dei valori BDI-II nel campione di Zetsche et al. (2019) è bimodale. Questo indica che le osservazioni della distribuzione si raggruppano in due cluster distinti: un gruppo di osservazioni tende ad avere valori BDI-II bassi, mentre l’altro gruppo tende ad avere valori BDI-II alti. Questi due cluster di osservazioni corrispondono al gruppo di controllo e al gruppo clinico nel campione di dati esaminato da Zetsche et al. (2019).",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Esplorare i dati numerici</span>"
    ]
  },
  {
    "objectID": "chapters/eda/04_exploring_numeric_data.html#indici-di-posizione",
    "href": "chapters/eda/04_exploring_numeric_data.html#indici-di-posizione",
    "title": "17  Esplorare i dati numerici",
    "section": "\n17.6 Indici di posizione",
    "text": "17.6 Indici di posizione\n\n17.6.1 Quantili\nLa distribuzione dei valori BDI-II di Zetsche et al. (2019) può essere sintetizzata attraverso l’uso dei quantili, che sono valori caratteristici che suddividono i dati in parti ugualmente numerose. I quartili sono tre quantili specifici: il primo quartile, \\(q_1\\), divide i dati in due parti, lasciando a sinistra il 25% del campione; il secondo quartile, \\(q_2\\), corrisponde alla mediana e divide i dati in due parti uguali; il terzo quartile lascia a sinistra il 75% del campione.\nInoltre, ci sono altri indici di posizione chiamati decili e percentili che suddividono i dati in parti di dimensioni uguali a 10% e 1%, rispettivamente.\nPer calcolare i quantili, i dati vengono prima ordinati in modo crescente e poi viene determinato il valore di \\(np\\), dove \\(n\\) è la dimensione del campione e \\(p\\) è l’ordine del quantile. Se \\(np\\) non è un intero, il valore del quantile corrisponde al valore del dato che si trova alla posizione successiva alla parte intera di \\(np\\). Se \\(np\\) è un intero, il valore del quantile corrisponde alla media dei dati nelle posizioni \\(k\\) e \\(k+1\\), dove \\(k\\) è la parte intera di \\(np\\).\nGli indici di posizione possono essere utilizzati per creare un box-plot, una rappresentazione grafica della distribuzione dei dati che è molto popolare e può essere utilizzata in alternativa ad un istogramma.\nAd esempio, per calcolare la mediana della distribuzione dei nove soggetti con un unico episodio di depressione maggiore del campione clinico di Zetsche et al. (2019), si determina il valore di \\(np = 9 \\cdot 0.5 = 4.5\\), che non è un intero. Pertanto, il valore del secondo quartile è pari al valore del dato che si trova alla posizione successiva alla parte intera di \\(np\\), ovvero \\(q_2 = x_{4 + 1} = 27\\). Per calcolare il quantile di ordine \\(2/3\\), si determina il valore di \\(np = 9 \\cdot 2/3 = 6\\), che è un intero. Quindi, il valore del quantile corrisponde alla media dei dati nelle posizioni \\(6\\) e \\(7\\), ovvero \\(q_{\\frac{2}{3}} = \\frac{1}{2} (x_{6} + x_{7}) = \\frac{1}{2} (33 + 33) = 33\\).\nUsiamo quantile() per trovare la soluzione dell’esercizio precedente.\n\nx = c(19, 26, 27, 28, 28, 33, 33, 41, 43)\nquantile(x, 2 / 3)\n#&gt; 66.66667% \n#&gt;        33",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Esplorare i dati numerici</span>"
    ]
  },
  {
    "objectID": "chapters/eda/04_exploring_numeric_data.html#mostrare-i-dati",
    "href": "chapters/eda/04_exploring_numeric_data.html#mostrare-i-dati",
    "title": "17  Esplorare i dati numerici",
    "section": "\n17.7 Mostrare i dati",
    "text": "17.7 Mostrare i dati\n\n17.7.1 Diagramma a scatola\nIl box plot è uno strumento grafico che visualizza la dispersione di una distribuzione. I boxplot forniscono una rappresentazione visiva sintetica di cinque valori caratteristici: minimo, primo quartile (25%), mediana (50%), terzo quartile (75%) e massimo. Spesso però, i boxplot “ignorano” i valori considerati anomali (outlier), segnalandoli con punti isolati.\nPer creare un box plot, si disegna un rettangolo (la “scatola”) di altezza arbitraria, basato sulla distanza interquartile (IQR), che corrisponde alla differenza tra il terzo quartile (\\(q_{0.75}\\)) e il primo quartile (\\(q_{0.25}\\)). La mediana (\\(q_{0.5}\\)) è rappresentata da una linea all’interno del rettangolo.\nAi lati della scatola, vengono tracciati due segmenti di retta, detti “baffi”, che rappresentano i valori adiacenti inferiore e superiore. Il valore adiacente inferiore è il valore più basso tra le osservazioni che è maggiore o uguale al primo quartile meno 1.5 volte la distanza interquartile. Il valore adiacente superiore è il valore più alto tra le osservazioni che è minore o uguale al terzo quartile più 1.5 volte la distanza interquartile.\nSe ci sono dei valori che cadono al di fuori dei valori adiacenti, vengono chiamati “valori anomali” e sono rappresentati individualmente nel box plot per evidenziare la loro presenza e posizione. In questo modo, il box plot fornisce una rappresentazione visiva della distribuzione dei dati, permettendo di individuare facilmente eventuali valori anomali e di comprendere la dispersione dei dati.\n\n\n17.7.2 Stratificazione\nNell’analisi dei dati, è comune suddividere le osservazioni in gruppi in base ai valori di una o più variabili associate a tali osservazioni. Questo processo è chiamato stratificazione, e i gruppi risultanti sono detti strati. Ad esempio, nella sezione successiva, dividiamo i valori dei punteggi BDI-II in due gruppi in base alla condizione sperimentale: campione clinico e campione di controllo.\nLa stratificazione è particolarmente utile nella visualizzazione dei dati, poiché spesso siamo interessati a comprendere come la distribuzione di una variabile differisca tra diversi sottogruppi.\nPer esempio, per rappresentare graficamente la distribuzione dei punteggi BDI-II nel gruppo dei pazienti e nel gruppo di controllo, possiamo utilizzare un box-plot. Questo tipo di grafico ci permette di confrontare visivamente la distribuzione dei punteggi tra i due gruppi, evidenziando eventuali differenze.\n\nggplot(df, aes(x = group, y = bdi)) +\n  geom_boxplot() +\n  labs(\n    x = \"Gruppo\", \n    y = \"BDI-II\"\n  )\n\n\n\n\n\n\n\nIn questo grafico:\n\nl’asse x rappresenta i due gruppi (pazienti e controllo),\nl’asse y rappresenta i punteggi BDI-II,\ni box (scatole) mostrano la distribuzione dei punteggi, con la linea centrale che indica la mediana e i “baffi” che rappresentano la variabilità dei dati.\n\nLa stratificazione ci aiuta a identificare rapidamente se ci sono differenze nella distribuzione dei punteggi BDI-II tra i due gruppi. Nel caso presente, il grafico mostra come non vi sia alcuna sovrapposizione tra le due distribuzioni.\nUn risultato migliore si ottiene utilizzando un grafico a violino (violin plot) e includendo anche i dati grezzi.\n\n17.7.3 Grafico a violino\nI grafici a violino combinano le caratteristiche dei box plot e dei grafici di densità di kernel (KDE plot) per offrire una rappresentazione più dettagliata dei dati.\n\nggplot(df, aes(x = group, y = bdi, fill = group)) +\n  geom_violin() +\n  labs(\n    x = \"Gruppo\",\n    y = \"BDI-II\"\n  )\n\n\n\n\n\n\n\n\n17.7.4 Grafico Beeswarm\nIl pacchetto {ggbeeswarm} include una funzione chiamata geom_beeswarm, che può essere utilizzata per creare un grafico beeswarm in ggplot2. Un grafico beeswarm è una variazione del grafico a punti che disperde i dati in modo che non si sovrappongano, rendendo visibili tutti i singoli punti dati. Questo tipo di visualizzazione è particolarmente utile quando si desidera esaminare la distribuzione e la densità di un set di dati, senza ricorrere all’uso di barre d’errore o di scatole e baffi (boxplot), mantenendo un’alta leggibilità anche quando i set di dati sono densi.\n\nggplot(df, aes(x = group, y = bdi, color = group)) +\n  geom_beeswarm(cex = 2) +\n  labs(\n    x = \"Gruppo\",\n    y = \"BDI-II\"\n  )",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Esplorare i dati numerici</span>"
    ]
  },
  {
    "objectID": "chapters/eda/04_exploring_numeric_data.html#riflessioni-conclusive",
    "href": "chapters/eda/04_exploring_numeric_data.html#riflessioni-conclusive",
    "title": "17  Esplorare i dati numerici",
    "section": "Riflessioni conclusive",
    "text": "Riflessioni conclusive\nIn questo capitolo abbiamo illustrato una varietà di tecniche per sintetizzare e visualizzare i dati numerici, concentrandoci sia sugli aspetti descrittivi (come distribuzioni di frequenze, istogrammi e distribuzioni cumulative) sia su metodi più raffinati come la stima della densità kernel. Questi strumenti non solo facilitano la comprensione immediata dei pattern e delle caratteristiche fondamentali dei dati, ma rappresentano anche un passaggio essenziale per identificare anomalie e guidare ulteriori analisi statistiche.\nLa capacità di trasformare dati grezzi in rappresentazioni grafiche chiare e intuitive è fondamentale per comunicare in modo efficace i risultati dell’analisi, soprattutto quando si tratta di supportare decisioni pratiche o di sviluppare ipotesi di ricerca. In questo senso, una visualizzazione accurata e ben strutturata consente di evidenziare aspetti come la forma della distribuzione, la presenza di outlier e le differenze tra sottogruppi, contribuendo a una più profonda interpretazione dei fenomeni studiati.\nInfine, l’integrazione di tecniche di visualizzazione con analisi statistiche sintetiche migliora la trasparenza e l’interpretabilità dei dati, offrendo un quadro completo che supporta sia la valutazione critica che la comunicazione dei risultati.\n\n\n\n\n\n\nProblemi\n\n\n\n\n\nIn questo esercizio, gli studenti raccoglieranno e analizzeranno dati relativi alla Satisfaction With Life Scale (SWLS) (Diener et al., 1985) e alla Scala della Rete Sociale di Lubben (LSNS-6) (Lubben et al., 2006). L’obiettivo è comprendere la relazione tra la soddisfazione di vita e la qualità delle relazioni sociali, esplorando la distribuzione delle variabili e le possibili associazioni tra di esse.\nLa Scala della Rete Sociale di Lubben a 6 item (LSNS-6) è uno strumento utilizzato per valutare l’isolamento sociale negli adulti più anziani, misurando il supporto sociale percepito sia da parte dei familiari che degli amici. La scala comprende sei domande suddivise in due sezioni:\nFAMIGLIA: Considerando le persone a cui sei legato per nascita, matrimonio, adozione, ecc.\n\nQuanti parenti vedi o senti almeno una volta al mese?\nCon quanti parenti ti senti a tuo agio nel parlare di questioni personali?\nCon quanti parenti ti senti così vicino da poter chiedere loro aiuto?\n\nAMICIZIE: Considerando tutti i tuoi amici, inclusi quelli che vivono nel tuo quartiere\n\nQuanti dei tuoi amici vedi o senti almeno una volta al mese?\nCon quanti amici ti senti a tuo agio nel parlare di questioni personali?\nCon quanti amici ti senti così vicino da poter chiedere loro aiuto?\n\nLa scala di risposta è:\n\n0 = nessuno\n1 = uno\n2 = due\n3 = tre o quattro\n4 = da cinque a otto\n5 = nove o più\n\nIl punteggio totale della LSNS-6 si ottiene sommando i punteggi dei sei item, con un range che va da 0 a 30. Un punteggio di 12 o inferiore indica un rischio di isolamento sociale.\nDati da Raccogliere\nOgni studente dovrà raccogliere i seguenti dati su se stesso e sui membri del proprio gruppo TPV:\n\n\nstudent_id: Identificativo univoco dello studente.\n\n\ngroup: Gruppo di appartenenza (es. Gruppo 1, Gruppo 2, ecc.).\n\n\nswls: Punteggio totale sulla Satisfaction With Life Scale (SWLS).\n\n\ngender: Genere (M, F).\n\n\nlsns_total: Punteggio totale della Scala della Rete Sociale di Lubben (LSNS-6).\n\n\nlsns_family: Punteggio della sottoscala engagement with family members (somma degli item 1-3).\n\n\nlsns_friends: Punteggio della sottoscala engagement with friends (somma degli item 4-6).\n\nQueste variabili permetteranno di investigare come la soddisfazione di vita sia associata alla quantità e qualità delle relazioni sociali, distinguendo tra contatti con la famiglia e con gli amici.\nObiettivi dell’Analisi\nL’esercizio è strutturato in tre parti:\n\n\nEsplorazione dei dati e distribuzione delle variabili\n\n\nVisualizzazione e confronto tra gruppi\n\nAnalisi delle possibili associazioni tra SWLS e le componenti della rete sociale\n\nParte 1: Esplorazione dei Dati\n1.1 Caricamento e preparazione del dataset\n\nImporta il dataset swls_lsns_students.csv.\n\nSeleziona le variabili indicate sopra.\n\nControlla ed elimina eventuali duplicati.\n\nControlla ed elimina eventuali valori mancanti.\n\n# Caricamento del dataset\ndf &lt;- rio::import(here::here(\"data\", \"swls_lsns_students.csv\"))\n\n# Selezione delle variabili\ndf &lt;- df |&gt; dplyr::select(student_id, group, swls, gender, \n                          lsns_total, lsns_family, lsns_friends)\n\n# Rimozione dei duplicati\ndf &lt;- df[!duplicated(df$student_id), ]\n\n# Rimozione dei valori mancanti\ndf &lt;- df[complete.cases(df), ]\n1.2 Distribuzione delle variabili\n\nCalcola la distribuzione di frequenza per swls, lsns_total, lsns_family e lsns_friends:\n\nFrequenze assolute e relative\n\nFrequenze cumulative\n\n\n\n# Frequenze assolute e relative\ntable(df$swls)\nprop.table(table(df$swls))\n\ntable(df$lsns_total)\nprop.table(table(df$lsns_total))\n\nCrea un istogramma della distribuzione delle variabili.\n\nggplot(df, aes(x = swls)) +\n  geom_histogram(bins = 10, fill = \"lightblue\", color = \"black\") +\n  labs(title = \"Distribuzione dei punteggi SWLS\",\n       x = \"Punteggio SWLS\",\n       y = \"Frequenza\")\nggplot(df, aes(x = lsns_total)) +\n  geom_histogram(bins = 10, fill = \"lightblue\", color = \"black\") +\n  labs(title = \"Distribuzione dei punteggi LSNS-6 (totale)\",\n       x = \"Punteggio LSNS-6\",\n       y = \"Frequenza\")\n\nCostruisci la funzione di distribuzione empirica cumulativa (eCDF).\n\nggplot(df, aes(x = swls)) +\n  stat_ecdf(geom = \"step\", color = \"blue\") +\n  labs(title = \"Funzione di distribuzione empirica cumulativa SWLS\",\n       x = \"Punteggio SWLS\",\n       y = \"F(x)\")\n\nGenera la curva di densità kernel (KDE) per ogni variabile.\n\nggplot(df, aes(x = swls)) +\n  geom_density(fill = \"lightblue\", alpha = 0.5) +\n  labs(title = \"Curva di densità dei punteggi SWLS\",\n       x = \"Punteggio SWLS\",\n       y = \"Densità\")\nggplot(df, aes(x = lsns_total)) +\n  geom_density(fill = \"lightblue\", alpha = 0.5) +\n  labs(title = \"Curva di densità LSNS-6 (totale)\",\n       x = \"Punteggio LSNS-6\",\n       y = \"Densità\")\nParte 2: Confronto tra Gruppi\n\nCostruisci una tabella di contingenza per gender e livello di rete sociale (alta o bassa, separando sopra e sotto la mediana di lsns_total).\n\ndf &lt;- df |&gt; \n  mutate(lsns_level = ifelse(lsns_total &gt;= median(lsns_total), \"Alto\", \"Basso\"))\n\ntable(df$gender, df$lsns_level)\nprop.table(table(df$gender, df$lsns_level), margin = 1)\n\nCrea un grafico a barre per la distribuzione di lsns_level per genere.\n\nggplot(df, aes(x = lsns_level, fill = gender)) +\n  geom_bar(position = \"dodge\") +\n  labs(title = \"Distribuzione del livello di rete sociale per genere\",\n       x = \"Livello LSNS\",\n       y = \"Conteggio\",\n       fill = \"Genere\")\n\nCostruisci un box plot per confrontare swls tra i gruppi di rete sociale.\n\nggplot(df, aes(x = lsns_level, y = swls, fill = lsns_level)) +\n  geom_boxplot() +\n  labs(title = \"Distribuzione dei punteggi SWLS per livello di rete sociale\",\n       x = \"Livello di rete sociale\",\n       y = \"Punteggio SWLS\")\n\nUsa un violin plot per visualizzare la distribuzione dettagliata.\n\nggplot(df, aes(x = lsns_level, y = swls, fill = lsns_level)) +\n  geom_violin(alpha = 0.5) +\n  geom_jitter(width = 0.1, alpha = 0.5) +\n  labs(title = \"Violin plot con dati grezzi sovrapposti\",\n       x = \"Livello di rete sociale\",\n       y = \"Punteggio SWLS\")\nParte 3: Analisi delle Associazioni tra SWLS e la Rete Sociale\nIl concetto di correlazione verrà approfondito nel Capitolo 21. Per i nostri scopi attuali, possiamo considerarlo come un indice numerico che misura l’intensità e la direzione dell’associazione tra due variabili. Un valore di 0 indica l’assenza di una relazione lineare tra le variabili, mentre i valori +1 e -1 indicano una relazione lineare perfetta, positiva o negativa rispettivamente. I valori intermedi tra -1 e +1 rappresentano associazioni più deboli o forti, a seconda della loro vicinanza agli estremi.\n\n\nCorrelazioni tra SWLS e le sottoscale della LSNS.\n\ncor(df$swls, df$lsns_total, method = \"pearson\")\ncor(df$swls, df$lsns_family, method = \"pearson\")\ncor(df$swls, df$lsns_friends, method = \"pearson\")\nSpiega in maniera inuitiva il significato dei valori ottenuti.\n\n\nGrafico di dispersione tra SWLS e LSNS-6 totale.\n\nUn grafico di dispersione è un diagramma cartesiano in cui ogni punto rappresenta un’osservazione (nel caso attuale, uno studente). Le coordinate dei punti sui due assi, X e Y, indicano i valori delle due variabili considerate per ciascuno studente.\nggplot(df, aes(x = lsns_total, y = swls)) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", color = \"red\") +\n  labs(title = \"Relazione tra rete sociale totale e SWLS\",\n       x = \"Punteggio LSNS-6 Totale\",\n       y = \"Punteggio SWLS\")\nPer ogni grafico generato, includi una descrizione chiara e concisa del suo significato in relazione ai dati analizzati.\nConclusioni\nL’obiettivo è analizzare se e come la soddisfazione di vita degli studenti universitari è influenzata dalle relazioni sociali, distinguendo tra engagement con la famiglia e con gli amici.\nConsegna\nConsegna il file .qmd contenente il codice, le visualizzazioni e le interpretazioni.\n\n\n\n\n\n\n\n\n\nInformazioni sull’ambiente di sviluppo\n\n\n\n\n\n\nsessionInfo()\n#&gt; R version 4.5.1 (2025-06-13)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.6.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] viridis_0.6.5         viridisLite_0.4.2     cowplot_1.2.0        \n#&gt;  [4] ggpubr_0.6.1          gridExtra_2.3         dslabs_0.8.0         \n#&gt;  [7] ggbeeswarm_0.7.2      pillar_1.11.0         tinytable_0.13.0     \n#&gt; [10] patchwork_1.3.2       ggdist_3.3.3          tidybayes_3.0.7      \n#&gt; [13] bayesplot_1.14.0      ggplot2_3.5.2         reliabilitydiag_0.2.1\n#&gt; [16] priorsense_1.1.1      posterior_1.6.1       loo_2.8.0            \n#&gt; [19] rstan_2.32.7          StanHeaders_2.32.10   brms_2.22.0          \n#&gt; [22] Rcpp_1.1.0            sessioninfo_1.2.3     conflicted_1.2.0     \n#&gt; [25] janitor_2.2.1         matrixStats_1.5.0     modelr_0.1.11        \n#&gt; [28] tibble_3.3.0          dplyr_1.1.4           tidyr_1.3.1          \n#&gt; [31] rio_1.2.3             here_1.0.1           \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;  [1] inline_0.3.21         sandwich_3.1-1        rlang_1.1.6          \n#&gt;  [4] magrittr_2.0.3        multcomp_1.4-28       snakecase_0.11.1     \n#&gt;  [7] compiler_4.5.1        systemfonts_1.2.3     vctrs_0.6.5          \n#&gt; [10] stringr_1.5.1         pkgconfig_2.0.3       arrayhelpers_1.1-0   \n#&gt; [13] fastmap_1.2.0         backports_1.5.0       labeling_0.4.3       \n#&gt; [16] rmarkdown_2.29        ragg_1.5.0            purrr_1.1.0          \n#&gt; [19] xfun_0.53             cachem_1.1.0          jsonlite_2.0.0       \n#&gt; [22] broom_1.0.9           parallel_4.5.1        R6_2.6.1             \n#&gt; [25] stringi_1.8.7         RColorBrewer_1.1-3    car_3.1-3            \n#&gt; [28] lubridate_1.9.4       estimability_1.5.1    knitr_1.50           \n#&gt; [31] zoo_1.8-14            R.utils_2.13.0        pacman_0.5.1         \n#&gt; [34] Matrix_1.7-4          splines_4.5.1         timechange_0.3.0     \n#&gt; [37] tidyselect_1.2.1      abind_1.4-8           codetools_0.2-20     \n#&gt; [40] curl_7.0.0            pkgbuild_1.4.8        lattice_0.22-7       \n#&gt; [43] withr_3.0.2           bridgesampling_1.1-2  coda_0.19-4.1        \n#&gt; [46] evaluate_1.0.5        survival_3.8-3        RcppParallel_5.1.11-1\n#&gt; [49] carData_3.0-5         tensorA_0.36.2.1      checkmate_2.3.3      \n#&gt; [52] stats4_4.5.1          distributional_0.5.0  generics_0.1.4       \n#&gt; [55] rprojroot_2.1.1       rstantools_2.5.0      scales_1.4.0         \n#&gt; [58] xtable_1.8-4          glue_1.8.0            emmeans_1.11.2-8     \n#&gt; [61] tools_4.5.1           data.table_1.17.8     ggsignif_0.6.4       \n#&gt; [64] mvtnorm_1.3-3         grid_4.5.1            QuickJSR_1.8.0       \n#&gt; [67] colorspace_2.1-1      nlme_3.1-168          beeswarm_0.4.0       \n#&gt; [70] vipor_0.4.7           Formula_1.2-5         cli_3.6.5            \n#&gt; [73] textshaping_1.0.3     svUnit_1.0.8          Brobdingnag_1.2-9    \n#&gt; [76] V8_7.0.0              gtable_0.3.6          R.methodsS3_1.8.2    \n#&gt; [79] rstatix_0.7.2         digest_0.6.37         TH.data_1.1-4        \n#&gt; [82] htmlwidgets_1.6.4     farver_2.1.2          R.oo_1.27.1          \n#&gt; [85] memoise_2.0.1         htmltools_0.5.8.1     lifecycle_1.0.4      \n#&gt; [88] MASS_7.3-65",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Esplorare i dati numerici</span>"
    ]
  },
  {
    "objectID": "chapters/eda/04_exploring_numeric_data.html#bibliografia",
    "href": "chapters/eda/04_exploring_numeric_data.html#bibliografia",
    "title": "17  Esplorare i dati numerici",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nDiener, E., Emmons, R. A., Larsen, R. J., & Griffin, S. (1985). The Satisfaction With Life Scale. Journal of Personality Assessment, 49(1), 71–75. https://doi.org/10.1207/s15327752jpa4901_13\n\n\nLubben, J., Blozik, E., Gillmann, G., Iliffe, S., Renteln Kruse, W. von, Beck, J. C., & Stuck, A. E. (2006). Performance of an abbreviated version of the Lubben Social Network Scale among three European community-dwelling older adult populations. The Gerontologist, 46(4), 503–513. https://doi.org/10.1093/geront/46.4.503\n\n\nZetsche, U., Buerkner, P.-C., & Renneberg, B. (2019). Future expectations in clinical depression: biased or realistic? Journal of Abnormal Psychology, 128(7), 678–688.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Esplorare i dati numerici</span>"
    ]
  },
  {
    "objectID": "chapters/eda/05_data_visualization.html",
    "href": "chapters/eda/05_data_visualization.html",
    "title": "18  Principi della visualizzazione dei dati",
    "section": "",
    "text": "Introduzione\nIn questo capitolo vengono introdotti i principi fondamentali della visualizzazione dei dati, accompagnati da esempi concreti e da una discussione sui principali errori da evitare. La visualizzazione non è soltanto un supporto estetico: è uno strumento essenziale per interpretare, comunicare e validare i risultati delle analisi.\nUn buon grafico permette di cogliere immediatamente pattern, anomalie e bias che resterebbero nascosti in una semplice tabella di valori. Per chi desidera un approfondimento sistematico, un riferimento utile è il capitolo Data Visualization del libro Introduction to Data Science..\nL’ampia disponibilità di dataset complessi e la diffusione di strumenti software sempre più accessibili hanno reso la visualizzazione un passaggio centrale in molti ambiti scientifici e professionali: non solo facilita la comunicazione dei risultati, ma stimola nuove domande e consente di individuare rapidamente errori o anomalie.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Principi della visualizzazione dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/05_data_visualization.html#introduzione",
    "href": "chapters/eda/05_data_visualization.html#introduzione",
    "title": "18  Principi della visualizzazione dei dati",
    "section": "",
    "text": "Panoramica del capitolo\n\nComunicare i risultati basati sui dati.\nUtilizzare ggplot2 per creare grafici personalizzati.\n\nRiconoscere i limiti di alcuni grafici comunemente utilizzati e comprendere perché evitarli.\n\n\n\n\n\n\n\nPrerequisiti\n\n\n\n\n\n\nLeggere Testing Statistical Charts: What Makes a Good Graph? (Vanderplas, Cook, and Hofmann 2020). Questo articolo descrive le migliori pratiche per la creazione di grafici.\nConsultare il capitolo Data visualization di Wickham et al. (2023). Questo capitolo fornisce una panoramica degli aspetti fondamentali della visualizzazione dei dati.\nConsultare Data Visualization. A practical introduction di Healy (2018).\nConsultare Fundamentals of Data Visualization di Wilke (2019).\nLeggere il post Open letter to journal editors: dynamite plots must die di Rafael Irizarry.\nConsultare il post The top ten worst graphs di Karl Broman.\nLeggere il capitolo Data Visualization di Introduction to Data Science.\nVisionare Communicating Science Using Visuals: Tips for Scientists.\n\n\n\n\n\n\n\n\n\n\nPreparazione del Notebook\n\n\n\n\n\n\nhere::here(\"code\", \"_common.R\") |&gt;\n  source()\n\n# Load packages\nif (!requireNamespace(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(dslabs, ggrepel, stringr)",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Principi della visualizzazione dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/05_data_visualization.html#unimmagine-vale-più-di-mille-parole",
    "href": "chapters/eda/05_data_visualization.html#unimmagine-vale-più-di-mille-parole",
    "title": "18  Principi della visualizzazione dei dati",
    "section": "\n18.1 Un’immagine vale più di mille parole",
    "text": "18.1 Un’immagine vale più di mille parole\nI numeri da soli raramente raccontano una storia chiara. Consideriamo, ad esempio, i dati sugli omicidi con armi da fuoco negli Stati Uniti. Una tabella con i valori grezzi permette di sapere, per ogni stato, popolazione e numero di omicidi. Tuttavia, osservandola è difficile farsi un’idea immediata della distribuzione: quali sono gli stati più popolosi o i meno popolosi? Qual è la dimensione “tipica” di uno stato? Esiste una relazione tra popolazione e numero totale di omicidi? E come si distribuiscono i tassi di omicidio tra le diverse regioni?\n\nhead(murders)\n#&gt;        state abb region population total\n#&gt; 1    Alabama  AL  South    4779736   135\n#&gt; 2     Alaska  AK   West     710231    19\n#&gt; 3    Arizona  AZ   West    6392017   232\n#&gt; 4   Arkansas  AR  South    2915918    93\n#&gt; 5 California  CA   West   37253956  1257\n#&gt; 6   Colorado  CO   West    5029196    65\n\nUn grafico, al contrario, rende queste relazioni visibili a colpo d’occhio. Nella figura che segue, la scala logaritmica su entrambi gli assi permette di rappresentare in modo compatto sia gli stati più grandi, come California, Texas e New York, sia quelli con popolazioni molto ridotte, come Wyoming, Vermont o Alaska. Si vede subito che la maggior parte degli stati ha una popolazione compresa tra uno e dieci milioni di abitanti, mentre gli stati più popolosi si collocano nettamente a destra del grafico.\n\n\n\n\n\n\n\n\nAllo stesso modo, la relazione tra popolazione e numero di omicidi appare chiara: più abitanti significa, in media, più omicidi. Tuttavia, si notano discrepanze: alcuni stati registrano più omicidi del previsto in rapporto alla popolazione, mentre altri ne hanno meno, segnalando l’influenza di fattori ulteriori. Anche la colorazione per regione rivela differenze interessanti: il Sud tende a registrare tassi più elevati, il Nord-Est valori relativamente bassi, l’Ovest mostra una grande variabilità, e il Midwest si colloca in posizione intermedia o bassa.\nIl detto “un’immagine vale più di mille parole” trova qui piena conferma: un buon grafico non solo comunica con immediatezza, ma spesso sostituisce lunghe spiegazioni numeriche, guidando lo sguardo verso i pattern più rilevanti e stimolando nuove domande di analisi.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Principi della visualizzazione dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/05_data_visualization.html#codificare-i-dati-attraverso-segnali-visivi",
    "href": "chapters/eda/05_data_visualization.html#codificare-i-dati-attraverso-segnali-visivi",
    "title": "18  Principi della visualizzazione dei dati",
    "section": "\n18.2 Codificare i dati attraverso segnali visivi",
    "text": "18.2 Codificare i dati attraverso segnali visivi\nOgni visualizzazione si fonda sulla trasformazione dei dati in segnali visivi che il nostro sistema percettivo può elaborare con immediatezza. Gli elementi più comuni sono la posizione, la lunghezza, gli angoli, l’area, la luminosità e la tonalità del colore. Non tutti, però, hanno la stessa efficacia.\nIl nostro cervello è particolarmente sensibile alle differenze spaziali: per questo la posizione e la lunghezza sono i canali più potenti per trasmettere informazione quantitativa. Un grafico a barre, ad esempio, permette di confrontare valori in modo rapido e preciso proprio perché sfrutta la lunghezza come segnale visivo. Al contrario, angoli e aree sono molto meno intuitivi: grafici a torta o bolle possono risultare accattivanti, ma spesso portano a errori percettivi, soprattutto quando le differenze tra categorie sono ridotte.\nIl colore gioca un ruolo cruciale quando vogliamo distinguere categorie o rappresentare variabili qualitative. È un elemento indispensabile nelle visualizzazioni multidimensionali, come le heatmap, ma deve essere usato con attenzione. Alcune combinazioni, come il rosso e il verde, possono rendere il grafico illeggibile per persone con daltonismo, e in generale una palette mal progettata può distogliere l’attenzione dai contenuti principali.\nInfine, è importante distinguere tra strumenti di precisione e strumenti di sintesi. Le tabelle sono ideali quando è necessario leggere valori numerici esatti, mentre i grafici diventano insostituibili di fronte a dataset complessi, perché mettono in evidenza pattern, tendenze e anomalie che difficilmente emergerebbero da un elenco di numeri.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Principi della visualizzazione dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/05_data_visualization.html#scelta-della-visualizzazione-più-adeguata",
    "href": "chapters/eda/05_data_visualization.html#scelta-della-visualizzazione-più-adeguata",
    "title": "18  Principi della visualizzazione dei dati",
    "section": "\n18.3 Scelta della visualizzazione più adeguata",
    "text": "18.3 Scelta della visualizzazione più adeguata\nNon esiste una visualizzazione “migliore” in assoluto: la scelta dipende sempre dalla natura dei dati e dallo scopo comunicativo. Se l’obiettivo è confrontare valori quantitativi tra diverse categorie, i grafici a barre o i dot plot risultano particolarmente chiari. Quando invece vogliamo descrivere la distribuzione di una variabile continua, strumenti come l’istogramma, il boxplot o i più moderni raincloud plots permettono di cogliere rapidamente forma, variabilità e presenza di outlier. Per indagare relazioni tra due variabili continue, i grafici di dispersione offrono un’immediatezza che difficilmente può essere sostituita da altre rappresentazioni.\nQualunque sia il tipo di grafico scelto, rimane centrale il principio della chiarezza. Una visualizzazione sovraccarica di dettagli, effetti grafici o decorazioni rischia di distrarre e confondere, mentre una rappresentazione essenziale mette in risalto il messaggio principale e guida lo sguardo verso le informazioni rilevanti. In questo senso, un buon grafico non è solo corretto dal punto di vista tecnico, ma è anche uno strumento di comunicazione efficace.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Principi della visualizzazione dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/05_data_visualization.html#aspetti-tecnici-della-visualizzazione",
    "href": "chapters/eda/05_data_visualization.html#aspetti-tecnici-della-visualizzazione",
    "title": "18  Principi della visualizzazione dei dati",
    "section": "\n18.4 Aspetti tecnici della visualizzazione",
    "text": "18.4 Aspetti tecnici della visualizzazione\nUna buona visualizzazione non dipende soltanto dalla scelta del tipo di grafico, ma anche da una serie di accorgimenti tecnici che ne determinano chiarezza e correttezza percettiva. Alcune regole, spesso sottovalutate, possono fare la differenza tra un grafico informativo e uno fuorviante.\n\n18.4.1 Inclusione dello zero\nQuando la lunghezza viene utilizzata come segnale visivo – come accade nei grafici a barre – l’asse deve sempre partire da zero. In caso contrario, le differenze appariranno artificialmente amplificate, inducendo interpretazioni scorrette. Nei grafici che si basano sulla posizione, come gli scatter plot, questa regola non è altrettanto stringente: qui l’inclusione dello zero dipende piuttosto dal contesto e dal significato delle variabili rappresentate.\n\n18.4.2 Prevenire le distorsioni\nAlcuni grafici, come i bubble plot, rischiano di esagerare le differenze fra i valori perché utilizzano l’area come segnale visivo. La superficie di un cerchio cresce infatti con il quadrato del raggio, producendo sproporzioni difficili da cogliere a occhio nudo. Un esempio emblematico è il grafico mostrato durante il Discorso sullo Stato dell’Unione del 2011, in cui il PIL degli Stati Uniti appariva molto più grande di quello cinese o francese. In realtà, la distorsione derivava proprio dall’aver reso il raggio proporzionale al valore, trasformando implicitamente la scala in quadratica. La lezione che se ne ricava è semplice: per confronti accurati, meglio affidarsi a posizione e lunghezza, evitando aree o volumi.\n\n\n\n\n\n\nState of the Union\n\n\n\n\n\n\n\n\n\n\nFigura 18.1: Fonte: Irizarry (2024).\n\n\nIl confronto sottostante illustra l’impatto della scelta tra raggio e area nella rappresentazione grafica:\n\n\n\n\n\nFigura 18.2: Fonte: Irizarry (2024).\n\n\n\n\n\n\n\nFigura 18.3: Fonte: Irizarry (2024).\n\n\nQuesto caso dimostra chiaramente come, per evitare distorsioni percettive, sia una prassi consolidata ottimizzare la visualizzazione dei dati attraverso confronti basati su lunghezze (o posizioni) anziché su aree o volumi, come evidenziato dall’errata rappresentazione del PIL nel grafico originale.\n\n\n\n\n18.4.3 Ordinamento delle categorie\nNei grafici categoriali, disporre le categorie in ordine alfabetico può sembrare neutrale, ma raramente aiuta la comprensione. Ordinare le categorie in base al valore della variabile di interesse, invece, evidenzia immediatamente pattern e relazioni, rendendo la lettura molto più intuitiva.\n\n18.4.4 Mostrare i dati, non solo le sintesi\nI cosiddetti dynamite plots, che riportano medie ed errori standard, sono ancora molto diffusi ma comunicano poco e spesso in modo fuorviante. Guardando una semplice barra con un intervallo di errore, non è possibile capire se i dati siano distribuiti in modo simmetrico, se esistano outlier o se i gruppi si sovrappongano. Visualizzazioni che mostrano i singoli dati – ad esempio dot plot arricchiti da tecniche come jitter e trasparenza – permettono invece di cogliere la variabilità interna, facilitano il confronto e raccontano una storia molto più completa.\n\n\n\n\n\n\nMostrare i dati\n\n\n\n\n\nConsideriamo il seguente grafico a barre (dynamite plot) che mostra la media (estremità superiore delle barre) e gli errori standard.\n\n\n\n\n\nFigura 18.4: Fonte: Irizarry (2024).\n\n\nQuesta visualizzazione offre informazioni limitate:\n\nLe barre partono da 0, suggerendo erroneamente l’esistenza di esseri umani alti pochi centimetri.\n\nNon chiarisce se tutti i maschi siano più alti delle femmine o come siano distribuite le altezze.\n\nUn approccio migliore è quello di mostrare i dati:\n\n\n\n\n\nFigura 18.5: Fonte: Irizarry (2024).\n\n\nLa visualizzazione di tutti i punti (238 femmine e 812 maschi) rivela l’intervallo dei dati, ma persiste un problema: i punti sovrapposti ostacolano l’interpretazione.\nOttimizzazioni: jitter e trasparenza\n\n\n\n\n\nFigura 18.6: Fonte: Irizarry (2024).\n\n\nDue miglioramenti chiave:\n\n\nJitter orizzontale: spostamento casuale dei punti per ridurre la sovrapposizione.\n\n\nAlpha blending: trasparenza graduale: le aree con più dati appaiono più scure.\n\nRisultati:\n\nsi osserva che i maschi sono in media più alti;\nè chiaro che vi è una grande variabilità e una notevole sovrapposizione tra le due distribuzioni.\n\nIn sintesi, strumenti semplici, come jitter e trasparenza, migliorano drasticamente l’interpretazione della distribuzione dei dati.\n\n\n\n\n18.4.5 Confronti coerenti\nQuando si confrontano distribuzioni diverse, ad esempio con istogrammi affiancati, è indispensabile utilizzare la stessa scala sugli assi. Differenze apparenti potrebbero dipendere soltanto da un’incoerenza di rappresentazione. Anche l’allineamento dei grafici gioca un ruolo importante: disporli in verticale o in orizzontale con assi coerenti rende il confronto immediato e riduce il rischio di fraintendimenti.\n\n\n\n\n\n\nFacilitare i confronti\n\n\n\n\n\nPoiché ci sono molti punti, è più efficace mostrare la distribuzione dei dati anziché i singoli valori. Per questo motivo, utilizziamo istogrammi separati per ciascun gruppo:\n\n\n\n\n\nFigura 18.7: Fonte: Irizarry (2024).\n\n\nTuttavia, in questo grafico non è immediatamente evidente che, in media, gli uomini siano più alti delle donne. Per accorgersene, bisogna osservare con attenzione e notare che l’asse x del grafico maschile copre un intervallo di valori più ampio. Un principio fondamentale nella comparazione di dati tra due grafici è mantenere le stesse scale sugli assi.\nNegli istogrammi, l’altezza media si riflette in spostamenti orizzontali: valori più bassi a sinistra, valori più alti a destra. Allineare i grafici in verticale aiuta a visualizzare meglio questa differenza quando gli assi sono coerenti:\n\n\n\n\n\nFigura 18.8: Fonte: Irizarry (2024).\n\n\nQuesto secondo grafico rende molto più evidente che, in media, gli uomini sono più alti delle donne.\n\n\n\n\n18.4.6 Trasformazioni logaritmiche\nMolti dati reali coprono ordini di grandezza molto ampi. In questi casi, una scala lineare comprime la maggior parte delle osservazioni e rende invisibili le differenze più sottili. Le trasformazioni logaritmiche sono uno strumento efficace per restituire leggibilità a distribuzioni fortemente asimmetriche. Altre trasformazioni, come la logistica o la radice quadrata, sono utili rispettivamente per rappresentare rapporti di probabilità o stabilizzare la varianza in dati di conteggio. La scelta della trasformazione non è mai neutrale, ma può illuminare pattern altrimenti nascosti.\n\n\n\n\n\n\nTrasformazioni logaritmiche\n\n\n\n\n\nConsideriamo questo grafico a barre, che mostra la popolazione media dei paesi di ciascun continente nel 2015:\n\n\n\n\n\nFigura 18.9: Fonte: Irizarry (2024).\n\n\nA prima vista, sembrerebbe che i paesi dell’Asia siano molto più popolosi rispetto a quelli degli altri continenti. Tuttavia, applicando il principio che ci chiede di mostrare i dati, notiamo rapidamente che questa differenza è dovuta alla presenza di due paesi con una popolazione estremamente elevata, presumibilmente India e Cina:\n\n\n\n\n\nFigura 18.10: Fonte: Irizarry (2024).\n\n\nConsideriamo ora come la trasformazione logaritmica possa migliorare la visualizzazione di dati distribuiti in modo asimmetrico (right-skewed). Esistono anche altre trasformazioni utili, come la logistica (logit), impiegata per interpretare variazioni nei rapporti di probabilità (odds), e la radice quadrata (sqrt), spesso usata per stabilizzare la varianza nei dati basati su conteggi.\nNel caso della popolazione dei paesi, la distribuzione è fortemente asimmetrica: la maggior parte delle nazioni ha una popolazione relativamente piccola, mentre poche hanno numeri estremamente elevati. Come mostrato nel boxplot precedente, questa disparità comprime la maggior parte dei dati in una piccola area del grafico, lasciando molto spazio inutilizzato. Questo rende difficile cogliere le differenze tra la maggior parte dei paesi.\nUna trasformazione logaritmica migliora la leggibilità di uno scatter plot quando i dati mostrano una forte asimmetria. Qui, applicando questa tecnica alle popolazioni nazionali, otteniamo una rappresentazione molto più chiara e informativa. Di seguito, confrontiamo il barplot originale con un boxplot in cui l’asse y è stato trasformato con il logaritmo:\n\n\n\n\n\nFigura 18.11: Fonte: Irizarry (2024).\n\n\nGrazie a questa trasformazione, scopriamo che la mediana della popolazione nei paesi africani è in realtà più alta rispetto a quella dei paesi asiatici, un’informazione che il grafico iniziale non rendeva evidente.\n\n\n\n\n18.4.7 Codifica di variabili aggiuntive\nQuando si vogliono rappresentare più di due variabili nello stesso grafico, si possono sfruttare ulteriori canali visivi come colore, forma o dimensione dei punti. È però necessario bilanciare informazione e leggibilità: troppe codifiche simultanee generano confusione. Inoltre, la scelta delle palette cromatiche deve garantire accessibilità, ad esempio evitando combinazioni problematiche per persone con daltonismo.\n\n\n\n\n\n\nCodificare una terza variabile\n\n\n\n\n\nEsaminiamo la relazione tra sopravvivenza infantile e reddito medio. Il grafico seguente rappresenta questa relazione includendo tre variabili aggiuntive: appartenenza all’OPEC, regione geografica e popolazione.\n\n\n\n\n\nFigura 18.12: Fonte: Irizarry (2024).\n\n\nLe variabili categoriali sono rappresentate attraverso il colore e la forma dei punti. La forma può essere modificata utilizzando l’argomento shape.\n\n\n\n\n18.4.8 Evitare il 3D superfluo\nGrafici tridimensionali, come pie chart o barre in prospettiva, attirano l’occhio ma raramente aggiungono informazione. Anzi, introducono distorsioni che complicano la lettura. Nella maggior parte dei casi, una rappresentazione bidimensionale è più chiara, più fedele e più efficace.\n\n18.4.9 Cifre significative\nMostrare un numero eccessivo di decimali non aumenta la precisione, ma rischia di appesantire la lettura. Una o due cifre significative sono quasi sempre sufficienti per trasmettere il messaggio in modo accurato e comprensibile.\n\n18.4.10 Conoscere il pubblico\nInfine, una regola trasversale: ogni visualizzazione deve tenere conto del pubblico di riferimento. Un grafico pensato per un’analisi interna può includere dettagli tecnici e livelli di complessità elevati; al contrario, quando il pubblico non è specializzato, è preferibile semplificare, ridurre il rumore visivo e accompagnare la rappresentazione con spiegazioni chiare.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Principi della visualizzazione dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/05_data_visualization.html#introduzione-a-ggplot2",
    "href": "chapters/eda/05_data_visualization.html#introduzione-a-ggplot2",
    "title": "18  Principi della visualizzazione dei dati",
    "section": "\n18.5 Introduzione a ggplot2\n",
    "text": "18.5 Introduzione a ggplot2\n\nIn R esistono diversi strumenti per creare grafici, ma il pacchetto ggplot2 si distingue per flessibilità e chiarezza. È diventato lo standard per la visualizzazione dei dati in molti ambiti scientifici, inclusa la psicologia, perché consente di tradurre un dataset complesso in rappresentazioni immediate e leggibili.\n\n18.5.1 Perché usare ggplot2?\nImmaginiamo di voler analizzare i livelli di ansia di un gruppo di studenti prima di un esame. I dati raccolti in una tabella ci danno informazioni preziose, ma difficili da cogliere a colpo d’occhio. Con un grafico, invece, possiamo visualizzare subito quali studenti mostrano livelli di ansia particolarmente elevati e se l’ansia tende a variare in funzione di caratteristiche come l’intolleranza all’incertezza (IU).\nPer illustrare le potenzialità di ggplot2, costruiamo un piccolo dataset simulato:\n\nset.seed(123)\n\nn &lt;- 200\ngender &lt;- sample(c(0, 1), n, replace = TRUE)\nanxiety &lt;- rnorm(n, mean = 50 + 10 * gender, sd = 10)\nstudy_hours &lt;- rnorm(n, mean = 30 - 0.3 * anxiety, sd = 5)\nIU &lt;- rnorm(n, mean = 50 + 0.5 * anxiety, sd = 10)\n\ndf &lt;- data.frame(\n  id = 1:n,\n  gender = factor(gender, levels = c(0, 1), labels = c(\"Male\", \"Female\")),\n  anxiety = round(anxiety, 1),\n  study_hours = round(study_hours, 1),\n  IU = round(IU, 1)\n)\n\nhead(df)\n#&gt;   id gender anxiety study_hours   IU\n#&gt; 1  1   Male    42.9        13.6 65.4\n#&gt; 2  2   Male    52.6        10.5 66.3\n#&gt; 3  3   Male    47.5        11.0 84.0\n#&gt; 4  4 Female    56.5         7.8 85.8\n#&gt; 5  5   Male    40.5        15.7 55.2\n#&gt; 6  6 Female    59.5        13.8 78.8\n\n\n18.5.2 La logica di ggplot2\n\nAlla base di ogni grafico con ggplot2 ci sono tre elementi fondamentali:\n\n\ni dati, cioè il dataset da rappresentare;\n\nle geometrie, ossia il tipo di grafico scelto (punti, barre, linee, boxplot);\n\nla mappatura estetica, che stabilisce come i dati vengono tradotti in segnali visivi (posizione sugli assi, colore, forma, dimensione).\n\nQuesta struttura rende il pacchetto molto intuitivo: basta dire a ggplot2 quali dati usare, come rappresentarli e con quali convenzioni grafiche.\n\n18.5.3 Un primo esempio\nSupponiamo di voler capire se esiste una relazione tra ore di studio e livelli di ansia. Possiamo costruire un semplice grafico a dispersione:\n\nlibrary(ggplot2)\n\ndf |&gt;\n  ggplot(aes(x = study_hours, y = anxiety)) +\n  geom_point()\n\n\n\n\n\n\n\nIn questo codice stiamo dicendo a ggplot2 di prendere come riferimento il dataset df, di mettere le ore di studio sull’asse X, i livelli di ansia sull’asse Y e di rappresentare ogni osservazione con un punto.\n\n18.5.4 Personalizzare il grafico\nUn aspetto importante di ggplot2 è la possibilità di arricchire facilmente la rappresentazione. Aggiungiamo, per esempio, un colore che distingua maschi e femmine, rendiamo i punti più visibili e inseriamo un titolo e delle etichette agli assi:\n\ndf |&gt;\n  ggplot(aes(x = study_hours, y = anxiety, color = gender)) +\n  geom_point(size = 3) +\n  labs(\n    x = \"Ore di studio\",\n    y = \"Livello di ansia\",\n    color = \"Genere\"\n  )\n\n\n\n\n\n\n\nIl risultato è un grafico più leggibile, in cui la distinzione per genere appare immediatamente chiara.\n\n18.5.5 Altri esempi\nCon gli stessi dati possiamo costruire grafici diversi, ciascuno utile a rispondere a domande specifiche. Per esplorare la distribuzione dell’ansia, ad esempio, un istogramma è la scelta naturale:\n\ndf |&gt;\n  ggplot(aes(x = anxiety)) +\n  geom_histogram(binwidth = 5) +\n  labs(x = \"Livello di ansia\", y = \"Frequenza\")\n\n\n\n\n\n\n\nSe invece vogliamo confrontare i livelli di ansia tra maschi e femmine, un boxplot mette in evidenza la mediana, la variabilità e la presenza di eventuali valori estremi:\n\ndf |&gt;\n  ggplot(aes(x = gender, y = anxiety, fill = gender)) +\n  geom_boxplot(alpha = 0.7) +\n  labs(x = \"Genere\", y = \"Livello di ansia\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nIn sintesi, con pochi comandi, ggplot2 consente di costruire grafici chiari e informativi. In psicologia, dove spesso si lavora con dataset complessi, è uno strumento prezioso per trasformare numeri in intuizioni: una buona visualizzazione permette non solo di comunicare risultati, ma anche di stimolare nuove domande di ricerca.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Principi della visualizzazione dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/05_data_visualization.html#riflessioni-conclusive",
    "href": "chapters/eda/05_data_visualization.html#riflessioni-conclusive",
    "title": "18  Principi della visualizzazione dei dati",
    "section": "Riflessioni conclusive",
    "text": "Riflessioni conclusive\nUna visualizzazione ben progettata ha il potere di trasformare dati complessi in messaggi immediati, riducendo il carico cognitivo e facilitando decisioni più consapevoli. Disegnare un grafico non significa solo scegliere un formato piacevole: implica assumersi la responsabilità di comunicare i risultati in modo corretto, evitando di indurre interpretazioni fuorvianti.\nPer raggiungere questo obiettivo, occorre innanzitutto essere chiari sul messaggio che si vuole trasmettere. Un buon grafico è costruito intorno a una domanda precisa e guida lo sguardo del lettore verso gli elementi più rilevanti. Colori, forme e dimensioni non vanno scelti per decorare, ma per orientare l’attenzione: una palette limitata, ben calibrata e accessibile è spesso più efficace di combinazioni cromatiche elaborate. Allo stesso modo, titoli ed etichette devono essere semplici e intuitivi, evitando abbreviazioni o tecnicismi che potrebbero confondere.\nÈ importante anche saper gestire la densità dell’informazione. Quando i dati sono numerosi, tecniche come la trasparenza, il jitter o un eventuale sottocampionamento aiutano a prevenire il sovraccarico visivo e rendono le distribuzioni leggibili. Mostrare i dati grezzi, quando possibile, arricchisce l’interpretazione e riduce il rischio che le sintesi statistiche nascondano pattern interessanti.\nAlcune buone pratiche ricorrono in quasi tutte le situazioni: ordinare le categorie in base ai valori, e non alfabeticamente, facilita i confronti; mantenere assi coerenti rende più immediata la comparazione tra grafici; partire da zero negli assi dei barplot evita distorsioni nelle proporzioni; usare trasformazioni logaritmiche quando i valori coprono ordini di grandezza molto diversi permette di restituire visibilità anche ai dati più piccoli.\nAltre accortezze riguardano la scelta delle codifiche visive. L’aggiunta di variabili tramite colore, forma o dimensione può arricchire un grafico, ma solo se mantiene un equilibrio tra informazione e leggibilità. Le rappresentazioni tridimensionali, al contrario, raramente offrono un reale vantaggio: sono spesso più fuorvianti che utili. Infine, i numeri stessi vanno comunicati con sobrietà: un eccesso di decimali non rende il dato più preciso, ma soltanto più difficile da leggere.\nTutte queste regole devono sempre essere rapportate al pubblico a cui ci si rivolge. In un’analisi tecnica interna è possibile includere dettagli complessi, mentre per un pubblico non specializzato occorre semplificare, spiegare e accompagnare la visualizzazione con un linguaggio accessibile. In definitiva, progettare una buona visualizzazione non significa soltanto “disegnare un grafico”, ma costruire un ponte tra i dati e chi li interpreta, con l’obiettivo di rendere la conoscenza condivisibile e utile.\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nIn questo esercizio analizzerai i dati raccolti dagli studenti sulla Satisfaction With Life Scale (SWLS) e sulla Lubben Social Network Scale (LSNS-6). Le variabili incluse sono:\n\n\nSWLS: Punteggio totale della Scala di Soddisfazione per la Vita.\n\nLSNS-6: Punteggio totale sulla scala della rete sociale.\n\nGenere: Maschio/Femmina.\n\nTipo di scuola superiore: Liceo classico o scientifico vs. altro.\n\nNumero di amici: Auto-riferito.\n\nNumero di uscite settimanali con gli amici.\n\nUtilizzerai questi dati per esplorare le distribuzioni, creare visualizzazioni efficaci e interpretare i risultati.\nEsercizi Teorici\n\n\nPrincipi della visualizzazione\n\nQuali sono i principali segnali visivi utilizzati nella visualizzazione dei dati? Fornisci un esempio pratico per ognuno.\nPerché la posizione e la lunghezza sono considerati segnali visivi più efficaci rispetto all’area e agli angoli?\nSpiega perché i grafici tridimensionali (3D) sono spesso inutili o fuorvianti.\n\n\n\nScelta della visualizzazione\n\nQuale tipo di grafico useresti per mostrare la distribuzione della variabile SWLS? Giustifica la tua risposta.\nSe volessi confrontare la distribuzione della SWLS tra due gruppi (ad esempio, in base al genere), quale grafico useresti? Perché?\n\n\n\nErrori comuni nella visualizzazione\n\nPerché i dynamite plots (grafici a barre con errore standard) sono considerati una cattiva pratica?\nSpiega perché è importante iniziare l’asse Y da zero in un barplot.\nPerché è preferibile ordinare le categorie in base ai valori invece che alfabeticamente?\n\n\n\nEsercizi Pratici in R\n1. Caricamento e ispezione dei dati\nCarica il dataset raccolto dagli studenti (dati_SWLS_LSNS.csv) e stampa un’anteprima dei dati.\n# Caricamento dei dati\ndf &lt;- read.csv(\"dati_SWLS_LSNS.csv\")\n\n# Esamina le prime righe\nhead(df)\nRispondi alle seguenti domande:\n\nQuante osservazioni ci sono nel dataset?\nCi sono valori mancanti? Se sì, quanti?\n\n2. Distribuzione delle variabili\nCrea le seguenti visualizzazioni per analizzare la distribuzione di SWLS e LSNS-6:\n\n\nIstogramma con sovrapposta la curva di densità.\n\nFunzione di distribuzione cumulativa empirica (eCDF).\n\nBox plot per la variabile SWLS.\n\n3. Confronto tra gruppi\n\nCrea un box plot della SWLS per genere.\nCrea un violin plot della LSNS-6 in base al tipo di scuola superiore.\n\n4. Relazioni tra variabili\n\nCrea un grafico di dispersione (scatter plot) per verificare se c’è una relazione tra il punteggio SWLS e il numero di amici.\nAggiungi una linea di regressione al grafico per facilitare l’interpretazione.\n\nggplot(df, aes(x = numero_amici, y = SWLS)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(\n       x = \"Numero di amici\",\n       y = \"Satisfaction With Life Scale (SWLS)\")\nDomande:\n\nQuale relazione osservi tra il numero di amici e la SWLS?\nIl numero di amici è un buon predittore della soddisfazione per la vita?\n\n5. Esplorazione della rete sociale\n\nCrea un barplot per mostrare la distribuzione delle risposte medie ai sei item della LSNS-6.\nEsplora la relazione tra la frequenza delle uscite settimanali e il punteggio totale LSNS-6 utilizzando un box plot.\n\nConsegna\nSalva i grafici creati e rispondi alle domande in forma scritta. Carica il file su Moodle.\n\n\n\n\n\n\n\n\n\nSoluzioni\n\n\n\n\n\n1. Risposte alle domande teoriche\nPrincipi della visualizzazione\n\nI segnali visivi principali sono posizione, lunghezza, angoli, area, luminosità e colore.\nLa posizione e lunghezza sono i segnali più efficaci perché l’occhio umano è molto preciso nel confrontare distanze e altezze, mentre è meno efficace nel confrontare angoli e aree.\nI grafici tridimensionali (3D) spesso aggiungono confusione senza migliorare la leggibilità.\n\nScelta della visualizzazione\n\nPer mostrare la distribuzione della SWLS, è preferibile usare istogrammi e box plot perché evidenziano la forma della distribuzione e la presenza di outlier.\nPer confrontare la SWLS tra generi, un box plot o violin plot è l’opzione migliore, perché mostra la distribuzione completa.\n\nErrori comuni\n\nI dynamite plots nascondono la distribuzione dei dati e non mostrano la variabilità interna ai gruppi.\nIn un barplot, l’asse Y deve iniziare da zero per evitare distorsioni visive.\nLe categorie nei barplot devono essere ordinate per valore per facilitare il confronto.\n\n2. Soluzioni pratiche in R\nCaricamento dei dati\ndf &lt;- read.csv(\"dati_SWLS_LSNS.csv\")\n\n# Esamina il dataset\ndim(df)  # Numero di righe e colonne\nsum(is.na(df))  # Conteggio valori mancanti\nDistribuzione delle variabili\nggplot(df, aes(x = SWLS)) +\n  geom_histogram(\n  aes(y = after_stat(density)), bins = 10, fill = \"blue\", alpha = 0.5\n  ) +\n  geom_density(color = \"red\", size = 1.2)\nggplot(df, aes(SWLS)) +\n  stat_ecdf(geom = \"step\")\nggplot(df, aes(x = \"\", y = SWLS)) +\n  geom_boxplot() +\n  coord_flip()\nConfronto tra gruppi\nggplot(df, aes(x = genere, y = SWLS, fill = genere)) +\n  geom_boxplot()\nggplot(df, aes(x = scuola, y = LSNS6, fill = scuola)) +\n  geom_violin()\nRelazioni tra variabili\nggplot(df, aes(x = numero_amici, y = SWLS)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\nggplot(df, aes(x = uscite_settimanali, y = LSNS6)) +\n  geom_boxplot()\nConclusioni\n(Ad esempio) Le visualizzazioni mostrano che:\n\nSWLS e LSNS-6 variano in base al genere e al tipo di scuola.\nIl numero di amici ha un impatto positivo sulla SWLS, ma la relazione è moderata.\nIl numero di uscite settimanali è correlato positivamente con la rete sociale (LSNS-6).\n\n\n\n\n\n\n\n\n\n\nInformazioni sull’ambiente di sviluppo\n\n\n\n\n\n\nsessionInfo()\n#&gt; R version 4.5.1 (2025-06-13)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.6.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] stringr_1.5.1         ggrepel_0.9.6         dslabs_0.8.0         \n#&gt;  [4] pillar_1.11.0         tinytable_0.13.0      patchwork_1.3.2      \n#&gt;  [7] ggdist_3.3.3          tidybayes_3.0.7       bayesplot_1.14.0     \n#&gt; [10] ggplot2_3.5.2         reliabilitydiag_0.2.1 priorsense_1.1.1     \n#&gt; [13] posterior_1.6.1       loo_2.8.0             rstan_2.32.7         \n#&gt; [16] StanHeaders_2.32.10   brms_2.22.0           Rcpp_1.1.0           \n#&gt; [19] sessioninfo_1.2.3     conflicted_1.2.0      janitor_2.2.1        \n#&gt; [22] matrixStats_1.5.0     modelr_0.1.11         tibble_3.3.0         \n#&gt; [25] dplyr_1.1.4           tidyr_1.3.1           rio_1.2.3            \n#&gt; [28] here_1.0.1           \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;  [1] svUnit_1.0.8          tidyselect_1.2.1      farver_2.1.2         \n#&gt;  [4] fastmap_1.2.0         TH.data_1.1-4         tensorA_0.36.2.1     \n#&gt;  [7] pacman_0.5.1          digest_0.6.37         timechange_0.3.0     \n#&gt; [10] estimability_1.5.1    lifecycle_1.0.4       survival_3.8-3       \n#&gt; [13] magrittr_2.0.3        compiler_4.5.1        rlang_1.1.6          \n#&gt; [16] tools_4.5.1           knitr_1.50            labeling_0.4.3       \n#&gt; [19] bridgesampling_1.1-2  htmlwidgets_1.6.4     curl_7.0.0           \n#&gt; [22] pkgbuild_1.4.8        RColorBrewer_1.1-3    abind_1.4-8          \n#&gt; [25] multcomp_1.4-28       withr_3.0.2           purrr_1.1.0          \n#&gt; [28] grid_4.5.1            stats4_4.5.1          colorspace_2.1-1     \n#&gt; [31] xtable_1.8-4          inline_0.3.21         emmeans_1.11.2-8     \n#&gt; [34] scales_1.4.0          MASS_7.3-65           cli_3.6.5            \n#&gt; [37] mvtnorm_1.3-3         rmarkdown_2.29        ragg_1.5.0           \n#&gt; [40] generics_0.1.4        RcppParallel_5.1.11-1 cachem_1.1.0         \n#&gt; [43] splines_4.5.1         parallel_4.5.1        vctrs_0.6.5          \n#&gt; [46] V8_7.0.0              Matrix_1.7-4          sandwich_3.1-1       \n#&gt; [49] jsonlite_2.0.0        arrayhelpers_1.1-0    systemfonts_1.2.3    \n#&gt; [52] glue_1.8.0            codetools_0.2-20      distributional_0.5.0 \n#&gt; [55] lubridate_1.9.4       stringi_1.8.7         gtable_0.3.6         \n#&gt; [58] QuickJSR_1.8.0        htmltools_0.5.8.1     Brobdingnag_1.2-9    \n#&gt; [61] R6_2.6.1              textshaping_1.0.3     rprojroot_2.1.1      \n#&gt; [64] evaluate_1.0.5        lattice_0.22-7        backports_1.5.0      \n#&gt; [67] memoise_2.0.1         broom_1.0.9           snakecase_0.11.1     \n#&gt; [70] rstantools_2.5.0      coda_0.19-4.1         gridExtra_2.3        \n#&gt; [73] nlme_3.1-168          checkmate_2.3.3       xfun_0.53            \n#&gt; [76] zoo_1.8-14            pkgconfig_2.0.3",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Principi della visualizzazione dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/05_data_visualization.html#bibliografia",
    "href": "chapters/eda/05_data_visualization.html#bibliografia",
    "title": "18  Principi della visualizzazione dei dati",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nHealy, K. (2018). Data visualization: a practical introduction. Princeton University Press.\n\n\nIrizarry, R. A. (2024). Introduction to Data Science: Data Wrangling and Visualization with R. CRC Press.\n\n\nWickham, H., Çetinkaya-Rundel, M., & Grolemund, G. (2023). R for data science. \" O’Reilly Media, Inc.\".\n\n\nWilke, C. O. (2019). Fundamentals of data visualization: a primer on making informative and compelling figures. O’Reilly Media.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Principi della visualizzazione dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/06_loc_scale.html",
    "href": "chapters/eda/06_loc_scale.html",
    "title": "19  Indicatori di tendenza centrale e variabilità",
    "section": "",
    "text": "Introduzione\nLa visualizzazione grafica dei dati rappresenta il pilastro fondamentale di ogni analisi quantitativa. Grazie alle rappresentazioni grafiche adeguate, è possibile individuare importanti caratteristiche di una distribuzione, quali la simmetria o l’asimmetria, nonché la presenza di una o più mode. Successivamente, al fine di descrivere sinteticamente le principali caratteristiche dei dati, si rende necessario l’utilizzo di specifici indici numerici. In questo capitolo, verranno presentati i principali indicatori della statistica descrittiva.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Indicatori di tendenza centrale e variabilità</span>"
    ]
  },
  {
    "objectID": "chapters/eda/06_loc_scale.html#introduzione",
    "href": "chapters/eda/06_loc_scale.html#introduzione",
    "title": "19  Indicatori di tendenza centrale e variabilità",
    "section": "",
    "text": "Panoramica del capitolo\n\n\nTendenza centrale: Scelta della misura (media, mediana, moda) in base al tipo di dati e alla presenza di valori anomali.\n\nVariabilità: Interpretazione di range, varianza, deviazione standard e IQR per quantificare la dispersione dei dati attorno al valore centrale\n\n\n\n\n\n\n\nPrerequisiti\n\n\n\n\n\n\nLeggere “Most Psychological Researchers Assume Their Samples Are Ergodic: Evidence From a Year of Articles in Three Major Journals” (Speelman et al., 2024).\nStudiare l’?sec-apx-sums.\n\n\n\n\n\n\n\n\n\n\nPreparazione del Notebook\n\n\n\n\n\n\nhere::here(\"code\", \"_common.R\") |&gt; \n  source()\n\n# Load packages\nif (!requireNamespace(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyr, viridis, vcd)",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Indicatori di tendenza centrale e variabilità</span>"
    ]
  },
  {
    "objectID": "chapters/eda/06_loc_scale.html#indici-di-tendenza-centrale",
    "href": "chapters/eda/06_loc_scale.html#indici-di-tendenza-centrale",
    "title": "19  Indicatori di tendenza centrale e variabilità",
    "section": "\n19.1 Indici di tendenza centrale",
    "text": "19.1 Indici di tendenza centrale\nGli indici di tendenza centrale sono misure statistiche che cercano di rappresentare un valore tipico o centrale all’interno di un insieme di dati. Sono utilizzati per ottenere una comprensione immediata della distribuzione dei dati senza dover analizzare l’intero insieme. Gli indici di tendenza centrale sono fondamentali nell’analisi statistica, in quanto forniscono una sintesi semplice e comprensibile delle caratteristiche principali di un insieme di dati. I principali indici di tendenza centrale sono:\n\n\nMedia: La media è la somma di tutti i valori divisa per il numero totale di valori. È spesso utilizzata come misura generale di tendenza centrale, ma è sensibile agli estremi (valori molto alti o molto bassi).\n\nMediana: La mediana è il valore che divide l’insieme di dati in due parti uguali. A differenza della media, non è influenzata da valori estremi ed è quindi più robusta in presenza di outlier.\n\nModa: La moda è il valore che appare più frequentemente in un insieme di dati. In alcuni casi, può non essere presente o esserci più di una moda.\n\nLa scelta dell’indice di tendenza centrale appropriato dipende dalla natura dei dati e dall’obiettivo dell’analisi. Ad esempio, la mediana potrebbe essere preferita alla media se l’insieme di dati contiene valori anomali che potrebbero distorcere la rappresentazione centrale. La conoscenza e l’applicazione corretta di questi indici possono fornire una preziosa intuizione sulle caratteristiche centrali di una distribuzione di dati.\n\n19.1.1 Moda\nLa moda (\\(\\text{Mo}\\)) rappresenta il valore della variabile che compare con maggiore frequenza in una distribuzione. In altre parole, è il valore più ricorrente nei dati.\n\nNelle distribuzioni unimodali, esiste una sola moda, che coincide con il valore centrale della distribuzione più frequente.\n\nTuttavia, in alcune distribuzioni, possono emergere più di una moda, rendendole multimodali. In questi casi, la moda perde il suo significato di indicatore unico di tendenza centrale, poiché la presenza di più valori con frequenze elevate rende difficile individuare un singolo punto di riferimento.\n\n19.1.2 Mediana\nLa mediana (\\(\\tilde{x}\\)) corrisponde al valore che divide il campione in due metà: il 50% dei dati è inferiore o uguale alla mediana e il restante 50% è superiore o uguale. A differenza della media, la mediana è meno influenzata dai valori estremi, rendendola una misura particolarmente robusta in presenza di dati asimmetrici o outlier.\n\n19.1.3 Media\nLa media aritmetica di un insieme di valori rappresenta il punto centrale o il baricentro della distribuzione dei dati. È calcolata come la somma di tutti i valori divisa per il numero totale di valori, ed è espressa dalla formula:\n\\[\n\\bar{x}=\\frac{1}{n}\\sum_{i=1}^n x_i,\n\\tag{19.1}\\] dove \\(x_i\\) rappresenta i valori nell’insieme, \\(n\\) è il numero totale di valori, e \\(\\sum\\) indica la sommatoria.\n\n19.1.3.1 Calcolo della media con R\n\nPer calcolare la media di un piccolo numero di valori in R, possiamo utilizzare la somma di questi valori e dividerla per il numero totale di elementi. Consideriamo ad esempio i valori 12, 44, 21, 62, 24:\n\n(12 + 44 + 21 + 62 + 24) / 5\n#&gt; [1] 32.6\n\novvero\n\nx &lt;- c(12, 44, 21, 62, 24)\nmean(x)\n#&gt; [1] 32.6\n\n\n19.1.3.2 Media spuntata\nLa media spuntata, indicata come \\(\\bar{x}_t\\) o trimmed mean, è un metodo di calcolo della media che prevede l’eliminazione di una determinata percentuale di dati estremi prima di effettuare la media aritmetica. Solitamente, viene eliminato il 10% dei dati, ovvero il 5% all’inizio e alla fine della distribuzione. Per ottenere la media spuntata, i dati vengono ordinati in modo crescente, \\(x_1 \\leq x_2 \\leq x_3 \\leq \\dots \\leq x_n\\), e quindi viene eliminato il primo 5% e l’ultimo 5% dei dati nella sequenza ordinata. Infine, la media spuntata è calcolata come la media aritmetica dei dati rimanenti. Questo approccio è utile quando ci sono valori anomali o quando la distribuzione è asimmetrica e la media aritmetica non rappresenta adeguatamente la tendenza centrale dei dati.\n\n\n\n\n\n\nEsempio\n\n\n\n\n\nPer illustrare la media spuntata, utilizzeremo i dati del Progetto Natsal, contenuti nel file sexual-partners.csv.\nNegli anni ’80, con la crescente preoccupazione per l’AIDS, le autorità sanitarie del Regno Unito si resero conto della mancanza di dati affidabili sui comportamenti sessuali della popolazione. In particolare, vi erano dubbi sulla frequenza con cui le persone cambiavano partner, sul numero di partner simultanei e sulle pratiche sessuali adottate. Questa conoscenza era essenziale per prevedere la diffusione delle malattie sessualmente trasmissibili nella società e per pianificare adeguatamente i servizi sanitari. Tuttavia, si faceva ancora riferimento ai dati raccolti da Alfred Kinsey negli Stati Uniti negli anni ’40, che non tenevano conto della rappresentatività del campione.\nA partire dalla fine degli anni ’80, vennero dunque avviati nel Regno Unito e negli Stati Uniti ampi e rigorosi studi sui comportamenti sessuali, nonostante una forte opposizione in alcuni ambienti. Nel Regno Unito, il governo guidato da Margaret Thatcher ritirò il proprio sostegno a un’importante indagine sui comportamenti sessuali all’ultimo momento. Fortunatamente, i ricercatori riuscirono a ottenere finanziamenti da enti benefici, dando vita al National Sexual Attitudes and Lifestyles Survey (Natsal). Da allora, questa indagine viene condotta ogni dieci anni, a partire dal 1990. La terza rilevazione, denominata Natsal-3, è stata effettuata intorno al 2010.\nPoniamoci il problema di descrivere la tendenza centrale con la media spuntata per i dati contenuti nel file sexual-partners.csv, separatamente per maschi e femmine. Il dataset fornisce la distribuzione del numero totale dichiarato di partner sessuali di sesso opposto nella vita per uomini e donne di età compresa tra 35 e 44 anni. I dati provengono dal sondaggio Natsal-3 e corrispondono a un totale di 796 uomini e 1193 donne.\nProcediamo all’importazione dei dati per iniziare l’analisi.\n\nsexual_partners &lt;- rio::import(here::here(\"data\", \"sexual-partners.csv\"))\n\nEsaminiamo alcune righe prese a caso dal data frame sexual_partners:\n\nsexual_partners[sample(1:nrow(sexual_partners), size = 10, replace = FALSE), ]\n#&gt;      Gender NumPartners\n#&gt; 1119  Woman           3\n#&gt; 1789  Woman          12\n#&gt; 1913  Woman          22\n#&gt; 1941  Woman          29\n#&gt; 826   Woman           1\n#&gt; 1827  Woman          14\n#&gt; 736     Man          40\n#&gt; 1803  Woman          12\n#&gt; 1296  Woman           4\n#&gt; 1685  Woman          10\n\nLa colonna Gender riporta il genere del rispondente e la colonna NumPartners il numero di partner sessuali di sesso opposto dichiarati.\nCalcoliamo la media spuntata per gli uomini:\n\nsex_partners_men &lt;- sexual_partners[sexual_partners$Gender == \"Man\", \"NumPartners\"]\nmean(sex_partners_men, trim = 0.10, na.rm = TRUE)\n#&gt; [1] 10.6\n\nCalcoliamo la media spuntata per le donne:\n\nsex_partners_women &lt;- sexual_partners[sexual_partners$Gender == \"Woman\", \"NumPartners\"]\nmean(sex_partners_women, trim = 0.10, na.rm = TRUE)\n#&gt; [1] 6.01\n\n\n\n\n\n19.1.3.3 Proprietà della media\nUna proprietà fondamentale della media è che la somma degli scarti di ciascun valore dalla media è zero:\n\\[\n\\sum_{i=1}^n (x_i - \\bar{x}) = 0.\\notag\n\\tag{19.2}\\] Infatti,\n\\[\n\\begin{aligned}\n\\sum_{i=1}^n (x_i - \\bar{x}) &= \\sum_i x_i - \\sum_i \\bar{x}\\notag\\\\\n&= \\sum_i x_i - n \\bar{x}\\notag\\\\\n&= \\sum_i x_i - \\sum_i x_i = 0.\\notag\n\\end{aligned}\n\\] Questa proprietà implica che i dati sono equamente distribuiti intorno alla media.\nIn R abbiamo:\n\nsum(x - mean(x))\n#&gt; [1] -7.11e-15\n\n\n\n\n\n\n\nNota sulla notazione scientifica\n\n\n\n\n\nQuando in un terminale viene visualizzato un numero come -7.105e-15 in notazione scientifica, esso corrisponde a \\(-7.105 \\cdot 10^{-15}\\), che è effettivamente zero nel contesto dei calcoli numerici.\nQuesta approssimazione è una conseguenza diretta della precisione finita dei calcolatori. I sistemi digitali, infatti, rappresentano i numeri reali attraverso una codifica in virgola mobile (floating point), che comporta inevitabili errori di arrotondamento. La ragione risiede nell’impossibilità di memorizzare numeri reali con precisione assoluta.\nLo standard IEEE 754 a doppia precisione (64 bit), ampiamente utilizzato, suddivide la memoria in tre componenti:\n\n1 bit per il segno (positivo/negativo),\n\n11 bit per l’esponente (intervallo di scala),\n\n52 bit per la mantissa (o significando), che definisce le cifre significative.\n\nGrazie a questa struttura, è possibile rappresentare numeri con una precisione di circa 15-17 cifre decimali. Tuttavia, qualsiasi valore non esprimibile in formato binario entro questi limiti subisce un troncamento o un arrotondamento, generando piccole discrepanze rispetto al risultato teorico.\n\n\n\n\n19.1.3.4 La media come centro di gravità dell’istogramma\nLa media aritmetica può essere interpretata come il centro di gravità o il punto di equilibrio della distribuzione dei dati. In termini fisici, il centro di gravità è il punto in cui la massa di un sistema è equilibrata o concentrata.\nIn termini statistici, possiamo considerare la media come il punto in cui la distribuzione dei dati è in equilibrio. Ogni valore dell’insieme di dati può essere visto come un punto materiale con una massa proporzionale al suo valore. Se immaginiamo questi punti disposti su una linea, con valori più grandi a destra e più piccoli a sinistra, la media corrisponderà esattamente al punto in cui la distribuzione sarebbe in equilibrio.\n\n\n\n\n\n\nPrincipio dei minimi quadrati\n\n\n\n\n\nIl metodo dei minimi quadrati afferma che la posizione della media minimizza la somma dei quadrati delle distanze dai dati. Matematicamente, ciò significa che la somma dei quadrati degli scarti tra ciascun valore osservato e la media è minima. Questo principio è alla base dell’analisi statistica della regressione e conferma il ruolo della media come centro di gravità della distribuzione dei dati.\nSimulazione. Utilizziamo una simulazione per verificare questo principio, calcolando la somma dei quadrati degli scarti per diversi valori e visualizzando il risultato con ggplot2.\n\n# Definizione dell'intervallo di valori da testare\nnrep &lt;- 10000\nM &lt;- seq(20, 40, length.out = nrep)\nres &lt;- rep(NA, nrep)\n\n# Calcolo della somma dei quadrati degli scarti per ciascun valore di M\nfor (i in 1:nrep) {\n  res[i] = sum((x - M[i])^2)\n}\n\n# Identificazione del valore minimo\nmin_index &lt;- which.min(res)\nmin_M &lt;- M[min_index]\n\n# Creazione del dataframe per ggplot\ndf &lt;- data.frame(M, res)\n\ndf |&gt; \n  ggplot(aes(x = M, y = res)) +\n  geom_line(color = \"blue\") +\n  geom_vline(xintercept = min_M, linetype = \"dashed\", color = \"red\") +\n  labs(\n    x = \"Valore di M\",\n    y = \"Somma dei quadrati degli scarti\"\n  ) \n\n\n\n\n\n\n\nStampiamo il minimo:\n\nmin_M\n#&gt; [1] 32.6\n\nConfronto con la media:\n\nmean(x)\n#&gt; [1] 32.6\n\nOsserviamo che il valore di M che minimizza la somma dei quadrati degli scarti coincide con la media dei dati, confermando il principio dei minimi quadrati.\n\n\n\n\n19.1.3.5 Le proporzioni sono medie\nSe una collezione consiste solo di uni e zeri, allora la somma della collezione è il numero di uni in essa, e la media della collezione è la proporzione di uni.\n\nzero_one &lt;- c(1, 1, 1, 0)\nresult &lt;- mean(zero_one)\nresult\n#&gt; [1] 0.75\n\nÈ possibile sostituire 1 con il valore booleano True e 0 con False:\n\nmean(c(TRUE, TRUE, TRUE, FALSE))\n#&gt; [1] 0.75\n\n\n19.1.3.6 Limiti della media aritmetica\nLa media aritmetica, tuttavia, ha alcune limitazioni: non sempre è l’indice più adeguato per descrivere accuratamente la tendenza centrale della distribuzione, specialmente quando si verificano asimmetrie o valori anomali (outlier). In queste situazioni, è più indicato utilizzare la mediana o la media spuntata (come spiegheremo successivamente).\n\n\n\n\n\n\nCome descrivere la tendenza centrale in distribuzioni asimmetriche\n\n\n\n\n\nGli indici di tendenza centrale – moda, mediana e media – assumono significati molto diversi quando la distribuzione dei dati è asimmetrica. Per esempio, consideriamo i dati del Progetto Natsal (sexual-partners.csv), che riportano il numero di partner sessuali di sesso opposto dichiarati da uomini e donne (35–44 anni).\n\nEsplorazione dei dati.\n\nEstraggo dieci righe a caso:\n\nsexual_partners[sample(1:nrow(sexual_partners), 10), ]\n#&gt;      Gender NumPartners\n#&gt; 448     Man           9\n#&gt; 1167  Woman           3\n#&gt; 1971  Woman          40\n#&gt; 687     Man          30\n#&gt; 1708  Woman          10\n#&gt; 490     Man          10\n#&gt; 721     Man          35\n#&gt; 248     Man           5\n#&gt; 1745  Woman          10\n#&gt; 1055  Woman           2\n\nIl dataset contiene due colonne principali:\n\n\nGender: genere del rispondente,\n\nNumPartners: numero di partner sessuali dichiarati.\n\nVediamo quanti soggetti ci sono in ciascun gruppo:\n\nsexual_partners |&gt; \n  group_by(Gender) |&gt; \n  summarise(count = n())\n#&gt; # A tibble: 2 × 2\n#&gt;   Gender count\n#&gt;   &lt;chr&gt;  &lt;int&gt;\n#&gt; 1 Man      796\n#&gt; 2 Woman   1193\n\nIl numero massimo riportato è molto alto (oltre 500), ma per chiarezza limitiamo l’analisi a valori ≤ 50:\n\nsexual_partners |&gt; \n  group_by(Gender) |&gt; \n  summarise(maximum = max(NumPartners))\n#&gt; # A tibble: 2 × 2\n#&gt;   Gender maximum\n#&gt;   &lt;chr&gt;    &lt;int&gt;\n#&gt; 1 Man        501\n#&gt; 2 Woman      550\n\n\nVisualizzazione.\n\nCalcoliamo e rappresentiamo la distribuzione dei partner (≤50) separatamente per genere:\n\nsexual_partners_truncated &lt;- sexual_partners |&gt; \n  filter(NumPartners &lt;= 50)\n\npercentage_data &lt;- sexual_partners_truncated %&gt;%\n  group_by(Gender, NumPartners) %&gt;%\n  summarise(Count = n(), .groups = \"drop\") %&gt;%\n  group_by(Gender) %&gt;%\n  mutate(Percentage = Count / sum(Count) * 100)\n\ny_max &lt;- max(percentage_data$Percentage)\n\ngender_labels &lt;- c(\"Man\" = \"Uomini 35–44\", \"Woman\" = \"Donne 35–44\")\n\npercentage_data |&gt;\n  ggplot(aes(NumPartners, Percentage, fill = Gender)) +\n  geom_col(position = \"dodge\", color = \"black\") +\n  facet_wrap(~Gender, labeller = labeller(Gender = gender_labels)) +\n  scale_y_continuous(limits = c(0, y_max)) +\n  labs(x = \"Numero di partner sessuali dichiarati\", y = \"Percentuale\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nLa distribuzione risulta altamente asimmetrica positiva: molti soggetti dichiarano pochi partner, pochi soggetti valori molto alti.\n\nIndici di tendenza centrale.\n\nCalcoliamo media, mediana e moda per ciascun genere:\n\nget_mode &lt;- function(x) {\n  tbl &lt;- table(x)\n  as.numeric(names(tbl)[which.max(tbl)])\n}\n\nsexual_partners_truncated |&gt; \n  group_by(Gender) |&gt; \n  summarise(\n    media   = mean(NumPartners, na.rm = TRUE),\n    mediana = median(NumPartners, na.rm = TRUE),\n    moda    = get_mode(NumPartners)\n  )\n#&gt; # A tibble: 2 × 4\n#&gt;   Gender media mediana  moda\n#&gt;   &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 Man    11.4        7     1\n#&gt; 2 Woman   7.51       5     1\n\n\nInterpretazione.\n\n\n\nMedia: più alta di mediana e moda → influenzata dalla coda lunga a destra.\n\nMediana: valore centrale, meno influenzata da estremi → misura più robusta.\n\nModa: valore più frequente (1 partner), ma spesso poco rappresentativa in distribuzioni molto sparse.\n\n\nConclusioni pratiche.\n\n\nIn distribuzioni asimmetriche, la mediana è in genere l’indice di tendenza centrale più affidabile.\nÈ utile riportare anche media e moda per evidenziare le differenze.\nLa descrizione numerica va sempre accompagnata da una visualizzazione grafica (istogrammi, boxplot) per cogliere l’asimmetria e i valori estremi.\nUna descrizione completa richiede anche misure di dispersione, che vedremo nella sezione successiva.\n\n\n\n\n\n19.1.3.7 La media come rappresentazione della psicologia umana: un’arma a doppio taglio?\nLa media è uno degli strumenti statistici più semplici e intuitivi che i ricercatori utilizzano per sintetizzare i dati. È un indice di tendenza centrale familiare e intuitivo: se chiediamo a un gruppo di persone la loro età e calcoliamo la media, otteniamo un valore che sintetizza in un’unica cifra l’informazione disponibile. Ma cosa significa, in realtà, “riassumere” i dati con la media? E soprattutto, questa operazione ha senso quando si studiano i processi psicologici e il comportamento umano?\nIn molte discipline scientifiche, il concetto di media è utile perché descrive fenomeni che tendono a essere stabili e uniformi. Per esempio, se misuriamo l’altezza di un gruppo di persone, possiamo aspettarci che la distribuzione sia approssimativamente normale e che la media offra una stima ragionevole di un valore tipico. Tuttavia, la mente umana e i processi psicologici non funzionano come il sistema cardiovascolare o i muscoli. Ogni persona ha esperienze uniche che plasmano le sue risposte, i suoi pensieri e le sue emozioni.\nUn problema centrale, sollevato da Speelman & McGann (2013), riguarda l’implicita assunzione che ci sia un vero valore sottostante ai processi psicologici che possiamo stimare attraverso la media, come se il comportamento umano fosse determinato da meccanismi identici in ogni individuo, con le differenze attribuibili solo a “rumore” sperimentale. Questo approccio, tipico della psicologia sperimentale tradizionale, assume che testare un gruppo di persone e mediare i loro risultati ci permetta di rivelare la struttura comune della mente umana. Ma questa assunzione è davvero giustificata?\n\n19.1.3.8 La fallacia ergodica e l’illusione dell’universalità\nUn errore metodologico frequente nella psicologia è la cosiddetta fallacia ergodica, ovvero l’errata convinzione che le caratteristiche medie di un gruppo possano essere automaticamente applicate ai singoli individui che lo compongono (Speelman et al., 2024). Questo equivoco nasce dall’idea che la media descriva un valore “tipico” valido per tutti, senza considerare le differenze individuali o le variazioni nel tempo.\nImmaginiamo di studiare la felicità di un gruppo di persone nel corso di una settimana e di calcolare la media dei loro punteggi di benessere giornalieri. Se lunedì una persona ha un punteggio di 2 (molto infelice), mercoledì 5 (moderatamente felice) e sabato 8 (molto felice), il suo punteggio medio sarà 5. Tuttavia, questo valore intermedio non rappresenta in alcun modo la realtà soggettiva vissuta da quella persona nei singoli giorni. Lo stesso problema si pone quando si usano le medie per descrivere abilità cognitive, tratti di personalità o stati emotivi: la media può nascondere fluttuazioni e differenze individuali fondamentali per comprendere la psicologia umana.\nIl rischio, come sottolineato da Molden & Dweck (2006), è che il nostro desiderio di trovare universalità nei processi cognitivi ci porti a enfatizzare somiglianze tra le persone, ignorando le variazioni individuali che possono essere altrettanto, se non più, informative. Per esempio, due studenti con lo stesso punteggio medio in un test di memoria potrebbero aver ottenuto quel risultato in modi completamente diversi: uno potrebbe aver avuto prestazioni costantemente nella media, mentre l’altro potrebbe aver avuto picchi di eccellenza alternati a difficoltà estreme.\n\n19.1.3.9 La media: uno strumento da usare con cautela\nQuesti problemi non significano che la media sia inutile in psicologia. È un indicatore potente e spesso informativo, ma deve essere interpretato con cautela. In particolare, non può essere usata per fare inferenze sui singoli individui senza considerare altre misure, come la varianza e la deviazione standard, che ci dicono quanto i dati siano dispersi intorno alla media.\nIn psicologia, comprendere la variabilità è tanto importante quanto individuare una tendenza centrale. Se vogliamo davvero capire il comportamento umano, dobbiamo chiederci non solo qual è il valore medio? ma anche quanto variano i dati? e cosa ci dice questa variabilità sulle differenze individuali? Nella prossima sezione, esamineremo questi concetti e vedremo come la varianza e la deviazione standard ci aiutano a catturare le differenze che la media, da sola, non può rivelare.\n\n\n\n\n\n\nLa scelta della misura di tendenza centrale\n\n\n\n\n\nLa selezione della misura di tendenza centrale più adeguata è un aspetto fondamentale dell’analisi statistica. Tale scelta deve basarsi sulla natura dei dati a disposizione, sulla loro distribuzione e sulla potenziale presenza di valori anomali o asimmetrie. Comprendere le caratteristiche distintive di ciascun indicatore è cruciale per una rappresentazione corretta e significativa dell’informazione.\nLa Moda trova la sua principale applicazione nei dati di tipo categoriale o nominale, dove rappresenta l’unica misura di tendenza centrale calcolabile. La sua utilità è massima nelle distribuzioni unimodali, ovvero quelle in cui un unico valore emerge con frequenza predominante. Al contrario, in presenza di distribuzioni multimodali, caratterizzate da più picchi di frequenza, la moda perde di significatività, poiché l’identificazione di un singolo valore rappresentativo diventa impossibile. Nei dati continui, infine, la moda può spesso non essere definita o risultare poco informativa.\nLa Media Aritmetica è l’indicatore più comune e fornisce una stima eccellente della tendenza centrale per distribuzioni simmetriche e prive di valori anomali. Tuttavia, la media presenta una critica debolezza: la sua estrema sensibilità ai valori estremi. In distribuzioni asimmetriche o contaminata da outlier, la media subisce uno spostamento marcato verso la coda della distribuzione, finendo per non rappresentare più fedelmente la maggior parte dei dati.\nLa Media Spuntata si propone come una valida alternativa alla media tradizionale quando si sospetta la presenza di valori anomali. Questo indicatore, calcolato escludendo una certa percentuale dei valori più estremi (ad esempio, il 5% per ogni coda), offre un compromesso vantaggioso. Rispetto alla media, è molto più robusta agli outlier; rispetto alla mediana, tiene conto di un maggior numero di osservazioni, preservando parte dell’informazione contenuta nei dati.\nLa Mediana, essendo definita come il valore centrale di una distribuzione ordinata, possiede una robustezza intrinseca ai valori anomali. La sua natura posizionale fa sì che essa non venga influenzata dall’entità numerica dei dati, ma solo dal loro rango. Questo la rende la misura d’elezione per distribuzioni fortemente asimmetriche o con outlier. Inoltre, è l’indicatore preferibile per i dati ordinali, dove le distanze tra le categorie non sono definite numericamente.\nLinee guida per una scelta consapevole.\nPer distribuzioni simmetriche e unimodali, la media aritmetica costituisce generalmente la scelta ottimale, in quanto ne coglie appieno il centro. Quando si affrontano distribuzioni asimmetriche o dataset contenenti valori anomali, la mediana è da preferire per la sua innata resistenza a tali distorsioni. La media spuntata rappresenta un’ottima opzione quando si desidera attenuare l’impatto di pochi outlier senza rinunciare completamente all’informazione che forniscono. Per i dati categoriali (nominali), la moda è l’unica misura applicabile, a patto che la distribuzione sia unimodale. In scenari complessi come le distribuzioni multimodali, nessuna singola misura è sufficiente a descrivere la tendenza centrale. In questi casi, è indispensabile affiancare agli indicatori statistici una visualizzazione grafica (ad esempio un istogramma) e una descrizione analitica dei diversi picchi presenti.\nIn sintesi, non esiste una misura migliore in assoluto. La media è potente per dati simmetrici, la mediana è lo strumento robusto per proteggersi dalle distorsioni, la media spuntata è un utile compromesso e la moda è essenziale per i dati categoriali. La scelta deve sempre essere guidata da una attenta valutazione preliminare della distribuzione dei dati.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Indicatori di tendenza centrale e variabilità</span>"
    ]
  },
  {
    "objectID": "chapters/eda/06_loc_scale.html#la-variabilità-nei-dati-psicologici",
    "href": "chapters/eda/06_loc_scale.html#la-variabilità-nei-dati-psicologici",
    "title": "19  Indicatori di tendenza centrale e variabilità",
    "section": "\n19.2 La variabilità nei dati psicologici",
    "text": "19.2 La variabilità nei dati psicologici\nNei fenomeni psicologici e comportamentali, la variabilità è una caratteristica intrinseca. Ad esempio, se misuriamo il livello di stress percepito da una persona più volte nella stessa giornata, è raro osservare lo stesso valore anche utilizzando strumenti identici. Allo stesso modo, un questionario standardizzato sull’autostima somministrato a un gruppo di studenti universitari restituirà punteggi differenti per ciascun partecipante. Anche registrando i tempi di reazione in un compito cognitivo, noteremo fluttuazioni sia tra individui diversi sia nelle prestazioni dello stesso individuo in prove ripetute.\nQuesta dispersione sistematica non è un “rumore” da ignorare, ma un elemento informativo cruciale. L’analisi statistica in psicologia ha infatti uno scopo duplice: da un lato, quantificare la variabilità; dall’altro, identificarne le origini. Differenze individuali, contesto ambientale, errori di misurazione o interazioni tra fattori sono solo alcune delle possibili fonti che contribuiscono alla variazione osservata.\nIn questa sezione esploreremo:\n\n\nLa scomposizione della variabilità in componenti spiegate (attribuibili a fattori noti, come un intervento sperimentale) e non spiegate (legate a elementi casuali o non controllati).\n\n\nStrumenti per descriverla, sia attraverso rappresentazioni grafiche (boxplot, istogrammi) sia mediante indici numerici (differenza interquartile, varianza, deviazione standard).\n\nComprendere la variabilità non è un esercizio tecnico, ma un passo fondamentale per interpretare fenomeni complessi come le differenze di personalità, le oscillazioni emotive o l’efficacia di una terapia. Ogni modello psicologico, infatti, deve fare i conti con questa dimensione dinamica e multideterminata dei dati.\n\n19.2.1 Quantili\nAccanto alle misure di tendenza centrale, i quantili descrivono la posizione relativa di un’osservazione in una distribuzione. Se media, mediana e moda individuano un valore “tipico”, i quantili rispondono invece a una domanda diversa: qual è il valore al di sotto del quale si colloca una determinata proporzione dei dati?\n\n19.2.1.1 Definizione\nIl quantile di ordine \\(p\\) (\\(0 &lt; p &lt; 1\\)) è il valore \\(q_p = x_{(k)}\\), dove \\(x_{(k)}\\) rappresenta il \\(k\\)-esimo elemento dei dati ordinati in senso crescente e \\(k = \\lceil p \\cdot n \\rceil\\), con \\(n\\) numero totale di osservazioni e \\(\\lceil \\cdot \\rceil\\) funzione di arrotondamento per eccesso. Questo è il cosiddetto quantile non interpolato. Quando \\(p \\cdot n\\) non è intero, si ricorre di norma all’interpolazione lineare tra due osservazioni consecutive: è il metodo implementato nei principali software statistici.\nPer chiarire, consideriamo i dati ordinati \\({15, 20, 23, 25, 28, 30, 35, 40, 45, 50}\\). Il 30° percentile (\\(p=0.3\\)) si calcola come \\(k = \\lceil 0.3 \\cdot 10 \\rceil = 3\\), dunque \\(q_{0.3} = 23\\).\n\n19.2.1.2 Percentili e quartili\nUn caso particolare sono i percentili, che suddividono la distribuzione in cento parti uguali. Il 25° percentile (o primo quartile \\(Q_1\\)) lascia al di sotto di sé un quarto dei dati, il 50° percentile corrisponde alla mediana e il 75° percentile (terzo quartile \\(Q_3\\)) delimita i tre quarti inferiori della distribuzione.\n\n19.2.1.3 Esempio applicativo\nPer illustrare l’uso dei quantili, consideriamo la variabile NumPartners, distinta per genere. Calcoliamo il 10° e il 90° percentile.\n\n# Quantili per gli uomini\nquantile(\n  sexual_partners[sexual_partners$Gender == \"Man\", \"NumPartners\"], \n  probs = c(0.1, 0.9)\n  )\n#&gt;  10%  90% \n#&gt;  1.0 34.5\n\n# Quantili per le donne\nquantile(\n  sexual_partners[sexual_partners$Gender == \"Woman\", \"NumPartners\"], \n  probs = c(0.1, 0.9)\n  )\n#&gt; 10% 90% \n#&gt;   1  18\n\nI risultati mostrano che, tra gli uomini, il 10% ha dichiarato al massimo un partner, mentre il 10% con i valori più elevati supera i 34 partner. Tra le donne, il 10° percentile coincide ancora con un partner, ma il 90° percentile non va oltre 18.\nQuesta differenza indica che le distribuzioni dei due gruppi condividono una base simile (molti individui con pochi partner) ma divergono nella coda superiore: negli uomini, pochi soggetti con valori estremi spingono la distribuzione verso destra, generando una maggiore asimmetria positiva.\n\n19.2.1.4 Misure di dispersione basate sui quantili\nI quantili possono essere utilizzati anche per costruire indici di variabilità che non fanno ipotesi sulla forma della distribuzione. La misura più semplice è l’intervallo di variazione, pari alla differenza tra valore massimo e minimo. Questo indice è immediato da calcolare, ma dipende esclusivamente dagli estremi e risulta quindi molto sensibile agli outlier.\n\nx &lt;- c(12, 18, 20, 22, 25, 28, 30, 35)\nrange(x)\n#&gt; [1] 12 35\ndiff(range(x))\n#&gt; [1] 23\n\nNell’esempio l’intervallo è 23, valore che descrive l’ampiezza complessiva dei dati ma non la loro distribuzione interna.\nUn indicatore più robusto è la differenza interquartile (IQR), che misura la distanza fra il terzo e il primo quartile, racchiudendo così il 50% centrale dei dati.\n\nIQR(x)\n#&gt; [1] 9\n\nSe in un gruppo di studenti i quartili sono \\(Q_1 = 25\\) e \\(Q_3 = 40\\), l’IQR è pari a 15: significa che metà dei punteggi si colloca in un intervallo di 15 unità. Questo indice riduce l’influenza dei valori estremi, anche se non rappresenta l’intera dispersione della distribuzione.\nIn sintesi, l’intervallo di variazione e l’IQR offrono due prospettive complementari: il primo fornisce un’idea immediata dell’ampiezza totale dei dati, il secondo descrive la variabilità tipica della parte centrale della distribuzione. Entrambi hanno limiti che rendono necessario affiancarli a misure più complete, come la varianza e la deviazione standard, di cui parleremo nella sezione successiva.\n\n19.2.2 La varianza\nLa varianza è una delle misure di dispersione più utilizzate in statistica perché tiene conto di tutte le osservazioni e descrive quanto i valori si discostano dalla loro media. Formalmente, se abbiamo \\(n\\) osservazioni \\(x_1, x_2, \\dots, x_n\\) e indichiamo con \\(\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i\\) la loro media, la varianza (in versione descrittiva) si calcola così:\n\\[\nS^2 = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2.\n\\tag{19.3}\\] In altre parole, per trovare la varianza:\n\ncalcoliamo la media di tutti i valori (\\(\\bar{x}\\)),\nsottraiamo la media a ciascun valore, ottenendo così lo scarto \\((x_i - \\bar{x})\\),\neleviamo ogni scarto al quadrato, per rendere positivi i valori ed enfatizzare gli scostamenti più grandi,\ninfine, facciamo la media di questi quadrati.\n\nMaggiore è la varianza, maggiore è la variabilità (o dispersione) dei dati rispetto alla media. Al contrario, una varianza prossima allo zero indica che le osservazioni sono molto vicine tra loro e quasi coincidenti con la media.\n\nNota su popolazione e campione. Spesso, nell’analisi di dati campionari, la varianza viene calcolata usando \\(\\frac{1}{n-1}\\) al denominatore al posto di \\(\\frac{1}{n}\\). In questo modo otteniamo una stima corretta (non distorta) della varianza della popolazione. Nel contesto della formula sopra riportata, invece, stiamo calcolando la varianza descrittiva (o popolazione completa).\n\n\n\n\n\n\n\nEsempio\n\n\n\n\n\nImmaginiamo di aver misurato il numero di ore di studio giornaliere di un piccolo gruppo di partecipanti a un esperimento di psicologia. I dati raccolti sono:\n\\[\nx = \\{3,\\, 1,\\, 4,\\, 2\\}.\n\\]\nPasso 1: Calcolo della media\n\\[\n\\bar{x} = \\frac{3 + 1 + 4 + 2}{4} = \\frac{10}{4} = 2.5.\n\\]\nPasso 2: Scarti dalla media\n\nPer il primo valore (\\(x_1 = 3\\)): \\(3 - 2.5 = 0.5\\)\n\nPer il secondo valore (\\(x_2 = 1\\)): \\(1 - 2.5 = -1.5\\)\n\nPer il terzo valore (\\(x_3 = 4\\)): \\(4 - 2.5 = 1.5\\)\n\nPer il quarto valore (\\(x_4 = 2\\)): \\(2 - 2.5 = -0.5\\)\n\n\nPasso 3: Quadrati degli scarti\n\n\\((0.5)^2 = 0.25\\)\n\\((-1.5)^2 = 2.25\\)\n\\((1.5)^2 = 2.25\\)\n\\((-0.5)^2 = 0.25\\)\n\nPasso 4: Calcolo della varianza\nFacciamo la media di questi valori:\n\\[\nS^2 = \\frac{0.25 + 2.25 + 2.25 + 0.25}{4} = \\frac{5}{4} = 1.25.\n\\]\nDunque la varianza è 1.25.\n\n\n\n\n19.2.2.1 Interpretazione\nNel caso dell’esempio precedente relativo alle ore di studio giornaliere, una varianza pari a 1.25 indica che le ore di studio giornaliere si discostano, in media, di 1.25 unità quadrate dalla media di 2.5 ore. Per comprendere meglio l’ordine di grandezza di questa dispersione, solitamente si fa riferimento alla deviazione standard, che è la radice quadrata della varianza. In questo caso, \\(\\sqrt{1.25} \\approx 1.12\\) ore.\n\nSe la varianza (o la deviazione standard) fosse stata molto più grande, avremmo dedotto che gli studenti del campione presentano abitudini di studio molto diverse.\nAl contrario, se la varianza fosse prossima a 0, significherebbe che quasi tutti studiano un numero di ore molto simile a 2.5.\n\n19.2.2.2 Calcolo in R\nSe volessimo effettuare in R i calcoli relativi all’esempio sulle ore di studio, potremmo fare così:\n\n# Dati\nx &lt;- c(3, 1, 4, 2)\n\n# Calcolo manuale della media\nmedia_x &lt;- mean(x)\n\n# Calcolo manuale della varianza secondo la formula descrittiva\nvarianza_descr &lt;- mean((x - media_x)^2)\nvarianza_descr\n#&gt; [1] 1.25\n# [1] 1.25\n\n# Calcolo della varianza con la funzione var() di R\n# (Attenzione: per default var() usa n-1 al denominatore)\nvarianza_campionaria &lt;- var(x)\nvarianza_campionaria\n#&gt; [1] 1.67\n# [1] 1.666667\n\nOsserviamo che var(x) dà un valore di circa 1.67 perché R, di default, calcola la varianza campionaria (con \\(n-1\\) al denominatore). Se vogliamo la varianza descrittiva (come nella formula con \\(n\\) al denominatore), usiamo la nostra varianza_descr.\nIn sintesi, la varianza fornisce un modo per quantificare quanto siano diverse tra loro le osservazioni. Nel caso dell’esempio sulle ore di studio, abbiamo visto che i valori, pur non essendo tutti identici, non mostrano una dispersione eccessiva (la varianza è 1.25). Se i comportamenti di studio fossero estremamente diversificati (per esempio, se qualcuno studiasse 0 ore al giorno e qualcun altro 10), la varianza sarebbe molto più elevata, indicando una marcata eterogeneità nel campione.\n\n19.2.2.3 Stima della varianza della popolazione\nSi noti il denominatore della formula della varianza. Nell’Equazione 19.3, ho utilizzato \\(n\\) come denominatore (l’ampiezza campionaria, ovvero il numero di osservazioni nel campione). In questo modo, otteniamo la varianza come statistica descrittiva del campione. Tuttavia, è possibile utilizzare \\(n-1\\) come denominatore alternativo:\n\\[\ns^2 = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2 .\n\\tag{19.4}\\] In questo secondo caso, otteniamo la varianza come stimatore della varianza della popolazione. Si può dimostrare che l’Equazione 19.4 fornisce una stima corretta (ovvero, non distorta) della varianza della popolazione da cui abbiamo ottenuto il campione, mentre l’Equazione 19.3 fornisce (in media) una stima troppo piccola della varianza della popolazione. Si presti attenzione alla notazione: \\(S^2\\) rappresenta la varianza come statistica descrittiva, mentre \\(s^2\\) rappresenta la varianza come stimatore.\n\n\n\n\n\n\nSimulazione\n\n\n\n\n\nPer illustrare questo punto, svolgiamo una simulazione. Consideriamo la distribuzione dei punteggi del quoziente di intelligenza (QI). I valori del QI seguono una particolare distribuzione chiamata distribuzione normale (v. Capitolo 20), con media 100 e deviazione standard 15. La forma di questa distribuzione è illustrata nella figura seguente.\n\n# Define parameters\nx &lt;- seq(100 - 4 * 15, 100 + 4 * 15, by = 0.001)\nmu &lt;- 100\nsigma &lt;- 15\n\n# Compute the PDF\npdf &lt;- dnorm(x, mean = mu, sd = sigma)\n\n# Plot using ggplot2\ndata &lt;- tibble(x = x, pdf = pdf)\nggplot(data, aes(x = x, y = pdf)) +\n  geom_line() +\n  labs(\n    x = \"QI\", \n    y = \"Densità\"\n  ) \n\n\n\n\n\n\n\nSupponiamo di estrarre un campione casuale di 4 osservazioni dalla popolazione del quoziente di intelligenza – in altre parole, supponiamo di misurare il quoziente di intelligenza di 4 persone prese a caso dalla popolazione.\n\nset.seed(123) \nx &lt;- rnorm(4, mean = 100, sd = 15)\nprint(x)\n#&gt; [1]  91.6  96.5 123.4 101.1\n\nCalcoliamo la varianza usando \\(n\\) al denominatore. Si noti che la vera varianza del quoziente di intelligenza è \\(15^2\\) = 225.\n\nvar(x)\n#&gt; [1] 197\n\nConsideriamo ora 10 campioni casuali del QI, ciascuno di ampiezza 4.\n\nmu &lt;- 100\nsigma &lt;- 15\nsize &lt;- 4\nniter &lt;- 10\nrandom_samples &lt;- list()\n\nset.seed(123) \n\nfor (i in 1:niter) {\n  one_sample &lt;- rnorm(size, mean = mu, sd = sigma)\n  random_samples[[i]] &lt;- one_sample\n}\n\nIl primo campione è\n\nrandom_samples[1]\n#&gt; [[1]]\n#&gt; [1]  91.6  96.5 123.4 101.1\n\nIl decimo campione è\n\nrandom_samples[10]\n#&gt; [[1]]\n#&gt; [1] 108.3  99.1  95.4  94.3\n\nStampiamo i valori di tutti i 10 campioni.\n\nrs &lt;- do.call(rbind, random_samples)\nrs\n#&gt;        [,1]  [,2]  [,3]  [,4]\n#&gt;  [1,]  91.6  96.5 123.4 101.1\n#&gt;  [2,] 101.9 125.7 106.9  81.0\n#&gt;  [3,]  89.7  93.3 118.4 105.4\n#&gt;  [4,] 106.0 101.7  91.7 126.8\n#&gt;  [5,] 107.5  70.5 110.5  92.9\n#&gt;  [6,]  84.0  96.7  84.6  89.1\n#&gt;  [7,]  90.6  74.7 112.6 102.3\n#&gt;  [8,]  82.9 118.8 106.4  95.6\n#&gt;  [9,] 113.4 113.2 112.3 110.3\n#&gt; [10,] 108.3  99.1  95.4  94.3\n\nPer ciascun campione (ovvero, per ciascuna riga della matrice precedente), calcoliamo la varianza usando la formula con \\(n\\) al denominatore. Otteniamo così 10 stime della varianza della popolazione del QI.\n\nx_var &lt;- apply(rs, 1, var)  # Applica la funzione var su ciascuna riga\nprint(x_var)\n#&gt;  [1] 196.94 337.54 168.55 218.68 333.48  34.52 264.38 234.08   1.97  40.47\n\nNotiamo due cose:\n\nle stime sono molto diverse tra loro; questo fenomeno è noto con il nome di variabilità campionaria;\nin media le stime sono troppo piccole.\n\nPer aumentare la sicurezza riguardo al secondo punto menzionato in precedenza, ripeteremo la simulazione utilizzando un numero di iterazioni maggiore.\n\nmu &lt;- 100\nsigma &lt;- 15\nsize &lt;- 4\nniter &lt;- 10000\nrandom_samples &lt;- list()\n\nset.seed(123) # Replace 123 with your desired seed for reproducibility\n\nfor (i in 1:niter) {\n  one_sample &lt;- rnorm(size, mean = mu, sd = sigma)\n  random_samples[[i]] &lt;- one_sample\n}\n\nrs &lt;- do.call(rbind, random_samples)\nx_var &lt;- apply(rs, 1, var) * (size - 1) / size  # Adjust for population variance (ddof = 0)\n\nEsaminiamo la distribuzione dei valori ottenuti.\n\n# Create a data frame for plotting\ndata &lt;- data.frame(x_var = x_var)\n\n# Plot the histogram using ggplot2\nggplot(data, aes(x = x_var)) +\n  geom_histogram(fill = \"lightblue\", bins = 30, color = \"black\") +\n  scale_x_continuous(limits = c(0, 1500)) +\n  scale_y_continuous(limits = c(0, 2250)) +\n  labs(\n    x = \"Varianza\", \n    y = \"Frequenza\"\n  )\n\n\n\n\n\n\n\nLa stima più verosimile della varianza del QI è dato dalla media di questa distribuzione.\n\nmean(x_var)\n#&gt; [1] 169\n\nSi noti che il nostro spospetto è stato confermato: il valore medio della stima della varianza ottenuta con l’Equazione 19.3 è troppo piccolo rispetto al valore corretto di \\(15^2 = 225\\).\nRipetiamo ora la simulazione usando la formula della varianza con \\(n-1\\) al denominatore.\n\nset.seed(123) \n\nmu &lt;- 100\nsigma &lt;- 15\nsize &lt;- 4\nniter &lt;- 10000\nrandom_samples &lt;- list()\n\nfor (i in 1:niter) {\n  one_sample &lt;- rnorm(size, mean = mu, sd = sigma)\n  random_samples[[i]] &lt;- one_sample\n}\n\nrs &lt;- do.call(rbind, random_samples)\nx_var &lt;- apply(rs, 1, var)  # ddof = 1 is default for var in R\n\nEsaminiamo la distribuzione dei valori ottenuti.\n\n# Create a data frame for plotting\ndata &lt;- data.frame(x_var = x_var)\n\n# Plot the histogram using ggplot2\nggplot(data, aes(x = x_var)) +\n  geom_histogram(fill = \"lightblue\", bins = 30, color = \"black\") +\n  scale_x_continuous(limits = c(0, 1500)) +\n  scale_y_continuous(limits = c(0, 2250)) +\n  labs(\n    x = \"Varianza Corretta\", \n    y = \"Frequenza\"\n  )\n\n\n\n\n\n\n\nNel secondo caso, se utilizziamo \\(n-1\\) come denominatore per calcolare la stima della varianza, il valore atteso di questa stima è molto vicino al valore corretto di 225. Se il numero di campioni fosse infinito, i due valori sarebbero identici.\n\nmean(x_var)\n#&gt; [1] 225\n\nIn conclusione, le due formule della varianza hanno scopi diversi.\n\nLa formula della varianza con \\(n\\) al denominatore viene utilizzata come statistica descrittiva per descrivere la variabilità di un particolare campione di osservazioni.\nD’altro canto, la formula della varianza con \\(n-1\\) al denominatore viene utilizzata come stimatore per ottenere la migliore stima della varianza della popolazione da cui quel campione è stato estratto.\n\n\n\n\n\n19.2.3 Deviazione standard\nPer interpretare la varianza in modo più intuitivo, si può calcolare la deviazione standard (o scarto quadratico medio o scarto tipo) prendendo la radice quadrata della varianza. La deviazione standard è espressa nell’unità di misura originaria dei dati, a differenza della varianza che è espressa nel quadrato dell’unità di misura dei dati. La deviazione standard fornisce una misura della dispersione dei dati attorno alla media, rendendo più facile la comprensione della variabilità dei dati.\nLa deviazione standard (o scarto quadratico medio, o scarto tipo) è definita come:\n\\[\ns^2 = \\sqrt{(n-1)^{-1} \\sum_{i=1}^n (x_i - \\bar{x})^2}.\n\\tag{19.5}\\] Quando tutte le osservazioni sono uguali, \\(s = 0\\), altrimenti \\(s &gt; 0\\).\n\n\n\n\n\n\nIl termine standard deviation è stato introdotto in statistica da Pearson nel 1894 assieme alla lettera greca \\(\\sigma\\) che lo rappresenta. Il termine italiano “deviazione standard” ne è la traduzione più utilizzata nel linguaggio comune; il termine dell’Ente Nazionale Italiano di Unificazione è tuttavia “scarto tipo”, definito come la radice quadrata positiva della varianza.\n\n\n\nLa deviazione standard \\(s\\) dovrebbe essere utilizzata solo quando la media è una misura appropriata per descrivere il centro della distribuzione, ad esempio nel caso di distribuzioni simmetriche. Tuttavia, è importante tener conto che, come la media \\(\\bar{x}\\), anche la deviazione standard è fortemente influenzata dalla presenza di dati anomali, ovvero pochi valori che si discostano notevolmente dalla media rispetto agli altri dati della distribuzione. In presenza di dati anomali, la deviazione standard può risultare ingannevole e non rappresentare accuratamente la variabilità complessiva della distribuzione. Pertanto, è fondamentale considerare attentamente il contesto e le caratteristiche dei dati prima di utilizzare la deviazione standard come misura di dispersione. In alcune situazioni, potrebbe essere più appropriato ricorrere a misure di dispersione robuste o ad altre statistiche descrittive per caratterizzare la variabilità dei dati in modo più accurato e affidabile.\n\n19.2.3.1 Interpretazione\nLa deviazione standard misura la dispersione dei dati rispetto alla media aritmetica. In termini semplici, indica quanto, in media, ciascun valore osservato si discosta dalla media del campione. Anche se è simile allo scarto semplice medio campionario (la media dei valori assoluti degli scarti rispetto alla media), la deviazione standard utilizza lo scarto quadratico medio e produce un valore leggermente diverso.\n\nEsempio 19.1 Per verificare l’interpretazione della deviazione standard, utilizziamo i punteggi relativi alle ore di studio di un piccolo numero di studenti.\n\nx &lt;- c(3, 1, 4, 2)\n\nstd_x &lt;- sqrt(var(x) * 3 / 4)\nstd_x\n#&gt; [1] 1.12\n\nLa deviazione standard calcolata è 1.12. Questo valore ci dice che, in media, ciascun punteggio si discosta di circa 1.12 ore dalla media aritmetica delle ore di studio di questo gruppo di studenti.\n\nValore più alto: indica maggiore dispersione dei dati intorno alla media.\nValore più basso: i dati sono più concentrati vicino alla media.\n\nSe calcoliamo anche lo scarto semplice medio campionario per confronto, otteniamo:\n\nmean(abs(x - mean(x)))\n#&gt; [1] 1\n\nI due valori (deviazione standard e scarto semplice medio) sono simili ma non identici, a causa delle diverse definizioni matematiche.\n\n\n19.2.4 Varianza spiegata e non spiegata\nLa varianza, come abbiamo visto, misura quanto i dati si disperdono attorno alla media. Un concetto fondamentale nei modelli statistici lineari è la distinzione tra varianza spiegata e varianza non spiegata, che ci permette di valutare quanto bene un modello teorico riesca a chiarire la variabilità osservata nei dati.\n\n19.2.4.1 Decomposizione della varianza\nQuando osserviamo un fenomeno (ad esempio i risultati di un test), troviamo inevitabilmente differenze tra individui. Queste differenze possono essere suddivise in due componenti principali:\n\n\nvarianza spiegata: la parte di variabilità che può essere attribuita a fattori identificati e misurabili;\n\nvarianza non spiegata: la parte rimanente di variabilità che non è chiarita dai fattori considerati.\n\nFormalmente, questa decomposizione può essere espressa come:\n\\[\n\\sum_{i=1}^{n}(Y_i - \\bar{Y})^2 = \\sum_{i=1}^{n}(\\hat{Y}_i - \\bar{Y})^2 + \\sum_{i=1}^{n}(Y_i - \\hat{Y}_i)^2 ,\n\\] dove:\n\n\n\\(Y_i\\) sono i dati osservati,\n\n\\(\\bar{Y}\\) è la media dei dati osservati,\n\n\\(\\hat{Y}_i\\) sono i valori attesi (previsti) dal modello teorico.\n\nIntuitivamente:\n\nla varianza totale (lato sinistro della formula) rappresenta la dispersione complessiva dei dati attorno alla loro media;\nla varianza spiegata (primo termine a destra) indica quanto bene i valori previsti dal modello descrivono il comportamento dei dati;\nla varianza non spiegata (secondo termine a destra) riflette ciò che il modello non riesce a prevedere.\n\n\n\n\n\n\n\nSimulazione\n\n\n\n\n\nSupponiamo di analizzare i punteggi di un esame universitario di “Psicometria” ottenuti da 200 studenti. La nostra teoria indica che i punteggi dipendano da:\n\nOre settimanali dedicate allo studio;\nPresenza o assenza di “paura della matematica” (math anxiety) (Barroso et al., 2021).\n\nNello specifico, ipotizziamo:\n\nuna relazione positiva tra ore di studio e punteggio ottenuto;\nuna riduzione del 30% del punteggio per chi presenta paura della matematica rispetto agli altri studenti, a parità di ore di studio.\n\nEcco la simulazione in R:\n\nset.seed(123)\n\n# Simuliamo i dati per 200 studenti\nn &lt;- 200\nore_studio &lt;- runif(n, min = 2, max = 15) |&gt; round()\npaura_mat &lt;- rbinom(n, 1, prob = 0.3)\n\nk &lt;- 2  # Ogni ora di studio corrisponde a circa 2 punti\n\n# Calcoliamo i punteggi attesi, limitati a 30 punti massimo\npunteggio_atteso &lt;- ore_studio * k * ifelse(paura_mat == 1, 0.7, 1) |&gt; round()\npunteggio_atteso &lt;- ifelse(punteggio_atteso &gt; 30, 30, punteggio_atteso)\n\n# Generiamo punteggi reali aggiungendo casualità (tra 0 e 30 punti)\npunteggio_reale &lt;- (punteggio_atteso + rnorm(n, mean = 0, sd = 3)) |&gt; round()\npunteggio_reale &lt;- pmin(pmax(punteggio_reale, 0), 30)\n\n# Creiamo il dataset finale\ndata &lt;- data.frame(ore_studio, paura_mat, punteggio_atteso, punteggio_reale)\nhead(data)\n#&gt;   ore_studio paura_mat punteggio_atteso punteggio_reale\n#&gt; 1          6         0               12              19\n#&gt; 2         12         1               24              28\n#&gt; 3          7         0               14              13\n#&gt; 4         13         0               26              28\n#&gt; 5         14         0               28              27\n#&gt; 6          3         1                6               5\n\nCalcoliamo ora la decomposizione della varianza usando le formule indicate:\n\n# Media dei punteggi reali\nmedia_reale &lt;- mean(data$punteggio_reale)\n\n# Calcolo delle componenti della varianza\nvarianza_totale &lt;- mean((data$punteggio_reale - media_reale)^2)\nvarianza_spiegata &lt;- mean((data$punteggio_atteso - media_reale)^2)\nvarianza_non_spiegata &lt;- mean((data$punteggio_reale - data$punteggio_atteso)^2)\n\n# Risultati\nc(varianza_totale, varianza_spiegata, varianza_non_spiegata)\n#&gt; [1] 60.37 51.65  8.29\n\nInterpretazione dei risultati:\n\nla varianza totale indica quanto in generale i punteggi differiscono tra loro,\nla varianza spiegata rappresenta quanto della variabilità totale può essere attribuita ai fattori teorici (ore di studio e paura della matematica),\nla varianza non spiegata evidenzia la variabilità residua che il modello non riesce a cogliere.\n\nLa proporzione di varianza spiegata è data dal rapporto:\n\nprop_spiegata &lt;- varianza_spiegata / varianza_totale\nprop_spiegata\n#&gt; [1] 0.856\n\nQuesta proporzione è sempre compresa tra 0 e 1:\n\nvalori vicini a 1 indicano che il modello è efficace nel descrivere i dati;\nvalori vicini a 0 suggeriscono che il modello non cattura adeguatamente la realtà osservata.\n\nQuesta decomposizione della varianza è uno strumento cruciale per valutare l’efficacia delle teorie e dei modelli statistici. Approfondiremo ulteriormente questi aspetti nel capitolo dedicato ai modelli di regressione (v. ?sec-linear-models-biv-model-frequentist).\n\n\n\n\n19.2.5 Deviazione mediana assoluta\nLa deviazione mediana assoluta (MAD) è una misura robusta di dispersione basata sulla mediana. È definita come la mediana dei valori assoluti delle deviazioni dei dati rispetto alla mediana:\n\\[\n\\text{MAD} = \\text{median} \\left( |X_i - \\text{median}(X)| \\right)\n\\tag{19.6}\\] La MAD è particolarmente utile per analizzare dati contenenti outlier o distribuzioni asimmetriche, poiché è meno influenzata dai valori estremi rispetto alla deviazione standard.\n\n19.2.5.1 Relazione tra MAD e deviazione standard in una distribuzione normale\nQuando i dati seguono una distribuzione normale (gaussiana), esiste una relazione approssimativa tra MAD e deviazione standard. La MAD può essere convertita in una stima della deviazione standard moltiplicandola per una costante di 1.4826:\n\\[\n\\sigma \\approx k \\times \\text{MAD},\n\\] dove:\n\n\n\\(\\sigma\\) è la deviazione standard,\nMAD è la Mediana della Deviazione Assoluta,\n\n\\(k\\) è una costante che, per una distribuzione normale, è tipicamente presa come circa 1.4826.\n\nQuesta costante deriva dalla proprietà della distribuzione normale, in cui circa il 50% dei valori si trova entro 0.6745 deviazioni standard dalla media.\nLa formula completa per convertire la MAD in una stima della deviazione standard in una distribuzione normale è:\n\\[\n\\sigma \\approx 1.4826 \\times \\text{MAD}\n\\] Questa relazione è utile per stimare la deviazione standard in modo più robusto, specialmente quando si sospetta la presenza di outlier o si ha a che fare con campioni piccoli. Di conseguenza, molti software restituiscono il valore MAD moltiplicato per questa costante per fornire un’indicazione più intuitiva della variabilità dei dati. Tuttavia, è importante notare che questa relazione si mantiene accurata solo per le distribuzioni che sono effettivamente normali. In presenza di distribuzioni fortemente asimmetriche o con elevati outlier, la deviazione standard e la MAD possono fornire indicazioni molto diverse sulla variabilità dei dati.\n\n\n\n\n\n\nDimostrazione numerica\n\n\n\n\n\nPer verificare questo principio, usiamo un campione di dati simulati dalla distribuzione del QI:\n\nqi &lt;- rnorm(200, 100, 15)\n1.4826 * median(abs(qi - median(qi)), na.rm = TRUE)\n#&gt; [1] 14.1\n\nOtteniamo un valore che è simile alla deviazione standard calcolata con:\n\nsqrt(\n  var(qi) * (length(qi) - 1) / length(qi)\n)\n#&gt; [1] 14.4\n\nCiò conferma la relazione tra MAD e deviazione standard in distribuzioni gaussiane.\nSe invece usiamo dei dati non normali, l’approssimazione non è buona:\n\nset.seed(123) \ny &lt;- rchisq(200, 1)\n1.4826 * median(abs(y - median(y)))\n#&gt; [1] 0.474\n\n\nsqrt(\n  var(y) * (length(y) - 1) / length(y)\n)\n#&gt; [1] 1.41\n\n\n\n\n\n19.2.5.2 Quando usare deviazione standard e MAD\n\nDeviazione standard: È la misura più appropriata per dati normalmente distribuiti e situazioni in cui l’obiettivo è descrivere la dispersione dei dati rispetto alla media. Tuttavia, è sensibile ai valori anomali (outlier).\nDeviazione mediana assoluta: È ideale quando i dati sono non normali, asimmetrici o contengono outlier. La MAD è più robusta poiché utilizza la mediana anziché la media e non è influenzata da valori estremi.\n\n19.2.6 Indici di variabilità relativi\nA volte può essere necessario confrontare la variabilità di grandezze incommensurabili, ovvero di caratteri misurati con differenti unità di misura. In queste situazioni, le misure di variabilità descritte in precedenza diventano inadeguate poiché dipendono dall’unità di misura utilizzata. Per superare questo problema, si ricorre a specifici numeri adimensionali chiamati indici relativi di variabilità.\nIl più importante di questi indici è il coefficiente di variazione (\\(C_v\\)), definito come il rapporto tra la deviazione standard (\\(\\sigma\\)) e la media dei dati (\\(\\bar{x}\\)):\n\\[\nC_v = \\frac{\\sigma}{\\bar{x}}.\n\\tag{19.7}\\] Il coefficiente di variazione è un numero puro e permette di confrontare la variabilità di distribuzioni con unità di misura diverse.\nUn altro indice relativo di variabilità è la differenza interquartile rapportata a uno dei tre quartili (primo quartile, terzo quartile o mediana). Questo indice è definito come:\n\\[\n\\frac{x_{0.75} - x_{0.25}}{x_{0.25}}, \\qquad \\frac{x_{0.75} - x_{0.25}}{x_{0.75}}, \\qquad \\frac{x_{0.75} - x_{0.25}}{x_{0.50}}.\n\\] Questi indici relativi di variabilità forniscono una misura adimensionale della dispersione dei dati, rendendo possibile il confronto tra grandezze con diverse unità di misura e facilitando l’analisi delle differenze di variabilità tra i dati.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Indicatori di tendenza centrale e variabilità</span>"
    ]
  },
  {
    "objectID": "chapters/eda/06_loc_scale.html#riflessioni-conclusive",
    "href": "chapters/eda/06_loc_scale.html#riflessioni-conclusive",
    "title": "19  Indicatori di tendenza centrale e variabilità",
    "section": "Riflessioni conclusive",
    "text": "Riflessioni conclusive\nLe statistiche descrittive forniscono strumenti essenziali per sintetizzare e comprendere i dati raccolti in psicologia e nelle scienze sociali. Le misure di tendenza centrale, come la media, la mediana e la moda, ci permettono di individuare un valore tipico o rappresentativo di una distribuzione, facilitando la sintesi e l’interpretazione generale dei dati raccolti. Parallelamente, gli indici di dispersione, come la deviazione standard, la varianza e l’intervallo interquartile, offrono informazioni cruciali sulla variabilità, mostrandoci quanto i singoli dati siano vicini o distanti da questa tendenza centrale.\nTuttavia, è fondamentale riflettere attentamente sulle implicazioni teoriche e metodologiche che accompagnano l’uso di queste misure. In particolare, è importante considerare il rischio della fallacia ergodica, ovvero l’errata convinzione che i risultati ottenuti da medie e statistiche aggregate possano automaticamente applicarsi ai singoli individui. Nella pratica psicologica, infatti, ogni persona è caratterizzata da una notevole variabilità intra- e inter-individuale, che spesso non può essere adeguatamente rappresentata da semplici indicatori aggregati.\nLe statistiche descrittive rappresentano quindi un primo e fondamentale passo nella comprensione dei dati psicologici, ma devono essere integrate da analisi più approfondite e attente alle differenze individuali. L’uso critico e consapevole di questi strumenti statistici ci consente di evitare generalizzazioni eccessive, fornendo una visione più accurata e realistica dei fenomeni psicologici e comportamentali che studiamo.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Indicatori di tendenza centrale e variabilità</span>"
    ]
  },
  {
    "objectID": "chapters/eda/06_loc_scale.html#esercizi",
    "href": "chapters/eda/06_loc_scale.html#esercizi",
    "title": "19  Indicatori di tendenza centrale e variabilità",
    "section": "Esercizi",
    "text": "Esercizi\n\n\n\n\n\n\nProblemi\n\n\n\n\n\nParte 1: Domande Teoriche\n\n\nDefinizione e comprensione dei concetti\n\nSpiega la differenza tra media, mediana e moda.\nIn quali situazioni la mediana fornisce una misura della tendenza centrale migliore rispetto alla media?\nPerché la media è sensibile ai valori estremi?\nQuali sono i vantaggi della deviazione mediana assoluta (MAD) rispetto alla deviazione standard?\n\n\n\nInterpretazione della variabilità\n\nSpiega il concetto di varianza e la sua interpretazione.\nQual è la differenza tra varianza e deviazione standard?\nDescrivi in quali casi l’utilizzo del coefficiente di variazione è più appropriato rispetto alla deviazione standard.\nQuali sono i limiti della moda come indice di tendenza centrale?\n\n\n\nParte 2: Calcoli Manuali\n\n\nCalcolo della media, mediana e moda\n\n\nConsidera i seguenti punteggi totali della SWLS che sono stati raccolti in un campione di studenti. Calcola manualmente:\n\nLa media\nLa mediana\nLa moda\nIl range\n\n\n\n\n\nCalcolo della varianza e della deviazione standard\n\nUsando gli stessi dati dell’esercizio precedente, calcola:\n\nLa varianza\nLa deviazione standard\nLa deviazione mediana assoluta (MAD)\n\n\n\n\n\nParte 3: Esercizi con R\n\n\nAnalisi descrittiva con R\n\nCarica il dataset swls_scores.csv contenente i punteggi SWLS degli studenti.\nCalcola media, mediana e moda utilizzando R.\nCalcola la varianza e la deviazione standard utilizzando le funzioni appropriate in R.\n\nCodice suggerito:\nlibrary(tidyverse)\nlibrary(rio)\n\n# Caricamento del dataset\ndf &lt;- import(\"swls_scores.csv\")\n\n# Calcolo delle statistiche descrittive\nmean(df$swls_total)\nmedian(df$swls_total)\n\n# Moda (funzione personalizzata)\nget_mode &lt;- function(x) {\n  tbl &lt;- table(x)\n  as.numeric(names(tbl)[which.max(tbl)])\n}\nget_mode(df$swls_total)\n\n# Varianza e deviazione standard\nvar(df$swls_total)\nsd(df$swls_total)\n\n# Deviazione mediana assoluta\nmad(df$swls_total)\n\n\nVisualizzazione della distribuzione dei dati\n\nCrea un istogramma dei punteggi totali della SWLS.\nAggiungi una linea verticale che rappresenti la media e una che rappresenti la mediana.\n\nCodice suggerito:\nggplot(df, aes(x = swls_total)) +\n  geom_histogram(binwidth = 2, fill = \"blue\", alpha = 0.5, color = \"black\") +\n  geom_vline(aes(xintercept = mean(swls_total)), color = \"red\", linetype = \"dashed\", size = 1) +\n  geom_vline(aes(xintercept = median(swls_total)), color = \"green\", linetype = \"dotted\", size = 1) +\n  labs(title = \"Distribuzione dei punteggi SWLS\", x = \"Punteggio SWLS\", y = \"Frequenza\")\n\n\nParte 4: Domande di Comprensione\n\n\nAnalisi concettuale\n\nPerché la media aritmetica può essere considerata il “baricentro” della distribuzione dei dati?\nSe aggiungiamo un valore estremo al dataset, quale delle misure di tendenza centrale subirà il maggior impatto?\nIn quali situazioni la varianza campionaria è preferibile rispetto alla varianza della popolazione?\nQual è la relazione tra la deviazione standard e la varianza?\nFornisci un’interpretazione intuitiva della deviazione standard.\nDiscuti le differenze e le somiglianze tra la deviazione standard e MAD. Usa queste informazioni per ridescrivere in maniera intuitiva il significato di deviazione standard.\n\n\n\n\n\n\n\n\n\n\n\n\nSoluzioni\n\n\n\n\n\nPoniamo che i valori SWLS siano [ 18, 22, 26, 19, 24, 30, 26, 22, 18, 28, 21 ].\n\nMedia: \\[ \\bar{x} = \\frac{18 + 22 + 26 + 19 + 24 + 30 + 26 + 22 + 18 + 28 + 21}{11} = 23.36 \\]\nMediana: Ordinando i dati: [ 18, 18, 19, 21, 22, 22, 24, 26, 26, 28, 30 ] La mediana è il valore centrale: \\(22\\)\nModa: Il valore più frequente è 22 (appare due volte).\nVarianza: \\[ s^2 = \\frac{\\sum (x_i - \\bar{x})^2}{n-1} = 13.96 \\]\nDeviazione Standard: \\[ s = \\sqrt{13.96} = 3.73 \\]\nMAD: \\[ \\text{MAD} = \\text{mediana}(|X_i - \\text{mediana}(X)|) = 4 \\]\n\nSoluzioni con R\nI risultati eseguendo il codice R:\n\n\nMedia: 23.36\n\nMediana: 22\n\nModa: 22\n\nVarianza: 13.96\n\nDeviazione standard: 3.73\n\nMAD: 4\n\nSoluzioni alle Domande di Comprensione\n\nLa media è il baricentro poiché minimizza la somma degli scarti quadrati.\nLa media è più influenzata dai valori estremi rispetto alla mediana.\nLa varianza campionaria corregge la sottostima della varianza popolazionale.\nLa deviazione standard è la radice quadrata della varianza, consentendo un’interpretazione del risultato sulla scala dei dati grezzi.\nLa deviazione standard è simile, ma non identica, al valore medio degli scarti assoluti tra ciascun valore della distribuzione e la media. In altre parole, rappresenta la “distanza tipica” media tra le osservazioni e il valore medio della distribuzione.\nLa deviazione standard e il MAD (Median Absolute Deviation, o scarto medio assoluto) sono entrambi misure di variabilità che descrivono quanto i valori in un insieme di dati si discostino dal centro della distribuzione. Tuttavia, presentano alcune importanti differenze e somiglianze.\n\nSomiglianze\n\nEntrambe le misure quantificano la dispersione dei dati attorno a un punto centrale.\nSia la deviazione standard che il MAD utilizzano lo scarto (la differenza tra ciascun valore e un punto centrale) per calcolare la variabilità.\n\nDifferenze\n\n\nPunto centrale usato: La deviazione standard si basa sulla media aritmetica, mentre il MAD si basa sulla mediana.\n\nTrattamento degli scarti: Nella deviazione standard, gli scarti vengono elevati al quadrato prima di essere mediati, quindi la radice quadrata viene applicata al risultato finale. Questo processo penalizza maggiormente gli scarti più grandi, rendendo la deviazione standard più sensibile agli outlier. Il MAD, invece, considera semplicemente il valore assoluto degli scarti, rendendolo meno influenzato dagli estremi.\n\nSensibilità agli outlier: Poiché la deviazione standard dipende dai quadrati degli scarti, è più sensibile alle osservazioni estreme (outlier). Il MAD, essendo basato sulla mediana, è una misura più robusta e resiste meglio alla presenza di valori anomali.\n\nRidescrizione Intuitiva della Deviazione Standard\n\nLa deviazione standard può essere vista come una misura della “dispersione tipica” dei dati attorno alla media, ma con un’enfasi particolare sugli scarti più grandi. Immagina di prendere ogni valore del dataset, calcolarne la distanza dalla media, amplificare queste distanze attraverso il quadrato, poi trovare una sorta di “distanza media” ponderata. Questo processo dà maggiore peso agli scarti più grandi, fornendo così una visione della variabilità che tiene conto sia delle fluttuazioni ordinarie sia di eventuali valori estremi. In sintesi, mentre il MAD offre una visione più resistente e diretta della variabilità centrata sulla mediana, la deviazione standard fornisce una misura più dettagliata e sensibile alla forma complessiva della distribuzione, inclusi i suoi possibili outliers.\n\n\n\n\n\n\n\n\n\n\nInformazioni sull’ambiente di sviluppo\n\n\n\n\n\n\nsessionInfo()\n#&gt; R version 4.5.1 (2025-06-13)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.6.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] grid      stats     graphics  grDevices utils     datasets  methods  \n#&gt; [8] base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] vcd_1.4-13            viridis_0.6.5         viridisLite_0.4.2    \n#&gt;  [4] pillar_1.11.0         tinytable_0.13.0      patchwork_1.3.2      \n#&gt;  [7] ggdist_3.3.3          tidybayes_3.0.7       bayesplot_1.14.0     \n#&gt; [10] ggplot2_3.5.2         reliabilitydiag_0.2.1 priorsense_1.1.1     \n#&gt; [13] posterior_1.6.1       loo_2.8.0             rstan_2.32.7         \n#&gt; [16] StanHeaders_2.32.10   brms_2.22.0           Rcpp_1.1.0           \n#&gt; [19] sessioninfo_1.2.3     conflicted_1.2.0      janitor_2.2.1        \n#&gt; [22] matrixStats_1.5.0     modelr_0.1.11         tibble_3.3.0         \n#&gt; [25] dplyr_1.1.4           tidyr_1.3.1           rio_1.2.3            \n#&gt; [28] here_1.0.1           \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;  [1] gridExtra_2.3         inline_0.3.21         sandwich_3.1-1       \n#&gt;  [4] rlang_1.1.6           magrittr_2.0.3        multcomp_1.4-28      \n#&gt;  [7] snakecase_0.11.1      compiler_4.5.1        systemfonts_1.2.3    \n#&gt; [10] vctrs_0.6.5           stringr_1.5.1         pkgconfig_2.0.3      \n#&gt; [13] arrayhelpers_1.1-0    fastmap_1.2.0         backports_1.5.0      \n#&gt; [16] labeling_0.4.3        utf8_1.2.6            rmarkdown_2.29       \n#&gt; [19] ragg_1.5.0            purrr_1.1.0           xfun_0.53            \n#&gt; [22] cachem_1.1.0          jsonlite_2.0.0        broom_1.0.9          \n#&gt; [25] parallel_4.5.1        R6_2.6.1              stringi_1.8.7        \n#&gt; [28] RColorBrewer_1.1-3    lmtest_0.9-40         lubridate_1.9.4      \n#&gt; [31] estimability_1.5.1    knitr_1.50            zoo_1.8-14           \n#&gt; [34] R.utils_2.13.0        pacman_0.5.1          Matrix_1.7-4         \n#&gt; [37] splines_4.5.1         timechange_0.3.0      tidyselect_1.2.1     \n#&gt; [40] abind_1.4-8           codetools_0.2-20      curl_7.0.0           \n#&gt; [43] pkgbuild_1.4.8        lattice_0.22-7        withr_3.0.2          \n#&gt; [46] bridgesampling_1.1-2  coda_0.19-4.1         evaluate_1.0.5       \n#&gt; [49] survival_3.8-3        RcppParallel_5.1.11-1 tensorA_0.36.2.1     \n#&gt; [52] checkmate_2.3.3       stats4_4.5.1          distributional_0.5.0 \n#&gt; [55] generics_0.1.4        rprojroot_2.1.1       rstantools_2.5.0     \n#&gt; [58] scales_1.4.0          xtable_1.8-4          glue_1.8.0           \n#&gt; [61] emmeans_1.11.2-8      tools_4.5.1           data.table_1.17.8    \n#&gt; [64] mvtnorm_1.3-3         QuickJSR_1.8.0        colorspace_2.1-1     \n#&gt; [67] nlme_3.1-168          cli_3.6.5             textshaping_1.0.3    \n#&gt; [70] svUnit_1.0.8          Brobdingnag_1.2-9     V8_7.0.0             \n#&gt; [73] gtable_0.3.6          R.methodsS3_1.8.2     digest_0.6.37        \n#&gt; [76] TH.data_1.1-4         htmlwidgets_1.6.4     farver_2.1.2         \n#&gt; [79] R.oo_1.27.1           memoise_2.0.1         htmltools_0.5.8.1    \n#&gt; [82] lifecycle_1.0.4       MASS_7.3-65",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Indicatori di tendenza centrale e variabilità</span>"
    ]
  },
  {
    "objectID": "chapters/eda/06_loc_scale.html#bibliografia",
    "href": "chapters/eda/06_loc_scale.html#bibliografia",
    "title": "19  Indicatori di tendenza centrale e variabilità",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nBarroso, C., Ganley, C. M., McGraw, A. L., Geer, E. A., Hart, S. A., & Daucourt, M. C. (2021). A meta-analysis of the relation between math anxiety and math achievement. Psychological Bulletin, 147(2), 134–168.\n\n\nMolden, D. C., & Dweck, C. S. (2006). Finding\" meaning\" in psychology: a lay theories approach to self-regulation, social perception, and social development. American Psychologist, 61(3), 192–203.\n\n\nSpeelman, C. P., & McGann, M. (2013). How mean is the mean? Frontiers in Psychology, 4, 451.\n\n\nSpeelman, C. P., Parker, L., Rapley, B. J., & McGann, M. (2024). Most Psychological Researchers Assume Their Samples Are Ergodic: Evidence From a Year of Articles in Three Major Journals. Collabra: Psychology, 10(1).",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Indicatori di tendenza centrale e variabilità</span>"
    ]
  },
  {
    "objectID": "chapters/eda/07_introduction_normal_distribution.html",
    "href": "chapters/eda/07_introduction_normal_distribution.html",
    "title": "20  Introduzione alla distribuzione normale",
    "section": "",
    "text": "Introduzione\nIn questo capitolo forniremo un primo sguardo alla distribuzione normale, che sarà trattata in modo più approfondito nel ?sec-prob-cont-prob-distr. Introduciamo la distribuzione normale a questo punto poiché essa spiega in modo chiaro perché, in molte analisi, media e deviazione standard siano impiegate come principali descrittori di una distribuzione.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Introduzione alla distribuzione normale</span>"
    ]
  },
  {
    "objectID": "chapters/eda/07_introduction_normal_distribution.html#introduzione",
    "href": "chapters/eda/07_introduction_normal_distribution.html#introduzione",
    "title": "20  Introduzione alla distribuzione normale",
    "section": "",
    "text": "Panoramica del capitolo\n\nChe cos’è la distribuzione normale?\nCome si costruisce e come si interpreta la distribuzione normale normalizzata.\nCosa sono e come si interpretanto i diagrammi quantile-quantile.\nDistribuzione normale e statistiche descrittive.\n\n\n\n\n\n\n\nPrerequisiti\n\n\n\n\n\n\nLeggere il capitolo Exploring numerical data di Introduction to Modern Statistics (2e) di Mine Çetinkaya-Rundel e Johanna Hardin.\nLeggere il capitolo Distributions di Introduction to Data Science (Irizarry, 2024).\n\n\n\n\n::: {.callout-caution collapse=true=“Preparazione del Notebook”}\n\nhere::here(\"code\", \"_common.R\") |&gt; \n  source()\n\n# Load packages\nif (!requireNamespace(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(ggbeeswarm, dslabs, gridExtra)\n\n:::",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Introduzione alla distribuzione normale</span>"
    ]
  },
  {
    "objectID": "chapters/eda/07_introduction_normal_distribution.html#sec-normal-distribution",
    "href": "chapters/eda/07_introduction_normal_distribution.html#sec-normal-distribution",
    "title": "20  Introduzione alla distribuzione normale",
    "section": "\n20.1 La distribuzione normale",
    "text": "20.1 La distribuzione normale\nNel Capitolo 17, abbiamo visto come gli istogrammi e i grafici di densità forniscano utili riassunti visivi di una distribuzione. In questo capitolo, ci chiediamo se sia possibile riassumere una distribuzione in modo ancora più sintetico. Spesso si fa riferimento a media e deviazione standard come statistiche riassuntive fondamentali: in sostanza, un riassunto in due numeri. Per comprendere appieno il ruolo di questi valori, dobbiamo prima capire come è definita la distribuzione normale.\nLa distribuzione normale, nota anche come curva a campana o distribuzione gaussiana, è uno dei concetti matematici più conosciuti (si veda il ?sec-prob-cont-prob-distr). Uno dei motivi della sua fama è che numerose variabili nella realtà seguono, almeno approssimativamente, una distribuzione normale. Esempi includono le vincite nel gioco d’azzardo, l’altezza e il peso delle persone, la pressione sanguigna, i punteggi di alcuni test standardizzati e gli errori di misura negli esperimenti. I motivi matematici e probabilistici di queste approssimazioni verranno discussi in seguito; qui ci concentreremo sul come la distribuzione normale aiuti a riassumere i dati.\nAnziché partire da dati empirici, la distribuzione normale si definisce tramite una formula matematica. Per un intervallo generico \\((a,b)\\), la proporzione di valori che cade in tale intervallo si ottiene mediante:\n\\[\n\\text{Pr}(a &lt; x \\leq b) \\;=\\; \\int_a^b \\frac{1}{\\sqrt{2\\pi}\\,\\sigma} \\, e^{-\\tfrac12\\,\\bigl(\\tfrac{x - \\mu}{\\sigma}\\bigr)^2}\\, dx .\n\\tag{20.1}\\]\nNon è necessario memorizzare o padroneggiare i dettagli di questa formula, ma è importante sapere che la distribuzione normale è completamente determinata da due soli parametri: \\(\\mu\\) e \\(\\sigma\\). Gli altri simboli nella formula (\\(\\pi\\), \\(e\\), \\(a\\), \\(b\\)) rappresentano costanti matematiche o gli estremi dell’intervallo. In particolare, \\(\\mu\\) è il valore medio (o media) e \\(\\sigma\\) è la deviazione standard.\nQuesta distribuzione è simmetrica, centrata sulla media \\(\\mu\\), e la maggior parte dei valori (circa il 95%) si trova entro 2 deviazioni standard dalla media, cioè nell’intervallo \\(\\mu \\pm 2\\sigma\\). Ecco un esempio di come appare la distribuzione normale quando \\(\\mu = 0\\) e \\(\\sigma = 1\\):\n\nm &lt;- 0; s &lt;- 1\nnorm_dist &lt;- tibble(x = seq(-4, 4, length.out = 50)*s + m) |&gt; \n  mutate(density = dnorm(x, m, s))\nnorm_dist |&gt; \n  ggplot(aes(x, density)) + geom_line()\n\n\n\n\n\n\n\nIl fatto che la distribuzione sia descritta da due parametri implica che, se un insieme di dati reali si approssima bene a una distribuzione normale, due soli numeri (media e deviazione standard) possono fornire un riassunto sintetico della distribuzione. Vediamo ora come si calcolano, in pratica, questi due parametri per una lista di valori arbitraria.\nSupponiamo di avere un vettore x che contiene una serie di valori numerici. Abbiamo visto come, in R, la media si trova come:\nm &lt;- sum(x) / length(x)\ne la deviazione standard è:\ns &lt;- sqrt(sum((x - m)^2) / length(x))\nLa deviazione standard si può interpretare come la distanza media dei valori dalla loro media.\n\n20.1.1 Un esempio pratico\nPer calcolare media e deviazione standard dell’altezza maschile in un dataset, ipotizziamo che il vettore heights$height contenga le altezze di alcuni individui, mentre heights$sex contenga il genere corrispondente. Se vogliamo estrarre solo i valori relativi ai maschi, possiamo scrivere:\n\nindex &lt;- heights$sex == \"Male\"\nx &lt;- heights$height[index]\n\nQuindi usiamo le funzioni predefinite di R:\n\nm &lt;- mean(x)\ns &lt;- sd(x)\n\n\n\n\n\n\n\nNota: Per motivi che verranno chiariti in seguito, la funzione sd(x) effettua una divisione per \\(\\text{length}(x) - 1\\) invece che per \\(\\text{length}(x)\\). Tuttavia, se il numero di osservazioni è elevato, questa differenza è trascurabile.\n\n\n\nPossiamo ora mettere a confronto la curva di densità osservata dei dati (in blu) con quella teorica (in nero) della distribuzione normale con media e deviazione standard stimate:\n\nnorm_dist &lt;- tibble(\n  x = seq(-4, 4, length.out = 50)*s + m) |&gt; \n  mutate(density = dnorm(x, m, s))\n\nheights |&gt; \n  dplyr::filter(sex == \"Male\") |&gt; \n  ggplot(aes(height)) +\n  geom_density(fill = \"lightblue\") +\n  geom_line(aes(x, density), linewidth=1.5, data = norm_dist)\n\n\n\n\n\n\n\nCome si vede, la curva normale fornisce una buona approssimazione per i dati sull’altezza maschile. Vedremo ora come verificare l’aderenza di una distribuzione ai dati, osservando le proporzioni di valori entro intervalli specifici.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Introduzione alla distribuzione normale</span>"
    ]
  },
  {
    "objectID": "chapters/eda/07_introduction_normal_distribution.html#unità-standard",
    "href": "chapters/eda/07_introduction_normal_distribution.html#unità-standard",
    "title": "20  Introduzione alla distribuzione normale",
    "section": "\n20.2 Unità standard",
    "text": "20.2 Unità standard\nPer i dati che seguono (o quasi) una distribuzione normale, è molto comodo utilizzare le cosiddette unità standard (Standard Units). Un valore \\(x\\) viene convertito in unità standard tramite la formula:\n\\[\nz = \\frac{x - m}{s} ,\n\\]\ndove \\(m\\) e \\(s\\) sono la media e la deviazione standard della distribuzione. Questa trasformazione ci dice di quante deviazioni standard un particolare valore si discosta dalla media. Ad esempio, se \\(z=0\\), il valore \\(x\\) corrisponde esattamente alla media; se \\(z = 2\\), il valore \\(x\\) si trova a due deviazioni standard sopra la media; se \\(z = -2\\), a due deviazioni standard sotto la media, e così via.\nIn R, possiamo calcolare le unità standard con la funzione:\n\nz &lt;- scale(x) |&gt; as.numeric()\nhead(z)\n#&gt; [1]  1.574  0.190 -0.364  1.297 -2.303 -0.641\n\nSe vogliamo sapere, ad esempio, quale frazione di individui si trova entro 2 deviazioni standard dalla media (cioè \\(|z| &lt; 2\\)), basta scrivere:\n\nmean(abs(z) &lt; 2)\n#&gt; [1] 0.95\n\nVedremo, in molti casi, un valore intorno al 95%, in linea con quanto previsto dalla distribuzione normale. Per confermare la bontà dell’approssimazione, si usano spesso i grafici quantile-quantile, detti anche qqplot.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Introduzione alla distribuzione normale</span>"
    ]
  },
  {
    "objectID": "chapters/eda/07_introduction_normal_distribution.html#grafici-quantile-quantile",
    "href": "chapters/eda/07_introduction_normal_distribution.html#grafici-quantile-quantile",
    "title": "20  Introduzione alla distribuzione normale",
    "section": "\n20.3 Grafici quantile-quantile",
    "text": "20.3 Grafici quantile-quantile\nUn modo sistematico per verificare quanto la distribuzione normale descriva bene i dati osservati consiste nel confrontare i quantili empirici con quelli teorici di una normale. Se i due insiemi di quantili sono molto simili, abbiamo un’ulteriore conferma dell’aderenza alla normalità.\n\nLa funzione di ripartizione della distribuzione normale standard si indica spesso con \\(\\Phi(x)\\). Ad esempio, \\(\\Phi(-1.96) \\approx 0.025\\) e \\(\\Phi(1.96) \\approx 0.975\\).\n\nL’inversa di \\(\\Phi\\), indicata come \\(\\Phi^{-1}(p)\\), ci dà il quantile corrispondente a una determinata probabilità \\(p\\). In R, pnorm calcola \\(\\Phi(x)\\) e qnorm calcola \\(\\Phi^{-1}(p)\\). Di default, pnorm e qnorm si riferiscono alla normale standard (media 0, deviazione standard 1), ma possiamo specificare valori diversi di media e deviazione standard tramite gli argomenti mean e sd.\n\nPer ottenere il quantile empirico da un vettore di dati in R, possiamo usare la funzione quantile. Ad esempio, se abbiamo un vettore x, il quantile associato alla probabilità \\(p\\) è il valore \\(q\\) per il quale mean(x &lt;= q) = p.\nEcco lo schema logico per costruire un qqplot:\n\nDefiniamo un vettore di proporzioni \\(p_1, p_2, \\dots, p_m\\).\n\nCalcoliamo i relativi quantili empirici dei nostri dati \\(\\{q_1, \\dots, q_m\\}\\) usando quantile(x, p_i).\n\nCalcoliamo i quantili teorici della normale (con la stessa media e la stessa deviazione standard dei dati) usando qnorm(p_i, mean, sd).\n\nRappresentiamo i punti \\((\\text{quantile teorico}, \\text{quantile empirico})\\). Se i dati sono davvero normali, tali punti si disporranno approssimativamente lungo la retta diagonale y = x.\n\nEsempio in R:\n\np &lt;- seq(0.05, 0.95, 0.05)\nsample_quantiles &lt;- quantile(x, p)\ntheoretical_quantiles &lt;- qnorm(p, mean = mean(x), sd = sd(x))\n\nqplot(theoretical_quantiles, sample_quantiles) + geom_abline()\n\n\n\n\n\n\n\nSe però abbiamo già convertito in unità standard (quindi \\(\\mu = 0\\) e \\(\\sigma = 1\\)), il confronto si semplifica:\n\nsample_quantiles &lt;- quantile(z, p)\ntheoretical_quantiles &lt;- qnorm(p)\nqplot(theoretical_quantiles, sample_quantiles) + geom_abline()\n\n\n\n\n\n\n\nIn pratica, per creare rapidamente un qqplot si usa spesso ggplot2 con la geometria geom_qq:\n\nheights |&gt; filter(sex == \"Male\") |&gt;\n  ggplot(aes(sample = scale(height))) + \n  geom_qq() +\n  geom_abline()\n\n\n\n\n\n\n\nCome abbiamo sottolineato, se i punti nel qqplot si dispongono lungo una retta, significa che la distribuzione dei dati è in accordo con la distribuzione teorica considerata (in questo caso, la normale). I qqplot possono essere usati anche per confrontare qualsiasi coppia di distribuzioni, non solo dati e normale teorica.\nQuesto indica che l’approssimazione normale è accurata per il gruppo maschile (nel nostro dataset).",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Introduzione alla distribuzione normale</span>"
    ]
  },
  {
    "objectID": "chapters/eda/07_introduction_normal_distribution.html#media-e-deviazione-standard-come-statistiche-descrittive-della-distribuzione",
    "href": "chapters/eda/07_introduction_normal_distribution.html#media-e-deviazione-standard-come-statistiche-descrittive-della-distribuzione",
    "title": "20  Introduzione alla distribuzione normale",
    "section": "\n20.4 Media e deviazione standard come statistiche descrittive della distribuzione",
    "text": "20.4 Media e deviazione standard come statistiche descrittive della distribuzione\nLa media e la deviazione standard sono due delle statistiche più comunemente utilizzate per descrivere la distribuzione di un insieme di dati. Queste misure sono particolarmente utili quando i dati seguono una distribuzione normale. In questo caso, la media e la deviazione standard contengono tutte le informazioni necessarie per caratterizzare completamente la forma della distribuzione.\n\n20.4.1 Distribuzione normale e statistiche descrittive\nLa distribuzione normale è definita dalla sua media (\\(\\mu\\)) e dalla sua deviazione standard (\\(\\sigma\\)). La formula della densità di probabilità della distribuzione normale è data dall’Equazione 20.1. Questa formula mostra che, conoscendo solo \\(\\mu\\) e \\(\\sigma\\), possiamo ricostruire l’intera curva di densità. Pertanto, se i dati empirici sono ben approssimati da una distribuzione normale, la media e la deviazione standard sono sufficienti per descrivere la distribuzione.\nSupponiamo di avere un dataset che segue una distribuzione normale con media 50 e deviazione standard 10. Possiamo generare dati casuali e visualizzare la curva di densità in R:\n\n# Generiamo dati da una distribuzione normale\nset.seed(123)\ndati &lt;- rnorm(1000, mean = 50, sd = 10)\n\n# Calcoliamo media e deviazione standard\nmedia &lt;- mean(dati)\ndeviazione_standard &lt;- sd(dati)\n\n# Visualizziamo la curva di densità\nggplot(data.frame(dati), aes(x = dati)) +\n  geom_density(fill = \"lightblue\") +\n  geom_vline(xintercept = media, color = \"red\", linetype = \"dashed\") +\n  labs(\n       x = \"Valori\",\n       y = \"Densità\") +\n  annotate(\"text\", x = media + 5, y = 0.03, label = paste(\"Media =\", round(media, 2)), color = \"red\") +\n  annotate(\"text\", x = media + 5, y = 0.025, label = paste(\"Deviazione Standard =\", round(deviazione_standard, 2)), color = \"blue\")\n\n\n\n\n\n\n\nIn questo esempio:\n\nLa curva di densità è centrata attorno alla media (\\(\\mu = 50\\)).\nLa deviazione standard (\\(\\sigma = 10\\)) determina la dispersione dei dati attorno alla media.\n\n20.4.2 Quando media e deviazione standard non sono sufficienti\nSebbene media e deviazione standard siano strumenti estremamente utili per descrivere distribuzioni normali, non sempre bastano a cogliere tutte le caratteristiche di un insieme di dati. In particolare, ci sono situazioni in cui la forma della distribuzione rende necessario ricorrere a misure aggiuntive. Di seguito presentiamo alcuni casi tipici.\n\n\nDistribuzioni Asimmetriche\nUna distribuzione si dice asimmetrica (o skewed) quando una coda è più “estesa” dell’altra.\n\nSe la coda più lunga è a destra, la distribuzione è asimmetrica positiva (o a destra).\n\nSe la coda più lunga è a sinistra, la distribuzione è asimmetrica negativa (o a sinistra).\nIn queste circostanze, la media tende a spostarsi verso la coda più lunga, mentre la mediana rimane più stabile e rappresentativa del valore centrale.\n\n\nDistribuzioni Multimodali\nUna distribuzione è multimodale quando presenta più picchi (o “modi”). Ciò significa che i dati si concentrano attorno a più di un valore, formando veri e propri sotto-gruppi. In questi casi, media e deviazione standard possono risultare poco significative, poiché non colgono la presenza di più poli di concentrazione.\n\nKurtosi\nLa kurtosi descrive quanto una distribuzione sia “appuntita” o “piatta” rispetto a una normale.\n\n\nAlta kurtosi indica picchi molto accentuati e code più lunghe, con una maggiore probabilità di valori estremi.\n\n\nBassa kurtosi segnala una forma più appiattita, con code ridotte e meno outlier.\n\n\n\nQuando le distribuzioni mostrano una di queste peculiarità, altre statistiche possono rivelarsi più informative:\n\nLa mediana, insensibile ai valori estremi, fornisce una descrizione più robusta del centro.\n\nI quartili, e in particolare l’intervallo interquartile, danno un’idea della dispersione principale trascurando le code.\n\nL’indice di asimmetria (skewness) misura il grado di sbilanciamento della distribuzione.\n\nL’indice di curtosi (kurtosis) quantifica la “pesantezza” delle code.\n\nNel seguente esempio, generiamo dati da una distribuzione esponenziale, notoriamente asimmetrica:\n\n# Generiamo dati da una distribuzione esponenziale\nset.seed(123)\ndati_esponenziali &lt;- rexp(1000, rate = 0.5)\n\n# Calcoliamo media e deviazione standard\nmedia_esp &lt;- mean(dati_esponenziali)\ndeviazione_standard_esp &lt;- sd(dati_esponenziali)\n\n# Visualizziamo la curva di densità\nggplot(data.frame(dati_esponenziali), aes(x = dati_esponenziali)) +\n  geom_density(fill = \"#F8766D\", alpha = 0.6) +\n  geom_vline(xintercept = media_esp, color = \"red\", linetype = \"dashed\") +\n  labs(\n       x = \"Valori\",\n       y = \"Densità\") +\n  annotate(\"text\", x = media_esp + 1, y = 0.2, \n           label = paste(\"Media =\", round(media_esp, 2)), \n           color = \"red\") +\n  annotate(\"text\", x = media_esp + 1, y = 0.18, \n           label = paste(\"Deviazione Standard =\", round(deviazione_standard_esp, 2)), \n           color = \"blue\")\n\n\n\n\n\n\n\nL’istogramma (o la densità) mostra chiaramente una coda lunga a destra, con molti valori piccoli e pochi valori grandi. In questo contesto:\n\nLa media tende a seguire la coda, diventando meno rappresentativa del “centro”.\n\nLa deviazione standard non descrive in modo efficace la variabilità, perché non considera adeguatamente la forte asimmetria.\n\nMisure alternative, come la mediana e i quartili, forniscono informazioni più affidabili:\n\n# Calcoliamo mediana e quartili\nmediana &lt;- median(dati_esponenziali)\nquartili &lt;- quantile(dati_esponenziali, probs = c(0.25, 0.75))\n\ncat(\"Mediana:\", mediana, \"\\n\")\n#&gt; Mediana: 1.46\ncat(\"Primo Quartile (Q1):\", quartili[1], \"\\n\")\n#&gt; Primo Quartile (Q1): 0.613\ncat(\"Terzo Quartile (Q3):\", quartili[2], \"\\n\")\n#&gt; Terzo Quartile (Q3): 2.85\n\nIn conclusione, quando i dati non presentano una forma vicina alla normalità (ad esempio perché asimmetrici, multimodali o con kurtosi anomala), media e deviazione standard possono risultare fuorvianti o poco utili. In questi casi, è fondamentale adottare misure alternative o complementari (mediana, quartili, skewness, kurtosis) per ottenere una descrizione più accurata della distribuzione.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Introduzione alla distribuzione normale</span>"
    ]
  },
  {
    "objectID": "chapters/eda/07_introduction_normal_distribution.html#riflessioni-conclusive",
    "href": "chapters/eda/07_introduction_normal_distribution.html#riflessioni-conclusive",
    "title": "20  Introduzione alla distribuzione normale",
    "section": "Riflessioni conclusive",
    "text": "Riflessioni conclusive\nIn questo capitolo, abbiamo esplorato alcuni concetti fondamentali per l’analisi dei dati e consolidato le basi per un’interpretazione più approfondita delle distribuzioni. In particolare, abbiamo:\n\n\nStudiato la distribuzione normale, una delle distribuzioni più importanti in statistica, e compreso perché la media e la deviazione standard siano parametri cruciali per descriverla. Questi indicatori ci permettono di riassumere in modo efficace le caratteristiche centrali e la variabilità dei dati.\n\nImparato a standardizzare i dati convertendoli in unità standard (z-score), il che ci consente di confrontare variabili con scale diverse o di valutare quanto un dato specifico si discosti dalla media in termini di deviazioni standard.\n\nIntrodotto il grafico quantile-quantile (QQ-plot), uno strumento visivo prezioso per verificare se i nostri dati seguono una distribuzione normale. Attraverso il QQ-plot, possiamo confrontare i quantili empirici dei nostri dati con quelli teorici della distribuzione normale, identificando eventuali deviazioni.\n\nGli strumenti descritti in questo capitolo rappresentano il primo passo essenziale nell’analisi esplorativa dei dati. Essi ci aiutano a formulare ipotesi solide e a riconoscere potenziali problemi o caratteristiche peculiari dei dati prima di applicare metodi statistici più avanzati, che approfondiremo nei prossimi capitoli.\nL’analisi esplorativa, combinando grafici intuitivi e statistiche descrittive appropriate, riveste quindi un ruolo fondamentale nel processo analitico. Essa non solo ci aiuta a comprendere meglio la natura dei dati, ma ci fornisce anche una base solida su cui costruire conclusioni statistiche attendibili e informate.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Introduzione alla distribuzione normale</span>"
    ]
  },
  {
    "objectID": "chapters/eda/07_introduction_normal_distribution.html#esercizi",
    "href": "chapters/eda/07_introduction_normal_distribution.html#esercizi",
    "title": "20  Introduzione alla distribuzione normale",
    "section": "Esercizi",
    "text": "Esercizi\n::: {.callout-tip=“Esercizi” collapse=“true”} Esercizi Teorici\nRispondi alle seguenti domande per consolidare la comprensione teorica della distribuzione normale e dei suoi concetti chiave.\n\n\nCaratteristiche della distribuzione normale\n\nQuali sono i due parametri principali della distribuzione normale?\n\nPerché la distribuzione normale è utilizzata così frequentemente in statistica?\n\nIn quali situazioni reali possiamo aspettarci che una variabile segua una distribuzione normale?\n\n\n\nMedia e deviazione standard\n\nQual è il significato della media in una distribuzione normale?\n\nCosa rappresenta la deviazione standard?\n\nIn che modo la deviazione standard influenza la forma della curva normale?\n\n\n\nZ-score e standardizzazione\n\nCos’è uno z-score e come si calcola?\n\nQual è il significato di un valore z=2? E di un valore z=-1.5?\n\nDopo la standardizzazione, quali saranno la media e la deviazione standard della variabile?\n\n\n\nVerifica della normalità\n\nSe hai un piccolo campione di dati (circa 15 osservazioni), quali metodi grafici puoi utilizzare per valutare se segue una distribuzione normale?\n\nQuali strumenti statistici puoi impiegare per testare la normalità?\n\nIn un QQ-plot, come puoi riconoscere se i dati seguono una distribuzione normale?\n\n\n\nEsercizi Pratici in R\nObiettivo: Analizzare i dati raccolti dagli studenti sulla Satisfaction With Life Scale (SWLS), comprendere la loro distribuzione e confrontarli con una distribuzione normale teorica.\nDati disponibili:\nUsa i dati della SWLS. I dati contengono anche informazioni sul genere e su un indice di rete sociale (LSNS).\n1. Esplorazione e Visualizzazione dei Dati SWLS\n\n\nCarica i dati raccolti dagli studenti e verifica la struttura del dataset.\n\n\nCalcola i valori di base: media, deviazione standard, minimo, massimo, e quantili della SWLS.\n\n\nCrea una rappresentazione visiva dei dati:\n\nIstogramma con sovrapposta una curva di densità.\n\nBoxplot per identificare eventuali outlier.\n\nViolin plot per osservare la distribuzione.\n\n\n\n2. Confronto con la Distribuzione Normale\n\n\nSovrapponi ai dati osservati una curva normale teorica basata su media e deviazione standard stimate dal campione.\n\n\nConfronta i quantili empirici con quelli teorici mediante un QQ-plot.\n\n\nCommenta il risultato: i dati SWLS seguono approssimativamente una normale? Se no, quali differenze noti?\n\n3. Standardizzazione dei Punteggi SWLS\n\n\nTrasforma i dati della SWLS in z-score per analizzarli in unità standardizzate.\n\nVerifica la nuova media e deviazione standard: dovrebbero essere 0 e 1 rispettivamente.\n\n\nConta quanti punteggi standardizzati si trovano entro 1, 2 e 3 deviazioni standard dalla media e confronta i valori attesi di 68%, 95% e 99.7%.\n\n4. Relazione tra SWLS e Interazione Sociale (LSNS)\n\n\nEsplora la relazione tra SWLS e il punteggio della Scala della Rete Sociale di Lubben (LSNS-6).\n\n\nCostruisci un grafico a dispersione per osservare la correlazione tra le due variabili.\n\n\nCalcola il coefficiente di correlazione di Pearson e commenta il risultato. Esiste una relazione tra soddisfazione della vita e supporto sociale?\n:::\n\n::: {.callout-tip=“Soluzioni” collapse=“true”} 1. Caratteristiche della distribuzione normale\na. Quali sono i due parametri principali della distribuzione normale?\nI due parametri principali che definiscono una distribuzione normale sono:\n\n\nLa media (μ): Indica il centro della distribuzione. Tutte le osservazioni si raggruppano attorno a questo valore.\n\nLa deviazione standard (σ): Descrive la dispersione o la variabilità dei dati attorno alla media.\n\nb. Perché la distribuzione normale è utilizzata così frequentemente in statistica?\nLa distribuzione normale è ampiamente usata per diversi motivi:\n\n\nTeorema del limite centrale: Afferma che, quando si sommano molte variabili casuali indipendenti, la loro distribuzione tende ad avvicinarsi a una normale, indipendentemente dalla forma originale delle singole distribuzioni.\n\nSemplicità matematica: La normale ha proprietà matematiche ben definite e permette di calcolare probabilità e intervalli con facilità.\n\nModellizzazione naturale: Molte variabili naturali e sociali (ad esempio, altezze, pesi, punteggi standardizzati) seguono approssimativamente una distribuzione normale.\n\nc. In quali situazioni reali possiamo aspettarci che una variabile segua una distribuzione normale?\nSi può aspettare una distribuzione normale in situazioni in cui:\n\nLe osservazioni sono influenzate da molti fattori casuali indipendenti (es. altezza di un individuo, errore di misurazione).\nI dati derivano da fenomeni naturali o biologici (es. pressione sanguigna, peso corporeo).\nSi analizzano medie campionarie di grandi dimensioni (grazie al teorema del limite centrale).\n\n2. Media e deviazione standard\na. Qual è il significato della media in una distribuzione normale?\nNella distribuzione normale, la media rappresenta il punto centrale della curva, ovvero il valore più probabile. È anche il punto di simmetria della distribuzione, dove metà delle osservazioni si trova a sinistra e l’altra metà a destra.\nb. Cosa rappresenta la deviazione standard?\nLa deviazione standard misura quanto i dati si discostano in media dalla media. Una deviazione standard bassa indica che i dati sono raggruppati strettamente attorno alla media, mentre una deviazione standard alta indica una maggiore dispersione.\nc. In che modo la deviazione standard influenza la forma della curva normale?\n\nUna deviazione standard piccola produce una curva alta e stretta, indicando una bassa variabilità.\nUna deviazione standard grande produce una curva bassa e larga, indicando una maggiore variabilità.\n\n3. Z-score e standardizzazione\na. Cos’è uno z-score e come si calcola?\nUno z-score misura quante deviazioni standard un dato si discosta dalla media. Viene calcolato come:\n\\[\nz = \\frac{x - \\mu}{\\sigma}\n\\]\ndove \\(x\\) è il valore osservato, \\(\\mu\\) è la media e \\(\\sigma\\) è la deviazione standard.\nb. Qual è il significato di un valore z=2? E di un valore z=-1.5?\n\nUn \\(z = 2\\) significa che il dato è posizionato a 2 deviazioni standard sopra la media.\nUn \\(z = -1.5\\) significa che il dato è posizionato a 1.5 deviazioni standard sotto la media.\n\nc. Dopo la standardizzazione, quali saranno la media e la deviazione standard della variabile?\nDopo la standardizzazione:\n\nLa media diventa \\(0\\).\nLa deviazione standard diventa \\(1\\).\n\n4. Verifica della normalità\na. Se hai un piccolo campione di dati (circa 15 osservazioni), quali metodi grafici puoi utilizzare per valutare se segue una distribuzione normale?\nPer piccoli campioni, i metodi grafici più utili sono:\n\n\nQQ-plot (Quantile-Quantile plot): Confronta i quantili dei dati con quelli di una distribuzione normale. Se i punti seguono una retta diagonale, i dati sono normali.\n\nIstogramma: Mostra la distribuzione dei dati, ma con campioni piccoli può essere meno preciso.\n\nb. Quali strumenti statistici puoi impiegare per testare la normalità?\nGli strumenti statistici più comuni per verificare la normalità sono:\n\n\nTest di Shapiro-Wilk: Ideale per piccoli campioni.\n\nTest di Kolmogorov-Smirnov: Usato per confrontare la distribuzione empirica con una normale.\n\nTest di Anderson-Darling: Sensibile alle code della distribuzione.\n\nc. In un QQ-plot, come puoi riconoscere se i dati seguono una distribuzione normale?\nIn un QQ-plot:\n\nSe i dati seguono una distribuzione normale, i punti si allineeranno lungo una retta diagonale.\nDeviazioni dalla retta indicano departi dalla normalità:\n\nCode pesanti: Punti esterni alla retta suggeriscono outlier.\nAsimmetria: Punti curvati suggeriscono skewness (asimmetria).\n\n\n\nEsplorazione e Visualizzazione dei Dati SWLS\nCaricamento e struttura dei dati\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Supponiamo che i dati siano i seguenti\nset.seed(42)\nswls &lt;- data.frame(\n  ID = 1:15,\n  SWLS = c(18, 22, 25, 21, 26, 19, 20, 23, 24, 17, 22, 27, 28, 21, 19),\n  Genere = c(\"M\", \"F\", \"F\", \"M\", \"F\", \"M\", \"M\", \"F\", \"M\", \"F\", \"M\", \"F\", \"F\", \"M\", \"M\"),\n  LSNS = c(16, 20, 22, 14, 19, 18, 17, 25, 23, 12, 21, 28, 26, 19, 15)\n)\n\nstr(swls)\nsummary(swls$SWLS)\nVisualizzazioni\n# Istogramma con curva di densità\nggplot(swls, aes(x = SWLS)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 6, fill = \"blue\", alpha = 0.5) +\n  geom_density(color = \"red\", size = 1) +\n  gg(\"Distribuzione dei punteggi SWLS\")\n\n# Boxplot\nggplot(swls, aes(y = SWLS)) +\n  geom_boxplot(fill = \"cyan\") +\n  gg(\"Boxplot dei punteggi SWLS\")\n2. Confronto con la Distribuzione Normale\n# QQ-plot per valutare la normalità\nggplot(swls, aes(sample = SWLS)) +\n  geom_qq() +\n  geom_abline() +\n  gg(\"QQ-plot dei punteggi SWLS\") +\n  theme_minimal()\nOsservazione:\n\nSe i punti si allineano lungo la diagonale, i dati sono approssimativamente normali.\nSe ci sono deviazioni marcate, la distribuzione potrebbe essere asimmetrica o presentare code pesanti.\n\n3. Standardizzazione dei punteggi SWLS\nswls$Z_SWLS &lt;- scale(swls$SWLS)\n\nmean(swls$Z_SWLS)  # Dovrebbe essere circa 0\nsd(swls$Z_SWLS)    # Dovrebbe essere circa 1\n\n# Proporzione entro 1, 2, 3 deviazioni standard\nmean(abs(swls$Z_SWLS) &lt; 1)  # Atteso ~68%\nmean(abs(swls$Z_SWLS) &lt; 2)  # Atteso ~95%\nmean(abs(swls$Z_SWLS) &lt; 3)  # Atteso ~99.7%\n4. Relazione tra SWLS e LSNS\n# Grafico di dispersione\nggplot(swls, aes(x = LSNS, y = SWLS)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  gg(\"Relazione tra SWLS e LSNS\") \n\n# Calcolo della correlazione\ncor(swls$SWLS, swls$LSNS)\nInterpretazione:\n\nUn valore di correlazione positivo indica che livelli più alti di supporto sociale (LSNS) sono associati a una maggiore soddisfazione della vita (SWLS).\n\nSe la correlazione è debole, il supporto sociale potrebbe non essere un predittore forte della soddisfazione della vita in questo campione ristretto.\n:::\n\n\n\n\n\n\n\nInformazioni sull’ambiente di sviluppo\n\n\n\n\n\n\nsessionInfo()\n#&gt; R version 4.5.1 (2025-06-13)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.6.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] gridExtra_2.3         dslabs_0.8.0          ggbeeswarm_0.7.2     \n#&gt;  [4] pillar_1.11.0         tinytable_0.13.0      patchwork_1.3.2      \n#&gt;  [7] ggdist_3.3.3          tidybayes_3.0.7       bayesplot_1.14.0     \n#&gt; [10] ggplot2_3.5.2         reliabilitydiag_0.2.1 priorsense_1.1.1     \n#&gt; [13] posterior_1.6.1       loo_2.8.0             rstan_2.32.7         \n#&gt; [16] StanHeaders_2.32.10   brms_2.22.0           Rcpp_1.1.0           \n#&gt; [19] sessioninfo_1.2.3     conflicted_1.2.0      janitor_2.2.1        \n#&gt; [22] matrixStats_1.5.0     modelr_0.1.11         tibble_3.3.0         \n#&gt; [25] dplyr_1.1.4           tidyr_1.3.1           rio_1.2.3            \n#&gt; [28] here_1.0.1           \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;  [1] inline_0.3.21         sandwich_3.1-1        rlang_1.1.6          \n#&gt;  [4] magrittr_2.0.3        multcomp_1.4-28       snakecase_0.11.1     \n#&gt;  [7] compiler_4.5.1        systemfonts_1.2.3     vctrs_0.6.5          \n#&gt; [10] stringr_1.5.1         pkgconfig_2.0.3       arrayhelpers_1.1-0   \n#&gt; [13] fastmap_1.2.0         backports_1.5.0       labeling_0.4.3       \n#&gt; [16] rmarkdown_2.29        ragg_1.5.0            purrr_1.1.0          \n#&gt; [19] xfun_0.53             cachem_1.1.0          jsonlite_2.0.0       \n#&gt; [22] broom_1.0.9           parallel_4.5.1        R6_2.6.1             \n#&gt; [25] stringi_1.8.7         RColorBrewer_1.1-3    lubridate_1.9.4      \n#&gt; [28] estimability_1.5.1    knitr_1.50            zoo_1.8-14           \n#&gt; [31] pacman_0.5.1          Matrix_1.7-4          splines_4.5.1        \n#&gt; [34] timechange_0.3.0      tidyselect_1.2.1      abind_1.4-8          \n#&gt; [37] codetools_0.2-20      curl_7.0.0            pkgbuild_1.4.8       \n#&gt; [40] lattice_0.22-7        withr_3.0.2           bridgesampling_1.1-2 \n#&gt; [43] coda_0.19-4.1         evaluate_1.0.5        survival_3.8-3       \n#&gt; [46] RcppParallel_5.1.11-1 tensorA_0.36.2.1      checkmate_2.3.3      \n#&gt; [49] stats4_4.5.1          distributional_0.5.0  generics_0.1.4       \n#&gt; [52] rprojroot_2.1.1       rstantools_2.5.0      scales_1.4.0         \n#&gt; [55] xtable_1.8-4          glue_1.8.0            emmeans_1.11.2-8     \n#&gt; [58] tools_4.5.1           mvtnorm_1.3-3         grid_4.5.1           \n#&gt; [61] QuickJSR_1.8.0        colorspace_2.1-1      nlme_3.1-168         \n#&gt; [64] beeswarm_0.4.0        vipor_0.4.7           cli_3.6.5            \n#&gt; [67] textshaping_1.0.3     svUnit_1.0.8          Brobdingnag_1.2-9    \n#&gt; [70] V8_7.0.0              gtable_0.3.6          digest_0.6.37        \n#&gt; [73] TH.data_1.1-4         htmlwidgets_1.6.4     farver_2.1.2         \n#&gt; [76] memoise_2.0.1         htmltools_0.5.8.1     lifecycle_1.0.4      \n#&gt; [79] MASS_7.3-65",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Introduzione alla distribuzione normale</span>"
    ]
  },
  {
    "objectID": "chapters/eda/07_introduction_normal_distribution.html#bibliografia",
    "href": "chapters/eda/07_introduction_normal_distribution.html#bibliografia",
    "title": "20  Introduzione alla distribuzione normale",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nIrizarry, R. A. (2024). Introduction to Data Science: Data Wrangling and Visualization with R. CRC Press.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Introduzione alla distribuzione normale</span>"
    ]
  },
  {
    "objectID": "chapters/eda/08_correlation.html",
    "href": "chapters/eda/08_correlation.html",
    "title": "21  Relazioni tra variabili",
    "section": "",
    "text": "Introduzione\nL’analisi delle associazioni tra variabili è un’operazione fondamentale nell’ambito della ricerca psicologica, ma nonostante la sua apparente semplicità, rappresenta uno degli aspetti più controversi e metodologicamente complessi. Sebbene possa sembrare un passaggio naturale successivo all’analisi univariata, questo processo solleva numerose questioni concettuali e pratiche che richiedono un’attenta riflessione.\nStoricamente, in psicologia, l’analisi delle associazioni tra variabili è stata spesso considerata come l’obiettivo finale del processo di ricerca. Questa visione si basa sull’idea che la descrizione delle relazioni tra variabili possa fornire una spiegazione esaustiva dei fenomeni psicologici. Tale approccio affonda le sue radici nel pensiero di Karl Pearson, il quale sosteneva che la spiegazione scientifica si esaurisse una volta delineate le associazioni tra le variabili osservate. Pearson (1911) affermava:\nSebbene sia indubbio che rispondere alla domanda posta da Pearson sia relativamente semplice, è altrettanto evidente che la nostra comprensione di un fenomeno non può dipendere unicamente dalle informazioni fornite dalle correlazioni. Le associazioni, infatti, non implicano causalità e possono risultare fuorvianti se interpretate in modo superficiale.\nIn contrasto con questa visione tradizionale, la cosiddetta “Causal Revolution” propone un paradigma radicalmente diverso, secondo il quale le associazioni tra variabili sono considerate come epifenomeni, ovvero manifestazioni superficiali di meccanismi più profondi. L’obiettivo principale della ricerca, in questo quadro, diventa l’identificazione e la comprensione delle relazioni causali. Per comprendere veramente i fenomeni psicologici, è essenziale indagare le cause sottostanti, andando oltre la mera descrizione delle associazioni.\nUn esempio emblematico è l’associazione tra il numero di scarpe e le abilità matematiche nei bambini. Questa correlazione è molto forte, ma se controlliamo per la variabile confondente “età”, l’associazione scompare. Questo dimostra che, in psicologia così come in altri campi, trovare correlazioni molto forti tra variabili non è necessariamente informativo riguardo ai meccanismi sottostanti al fenomeno studiato. È ovvio che il numero di scarpe non influisce sulle abilità matematiche, ma senza controllare per l’età, l’associazione rimane ingannevolmente forte.\nAllo stesso modo, può accadere che un’associazione apparentemente forte scompaia se non si tiene conto di variabili confondenti. Consideriamo, ad esempio, la relazione tra autostima e rendimento scolastico in un campione di adolescenti. Analizzando l’intera popolazione, la correlazione tra autostima e rendimento potrebbe risultare prossima a zero. Questo apparente risultato nullo, tuttavia, potrebbe nascondere una relazione più complessa, influenzata da un fattore confondente come il supporto familiare.\nQuando si controlla per il supporto familiare (ad esempio analizzando separatamente i gruppi con alto e basso sostegno), emerge una relazione positiva credibile tra autostima e rendimento scolastico all’interno del gruppo con supporto elevato. Questo esempio mostra come, a livello aggregato, l’effetto di due variabili possa apparire nullo, mentre il controllo per un confondente svela una relazione causale rilevante.\nIn conclusione, l’analisi delle associazioni rappresenta un punto di partenza fondamentale, ma non può sostituire l’indagine delle relazioni causali. Per progredire nella comprensione dei fenomeni psicologici, è necessario integrare l’analisi dei dati con modelli teorici robusti e un approccio critico volto a identificare e controllare i fattori confondenti. Solo così possiamo passare dalla semplice descrizione delle relazioni alla vera comprensione dei meccanismi causali che le governano.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Relazioni tra variabili</span>"
    ]
  },
  {
    "objectID": "chapters/eda/08_correlation.html#introduzione",
    "href": "chapters/eda/08_correlation.html#introduzione",
    "title": "21  Relazioni tra variabili",
    "section": "",
    "text": "“Quanto spesso, quando è stato osservato un nuovo fenomeno, sentiamo che viene posta la domanda: ‘qual è la sua causa?’. Questa è una domanda a cui potrebbe essere assolutamente impossibile rispondere. Invece, può essere più facile rispondere alla domanda: ‘in che misura altri fenomeni sono associati con esso?’. Dalla risposta a questa seconda domanda possono risultare molte preziose conoscenze.”\n\n\n\n\n\n\nIn presenza di un forte supporto familiare, una maggiore autostima potrebbe effettivamente favorire migliori risultati scolastici.\n\nAl contrario, in assenza di tale supporto, anche livelli elevati di autostima potrebbero non tradursi in un rendimento scolastico migliore, a causa di risorse emotive e pratiche limitate.\n\n\n\nPanoramica del capitolo\n\nComprendere e calcolare la covarianza, la correlazione di Pearson e la correlazione di Spearman.\nCorrelazione e associazioni non lineari.\n\n\n\n\n\n\n\n\nPrerequisiti\n\n\n\n\n\n\nLeggere l’introduzione dell’articolo “The curious case of the cross-sectional correlation” (Hamaker, 2024).\n\n\n\n\n\n\n\n\n\n\nPreparazione del Notebook\n\n\n\n\n\n\nhere::here(\"code\", \"_common.R\") |&gt; \n  source()\n\n# Load packages\nif (!requireNamespace(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(readr)",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Relazioni tra variabili</span>"
    ]
  },
  {
    "objectID": "chapters/eda/08_correlation.html#terminologia",
    "href": "chapters/eda/08_correlation.html#terminologia",
    "title": "21  Relazioni tra variabili",
    "section": "\n21.1 Terminologia",
    "text": "21.1 Terminologia\nLa discussione dei metodi utilizzati per individuare le relazioni causali sarà trattata successivamente. In questo capitolo, ci concentreremo sui concetti statistici fondamentali necessari per descrivere le associazioni lineari tra variabili. È importante sottolineare che, sebbene esistano indici statistici per quantificare associazioni non lineari, la maggior parte degli psicologi si limita all’utilizzo di indici lineari.\nNel linguaggio comune, termini come “dipendenza”, “associazione” e “correlazione” vengono spesso usati in modo intercambiabile. Tuttavia, da un punto di vista tecnico, è importante distinguere questi concetti:\n\n\nAssociazione: questo termine indica una relazione generale tra variabili, dove la conoscenza del valore di una variabile fornisce informazioni su un’altra.\n\nCorrelazione: descrive una relazione specifica e quantificabile, indicando se due variabili tendono a variare insieme in modo sistematico. Ad esempio, in una correlazione positiva, se \\(X &gt; \\mu_X\\), è probabile che anche \\(Y &gt; \\mu_Y\\). La correlazione specifica il segno e l’intensità di una relazione lineare.\n\nDipendenza: indica una relazione causale tra le variabili, dove la variazione della variabile causale porta probabilisticamente alla variazione della variabile dipendente.\n\nÈ cruciale comprendere che non tutte le associazioni sono correlazioni e, soprattutto, che la correlazione non implica necessariamente causalità. Questa distinzione è fondamentale per interpretare correttamente i dati e evitare conclusioni errate sulle relazioni tra variabili.\nIn questo capitolo, esamineremo due misure statistiche fondamentali per valutare la relazione lineare tra due variabili: la covarianza e la correlazione. Questi indici ci permettono di descrivere il grado e la direzione dell’associazione lineare tra variabili, quantificando come queste variano congiuntamente.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Relazioni tra variabili</span>"
    ]
  },
  {
    "objectID": "chapters/eda/08_correlation.html#analisi-della-relazione-tra-due-misure-della-depressione",
    "href": "chapters/eda/08_correlation.html#analisi-della-relazione-tra-due-misure-della-depressione",
    "title": "21  Relazioni tra variabili",
    "section": "\n21.2 Analisi della relazione tra due misure della depressione",
    "text": "21.2 Analisi della relazione tra due misure della depressione\nL’obiettivo di questo esempio è esaminare la relazione tra due scale psicometriche che misurano la depressione: il Beck Depression Inventory II (BDI-II) e la Center for Epidemiologic Studies Depression Scale (CES-D). Lo studio di Zetsche et al. (2019) ha indagato se le aspettative negative possano costituire un meccanismo centrale nel mantenimento e nella reiterazione della depressione. In particolare, i ricercatori hanno confrontato 30 soggetti con almeno un episodio depressivo maggiore e 37 individui senza diagnosi depressiva.\nStrumenti di misurazione:\n\nBDI-II: strumento di autovalutazione che misura l’intensità dei sintomi depressivi riscontrati nelle ultime due settimane. Composto da 21 item, ciascuno valutato su una scala da 0 a 3, fornisce una stima della gravità della depressione.\nCES-D: scala anch’essa di autovalutazione, progettata per quantificare i sintomi depressivi sperimentati nella settimana precedente, principalmente in popolazioni generali, con particolare attenzione ad adolescenti e giovani adulti.\n\nPoiché entrambi gli strumenti mirano a misurare lo stesso costrutto, è ragionevole aspettarsi una relazione lineare tra i punteggi ottenuti, pur riconoscendo che errori di misurazione e unità di misura diverse possono generare discrepanze.\nPer verificare la relazione tra i punteggi BDI-II e CES-D, i dati sono stati processati come segue:\n\n# Leggi i dati dal file CSV\ndf &lt;- rio::import(here::here(\"data\", \"data.mood.csv\"))\n\n# Seleziona le colonne di interesse\ndf &lt;- df |&gt;\n  dplyr::select(\"esm_id\", \"group\", \"bdi\", \"cesd_sum\")\n\n# Rimuovi righe duplicate e casi con valori mancanti in 'bdi'\ndf &lt;- df[!duplicated(df), ]\ndf &lt;- df[!is.na(df$bdi), ]\n\nSuccessivamente, è stato realizzato un grafico a dispersione (scatterplot) in cui i valori del BDI-II sono stati posti sull’asse delle ascisse e quelli del CES-D sull’asse delle ordinate. Ogni punto rappresenta un partecipante, suddiviso per gruppo (soggetti depressi e controlli). L’aggiunta di una retta di regressione, ottenuta mediante un modello lineare, consente di valutare visivamente la tendenza di associazione tra le due misure.\n\n\n\n\n\n\n\n\nInterpretazione dei risultati: il grafico a dispersione evidenzia una tendenza approssimativamente lineare tra i punteggi del BDI-II e della CES-D, indicando che, in linea teorica, le due scale sono associate nel misurare il livello di depressione. Tuttavia, la presenza di una certa dispersione dei dati rispetto alla retta ideale sottolinea l’influenza degli errori di misurazione e della natura arbitraria delle unità di misura utilizzate. Per una valutazione più accurata della relazione, è necessario ricorrere a indici statistici in grado di quantificare sia la forza che la direzione dell’associazione.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Relazioni tra variabili</span>"
    ]
  },
  {
    "objectID": "chapters/eda/08_correlation.html#covarianza",
    "href": "chapters/eda/08_correlation.html#covarianza",
    "title": "21  Relazioni tra variabili",
    "section": "\n21.3 Covarianza",
    "text": "21.3 Covarianza\nIniziamo a considerare il più importante di tali indici, chiamato covarianza. In realtà la definizione di questo indice non ci sorprenderà più di tanto in quanto, in una forma solo apparentemente diversa, l’abbiamo già incontrata in precedenza. Ci ricordiamo infatti che la varianza di una generica variabile \\(X\\) è definita come la media degli scarti quadratici di ciascuna osservazione dalla media:\n\\[\nS_{XX} = \\frac{1}{n} \\sum_{i=1}^n(X_i - \\bar{X}) (X_i - \\bar{X}).\n\\tag{21.1}\\]\nLa varianza viene talvolta descritta come la “covarianza di una variabile con sé stessa”. Adesso facciamo un passo ulteriore. Invece di valutare la dispersione di una sola variabile, ci chiediamo come due variabili \\(X\\) e \\(Y\\) “variano insieme” (co-variano). È facile capire come una risposta a tale domanda possa essere fornita da una semplice trasformazione della formula precedente che diventa:\n\\[\nS_{XY} = \\frac{1}{n} \\sum_{i=1}^n(X_i - \\bar{X}) (Y_i - \\bar{Y}).\n\\tag{21.2}\\]\nL’Equazione 21.2 ci fornisce la definizione della covarianza.\n\n21.3.1 Interpretazione\nPer capire il significato dell’Equazione 21.2, supponiamo di dividere il grafico precedente in quattro quadranti definiti da una retta verticale passante per la media dei valori BDI-II e da una retta orizzontale passante per la media dei valori CES-D. Numeriamo i quadranti partendo da quello in basso a sinistra e muovendoci in senso antiorario.\nSe prevalgono punti nel I e III quadrante, allora la nuvola di punti avrà un andamento crescente (per cui a valori bassi di \\(X\\) tendono ad associarsi valori bassi di \\(Y\\) e a valori elevati di \\(X\\) tendono ad associarsi valori elevati di \\(Y\\)) e la covarianza avrà segno positivo. Mentre se prevalgono punti nel II e IV quadrante la nuvola di punti avrà un andamento decrescente (per cui a valori bassi di \\(X\\) tendono ad associarsi valori elevati di \\(Y\\) e a valori elevati di \\(X\\) tendono ad associarsi valori bassi di \\(Y\\)) e la covarianza avrà segno negativo. Dunque, il segno della covarianza ci informa sulla direzione della relazione lineare tra due variabili: l’associazione lineare si dice positiva se la covarianza è positiva, negativa se la covarianza è negativa.\n\n\n\n\n\n\nEsempio.\n\n\n\n\n\nImplementiamo l’Equazione 21.2 in R.\n\ncov_value &lt;- function(x, y) {\n  mean_x &lt;- sum(x) / length(x)\n  mean_y &lt;- sum(y) / length(y)\n\n  sub_x &lt;- x - mean_x\n  sub_y &lt;- y - mean_y\n\n  sum_value &lt;- sum(sub_y * sub_x)\n  denom &lt;- length(x)\n\n  cov &lt;- sum_value / denom\n  return(cov)\n}\n\nPer i dati mostrati nel diagramma, la covarianza tra BDI-II e CESD è 207.4\n\nx &lt;- df$bdi\ny &lt;- df$cesd_sum\n\ncov_value(x, y)\n#&gt; [1] 207\n\nOppure, in maniera più semplice:\n\nmean((x - mean(x)) * (y - mean(y)))\n#&gt; [1] 207\n\nLo stesso risultato si ottiene con la funzione cov:\n\ncov(x, y) * (length(x) - 1) / length(x)\n#&gt; [1] 207\n\nLa funzione cov(x, y) calcola la covarianza tra due array, x e y utilizzando \\(n-1\\) al denominatore.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Relazioni tra variabili</span>"
    ]
  },
  {
    "objectID": "chapters/eda/08_correlation.html#correlazione",
    "href": "chapters/eda/08_correlation.html#correlazione",
    "title": "21  Relazioni tra variabili",
    "section": "\n21.4 Correlazione",
    "text": "21.4 Correlazione\nLa direzione della relazione tra le variabili è indicata dal segno della covarianza, ma il valore assoluto di questo indice non fornisce informazioni utili poiché dipende dall’unità di misura delle variabili. Ad esempio, considerando l’altezza e il peso delle persone, la covarianza sarà più grande se l’altezza è misurata in millimetri e il peso in grammi, rispetto al caso in cui l’altezza è in metri e il peso in chilogrammi. Pertanto, per descrivere la forza e la direzione della relazione lineare tra due variabili in modo adimensionale, si utilizza l’indice di correlazione.\nLa correlazione è ottenuta standardizzando la covarianza tramite la divisione delle deviazioni standard (\\(s_X\\), \\(s_Y\\)) delle due variabili:\n\\[\nr = \\frac{S_{XY}}{S_X S_Y}.\n\\tag{21.3}\\]\nLa quantità che si ottiene dall’Equazione 21.3 viene chiamata correlazione di Bravais-Pearson (dal nome degli autori che, indipendentemente l’uno dall’altro, l’hanno introdotta).\nIn maniera equivalente, per una lista di coppie di valori \\((x_1, y_1), \\dots, (x_n, y_n)\\), il coefficiente di correlazione è definito come la media del prodotto dei valori standardizzati:\n\\[\nr = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\frac{x_i - \\bar{x}}{\\sigma_x} \\right) \\left( \\frac{y_i - \\bar{y}}{\\sigma_y} \\right),\n\\tag{21.4}\\]\ndove \\(\\bar{x}\\) e \\(\\bar{y}\\) rappresentano, rispettivamente, le medie dei valori \\(x\\) e \\(y\\), e \\(\\sigma_x\\) e \\(\\sigma_y\\) sono le rispettive deviazioni standard.\nNell’Equazione 21.4, i valori \\(x_i\\) e \\(y_i\\) vengono prima standardizzati sottraendo la media e dividendo per la deviazione standard, e poi si calcola la media del prodotto di questi valori standardizzati.\n\n21.4.1 Proprietà\nIl coefficiente di correlazione ha le seguenti proprietà:\n\nha lo stesso segno della covarianza, dato che si ottiene dividendo la covarianza per due numeri positivi;\nè un numero puro, cioè non dipende dall’unità di misura delle variabili;\nassume valori compresi tra -1 e +1.\n\n21.4.2 Interpretazione\nAll’indice di correlazione possiamo assegnare la seguente interpretazione:\n\n\n\\(r_{XY} = -1\\) \\(\\rightarrow\\) perfetta relazione negativa: tutti i punti si trovano esattamente su una retta con pendenza negativa (dal quadrante in alto a sinistra al quadrante in basso a destra);\n\n\\(r_{XY} = +1\\) \\(\\rightarrow\\) perfetta relazione positiva: tutti i punti si trovano esattamente su una retta con pendenza positiva (dal quadrante in basso a sinistra al quadrante in alto a destra);\n\n\\(-1 &lt; r_{XY} &lt; +1\\) \\(\\rightarrow\\) presenza di una relazione lineare di intensità diversa;\n\n\\(r_{XY} = 0\\) \\(\\rightarrow\\) assenza di relazione lineare tra \\(X\\) e \\(Y\\).\n\n\n\n\n\n\n\nEsempio.\n\n\n\n\n\nPer i dati riportati nel diagramma della ?sec-zetsche-scatter, la covarianza è 207.4. Il segno positivo della covarianza ci dice che tra le due variabili c’è un’associazione lineare positiva. Per capire quale sia l’intensità della relazione lineare calcoliamo la correlazione. Essendo le deviazioni standard del BDI-II e del CES-D rispettavamente uguali a 15.37 e 14.93, la correlazione diventa uguale a \\(\\frac{207.426}{15.38 \\cdot 14.93} = 0.904.\\) Tale valore è prossimo a 1.0, il che vuol dire che i punti del diagramma a dispersione non si discostano troppo da una retta con una pendenza positiva.\nTroviamo la correlazione con la funzione corrcoef():\n\ncor(x, y)\n#&gt; [1] 0.904\n\nReplichiamo il risultato implementando l’Equazione 21.3:\n\ns_xy &lt;- mean((x - mean(x)) * (y - mean(y)))\ns_x &lt;- sqrt(mean((x - mean(x))^2)) # Deviazione standard popolazione\ns_y &lt;- sqrt(mean((y - mean(y))^2)) # Deviazione standard popolazione\nr_xy &lt;- s_xy / (s_x * s_y)\nprint(r_xy)\n#&gt; [1] 0.904\n\nUn altro modo ancora per trovare la correlazione tra i punteggi BDI-II e CESD è quello di applicare l’Equazione 21.4:\n\nz_x &lt;- (x - mean(x)) / sqrt(mean((x - mean(x))^2)) \n# Standardizzazione con deviazione standard popolazione\nz_y &lt;- (y - mean(y)) / sqrt(mean((y - mean(y))^2)) \n# Standardizzazione con deviazione standard popolazione\nmean(z_x * z_y)\n#&gt; [1] 0.904\n\n\n\n\n\n\n\n\n\n\nEsempio - Gender bias.\n\n\n\n\n\nUn uso interessante delle correlazioni viene fatto in un recente articolo di Guilbeault et al. (2024). Il concetto di “gender bias” si riferisce alla tendenza sistematica di favorire un sesso rispetto all’altro, spesso a scapito delle donne. Lo studio di Guilbeault et al. (2024) analizza come le immagini online influenzino la diffusione su vasta scala di questo preconcetto di genere.\nAttraverso un vasto insieme di immagini e testi raccolti online, gli autori dimostrano che sia le misurazioni basate sulle immagini che quelle basate sui testi catturano la frequenza con cui varie categorie sociali sono associate a rappresentazioni di genere, valutate su una scala da -1 (femminile) a 1 (maschile), con 0 che indica una neutralità di genere. Questo consente di quantificare il preconcetto di genere come una forma di bias statistico lungo tre dimensioni: la tendenza delle categorie sociali ad associarsi a un genere specifico nelle immagini e nei testi, la rappresentazione relativa delle donne rispetto agli uomini in tutte le categorie sociali nelle immagini e nei testi, e il confronto tra le associazioni di genere nei dati delle immagini e dei testi con la distribuzione empirica delle donne e degli uomini nella società. Il lavoro di Guilbeault et al. (2024) evidenzia che il preconcetto di genere è molto più evidente nelle immagini rispetto ai testi, come mostrato nel pannello C della figura.\nSi noti che, nel grafico del pannello C della figura, ogni punto può essere interpretato come una misura di correlazione. La misura utilizzata da Guilbeault et al. (2024) riflette il grado di associazione tra le categorie sociali e le rappresentazioni di genere presenti nelle immagini e nei testi analizzati. Quando la misura è vicina a +1, indica una forte associazione positiva tra una categoria sociale specifica e una rappresentazione di genere maschile, mentre un valore vicino a -1 indica una forte associazione negativa con una rappresentazione di genere femminile. Un valore di 0, invece, suggerisce che non vi è alcuna associazione tra la categoria sociale considerata e un genere specifico, indicando una sorta di neutralità di genere. In sostanza, questa misura di frequenza può essere interpretata come una correlazione che riflette la tendenza delle categorie sociali a essere rappresentate in un modo o nell’altro nelle immagini e nei testi analizzati, rispetto ai concetti di genere femminile e maschile.\n\n\nIl preconcetto di genere è più prevalente nelle immagini online (da Google Immagini) e nei testi online (da Google News). A. La correlazione tra le associazioni di genere nelle immagini da Google Immagini e nei testi da Google News per tutte le categorie sociali (n = 2.986), organizzate per decili. B. La forza dell’associazione di genere in queste immagini e testi online per tutte le categorie (n = 2.986), suddivisa in base al fatto che queste categorie siano inclinate verso il femminile o il maschile. C. Le associazioni di genere per un campione di occupazioni secondo queste immagini e testi online; questo campione è stato selezionato manualmente per evidenziare i tipi di categorie sociali e preconcetti di genere esaminati. (Figura tratta da Guilbeault et al. (2024)).",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Relazioni tra variabili</span>"
    ]
  },
  {
    "objectID": "chapters/eda/08_correlation.html#correlazione-di-spearman",
    "href": "chapters/eda/08_correlation.html#correlazione-di-spearman",
    "title": "21  Relazioni tra variabili",
    "section": "\n21.5 Correlazione di Spearman",
    "text": "21.5 Correlazione di Spearman\nUn’alternativa per valutare la relazione lineare tra due variabili è il coefficiente di correlazione di Spearman, che si basa esclusivamente sull’ordine dei dati e non sugli specifici valori. Questo indice di associazione è particolarmente adatto quando gli psicologi sono in grado di misurare solo le relazioni di ordine tra diverse modalità di risposta dei soggetti, ma non l’intensità della risposta stessa. Tali variabili psicologiche che presentano questa caratteristica sono definite come “ordinali”.\n\n\n\n\n\n\nÈ importante ricordare che, nel caso di una variabile ordinale, non è possibile utilizzare le statistiche descrittive convenzionali come la media e la varianza per sintetizzare le osservazioni. Tuttavia, è possibile riassumere le osservazioni attraverso una distribuzione di frequenze delle diverse modalità di risposta. Come abbiamo appena visto, la direzione e l’intensità dell’associazione tra due variabili ordinali possono essere descritte utilizzando il coefficiente di correlazione di Spearman.\n\n\n\n\n\n\n\n\n\nEsempio.\n\n\n\n\n\nPer fornire un esempio, consideriamo due variabili di scala ordinale e calcoliamo la correlazione di Spearman tra di esse.\n\ncor.test(c(1, 2, 3, 4, 5), c(5, 6, 7, 8, 7), method = \"spearman\")\n#&gt; \n#&gt;  Spearman's rank correlation rho\n#&gt; \n#&gt; data:  c(1, 2, 3, 4, 5) and c(5, 6, 7, 8, 7)\n#&gt; S = 4, p-value = 0.09\n#&gt; alternative hypothesis: true rho is not equal to 0\n#&gt; sample estimates:\n#&gt;   rho \n#&gt; 0.821",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Relazioni tra variabili</span>"
    ]
  },
  {
    "objectID": "chapters/eda/08_correlation.html#oltre-la-correlazione-e-la-covarianza-quando-lassociazione-tra-variabili-è-più-complessa",
    "href": "chapters/eda/08_correlation.html#oltre-la-correlazione-e-la-covarianza-quando-lassociazione-tra-variabili-è-più-complessa",
    "title": "21  Relazioni tra variabili",
    "section": "\n21.6 Oltre la correlazione e la covarianza: quando l’associazione tra variabili è più complessa",
    "text": "21.6 Oltre la correlazione e la covarianza: quando l’associazione tra variabili è più complessa\nIn questo capitolo abbiamo introdotto due misure fondamentali per descrivere la relazione tra due variabili: covarianza e correlazione. La covarianza fornisce un’indicazione del modo in cui due variabili si discostano congiuntamente dalle proprie medie, mentre la correlazione ne standardizza i valori, permettendo un confronto più immediato tra diverse coppie di variabili e garantendo un indice compreso tra -1 e +1. Una correlazione vicina a +1 indica una forte relazione lineare positiva, vicina a -1 una forte relazione lineare negativa, mentre un valore vicino a 0 segnala l’assenza di una chiara relazione lineare.\nTuttavia, è cruciale comprendere che la correlazione descrive esclusivamente la dimensione lineare della relazione tra due variabili. Questo significa che una correlazione nulla (pari a zero) non implica affatto che non vi sia alcuna relazione tra le variabili, ma semplicemente che non esiste una relazione lineare. Possono esistere relazioni non lineari anche molto forti, non catturate da questo indice. Inoltre, altre situazioni possono trarre in inganno, come nei casi in cui i dati siano raggruppati in sottogruppi con proprietà differenti o siano frutto di particolari processi di selezione.\n\n21.6.1 Correlazione nulla e relazioni non lineari\nUna correlazione pari a zero può nascondere relazioni non lineari anche molto marcate. Per esempio, se una variabile Y aumenta solo quando X è molto alta o molto bassa, ma rimane costante per valori intermedi di X, questa curva a “U” può generare una correlazione vicina allo zero, pur esistendo un forte legame non lineare.\nUn esempio illustrativo è fornito dal cosiddetto Datasaurus Dozen, un insieme di tredici dataset con la stessa media, deviazione standard e correlazione tra le variabili, ma con distribuzioni visivamente e strutturalmente molto diverse. In ognuno di questi dataset la correlazione di Pearson è pari a zero, ma l’ispezione grafica rivela pattern e forme ben definite. Questo ci ricorda che è sempre bene accompagnare le misure numeriche con una visualizzazione grafica dei dati.\n\ndatasaurus_data &lt;- read.csv(\"../../data/datasaurus.csv\")\n\ndatasaurus_summary &lt;- datasaurus_data %&gt;%\n  group_by(dataset) %&gt;%\n  summarise(\n    x_count = n(),\n    x_mean = mean(x, na.rm = TRUE),\n    x_std = sd(x, na.rm = TRUE),\n    y_count = n(),\n    y_mean = mean(y, na.rm = TRUE),\n    y_std = sd(y, na.rm = TRUE)\n  )\n\ndatasaurus_summary\n#&gt; # A tibble: 13 × 7\n#&gt;    dataset    x_count x_mean x_std y_count y_mean y_std\n#&gt;    &lt;chr&gt;        &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1 away           142   54.3  16.8     142   47.8  26.9\n#&gt;  2 bullseye       142   54.3  16.8     142   47.8  26.9\n#&gt;  3 circle         142   54.3  16.8     142   47.8  26.9\n#&gt;  4 dino           142   54.3  16.8     142   47.8  26.9\n#&gt;  5 dots           142   54.3  16.8     142   47.8  26.9\n#&gt;  6 h_lines        142   54.3  16.8     142   47.8  26.9\n#&gt;  7 high_lines     142   54.3  16.8     142   47.8  26.9\n#&gt;  8 slant_down     142   54.3  16.8     142   47.8  26.9\n#&gt;  9 slant_up       142   54.3  16.8     142   47.8  26.9\n#&gt; 10 star           142   54.3  16.8     142   47.8  26.9\n#&gt; 11 v_lines        142   54.3  16.8     142   47.8  26.9\n#&gt; 12 wide_lines     142   54.3  16.8     142   47.8  26.9\n#&gt; 13 x_shape        142   54.3  16.8     142   47.8  26.9\n\n\ndatasaurus_data |&gt;\n  ggplot(aes(x = x, y = y)) +\n  geom_point(alpha = 0.7) +\n  facet_wrap(~dataset, nrow = 4, ncol = 4) +\n  labs(x = NULL, y = NULL)\n\n\n\n\n\n\n\nTutti questi esempi rafforzano l’idea che l’assenza di correlazione lineare non significa assenza di relazione: potremmo avere strutture curve, pattern complessi o punti anomali (outlier) capaci di modificare radicalmente la forma della relazione tra le variabili.\nOltre alle relazioni non lineari, esistono situazioni in cui la correlazione, da sola, può fornire un’immagine distorta dei dati. Tra queste ricordiamo due fenomeni noti come il paradosso di Simpson e il paradosso di Berkson.\n\n21.6.2 Paradosso di Simpson\nIl paradosso di Simpson si verifica quando, guardando i dati raggruppati per sottogruppi, si osserva una certa relazione tra due variabili, ma aggregando i dati di tutti i sottogruppi insieme emerge una relazione opposta. In altre parole, la tendenza che appare quando si considerano i dati divisi per gruppi scompare o si inverte quando si esamina l’intero campione senza tenere conto della suddivisione in sottogruppi.\nImmaginiamo di avere due dipartimenti universitari, A e B. All’interno di ciascun dipartimento, la relazione tra il voto di laurea (X) e la performance in un successivo programma di specializzazione (Y) è positiva: all’aumentare del voto di laurea aumenta, in media, la performance nella scuola di specializzazione.\nTuttavia, supponiamo che il Dipartimento A abbia in generale voti di laurea mediamente più bassi, ma ottime performance nella specializzazione; mentre il Dipartimento B abbia voti di laurea mediamente più alti, ma performance alla specializzazione un po’ più basse. Se uniamo tutti gli studenti dei due dipartimenti senza considerarne l’appartenenza, potremmo osservare una relazione negativa tra voto di laurea e performance, sovvertendo le conclusioni tratte guardando ai singoli sottogruppi.\nEcco come possiamo simulare questi dati in R:\n\nset.seed(123)\n\n# Numero di osservazioni per dipartimento\nn &lt;- 100\n\n# Dipartimento A:\n# Voti di laurea (X) più bassi, ma performance (Y) più alta.\n# Creiamo un legame positivo tra X e Y all'interno di A.\nX_A &lt;- rnorm(n, mean = 50, sd = 5)    # Voti laurea mediamente più bassi\nY_A &lt;- X_A + rnorm(n, mean = 20, sd = 5) # Performance alta e correlata positivamente con X\n\n# Dipartimento B:\n# Voti di laurea (X) più alti, ma performance (Y) più bassa.\n# Anche qui creiamo un legame positivo tra X e Y all'interno di B, \n# ma con un offset tale che globalmente i voti alti coincidano con performance minori.\nX_B &lt;- rnorm(n, mean = 60, sd = 5)    # Voti laurea mediamente più alti\nY_B &lt;- X_B - rnorm(n, mean = 10, sd = 5) # Performance più bassa ma comunque correlata positivamente con X all’interno di B\n\n# Creiamo un dataframe con tutti i dati\ndipartimento &lt;- c(rep(\"A\", n), rep(\"B\", n))\nX &lt;- c(X_A, X_B)\nY &lt;- c(Y_A, Y_B)\ndati &lt;- data.frame(dipartimento, X, Y)\n\n# Correlazioni all'interno dei dipartimenti\ncat(\"Correlazione nel Dipartimento A:\", cor(X_A, Y_A), \"\\n\")\n#&gt; Correlazione nel Dipartimento A: 0.667\ncat(\"Correlazione nel Dipartimento B:\", cor(X_B, Y_B), \"\\n\")\n#&gt; Correlazione nel Dipartimento B: 0.693\n\n# Correlazione sull'intero dataset (senza distinguere i dipartimenti)\ncat(\"Correlazione globale:\", cor(X, Y), \"\\n\")\n#&gt; Correlazione globale: -0.335\n\n\n# Visualizziamo i dati\nggplot(dati, aes(x = X, y = Y, color = dipartimento)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(\n    x = \"Voto di laurea\", \n    y = \"Performance\\nnella specializzazione\",\n    color = \"Dipartimento\"\n  )\n\n\n\n\n\n\n\n\nAll’interno del Dipartimento A: la correlazione tra voto di laurea (X) e performance (Y) è positiva. Ciò significa che, per gli studenti di A, avere un voto di laurea più alto è associato a una performance maggiore nella specializzazione.\nAll’interno del Dipartimento B: la correlazione tra X e Y è anch’essa positiva, indicando che anche nel secondo dipartimento voti più alti tendono ad accompagnarsi a performance più alte.\nConsiderando entrambi i dipartimenti insieme: a causa delle differenze nei livelli medi di X e Y tra i dipartimenti, unendo i dati senza distinguere il gruppo di appartenenza otteniamo una correlazione globale negativa. Questo significa che, ignorando la suddivisione in dipartimenti, sembra che aumentare il voto di laurea sia associato a una diminuzione della performance nella specializzazione — una conclusione opposta a quella tratta dall’analisi separata dei due sottogruppi.\n\nQuesto è un esempio concreto del paradosso di Simpson: la relazione osservata in sottogruppi omogenei si inverte quando si aggregano i dati, mettendo in guardia sulla necessità di considerare con attenzione la struttura dei dati e i fattori confondenti prima di trarre conclusioni.\n\n21.6.3 Paradosso di Berkson\nIl paradosso di Berkson è un fenomeno legato alla selezione del campione. Se il dataset non è rappresentativo della popolazione generale, la relazione osservata può risultare artificiale o opposta a quella esistente su un campione più ampio. Per esempio, analizzando solo ciclisti professionisti, potremmo non vedere alcuna relazione tra VO2 max e probabilità di vincere una gara, poiché tutti hanno già superato una certa soglia di VO2 max. Considerando la popolazione generale, invece, potrebbe emergere chiaramente una relazione positiva tra questi due fattori. Questo paradosso evidenzia l’importanza di considerare il processo di selezione dei dati e di chiedersi se il campione analizzato sia adeguato a rispondere alla domanda di ricerca.\n\n21.6.4 Limiti delle statistiche riassuntive semplici\nUn esempio famoso che dimostra i limiti delle semplici statistiche descrittive — come media, deviazione standard e correlazione — è il quartetto di Anscombe. Questo insieme di quattro piccoli dataset possiede identiche medie, varianze e correlazioni tra variabili, ma rappresenta relazioni estremamente differenti tra X e Y (Anscombe, 1973).\nImportiamo il dataset anscombe già disponibile nel pacchetto datasets di R:\n\ndata(\"anscombe\")\n\nanscombe |&gt; \n  head()\n#&gt;   x1 x2 x3 x4   y1   y2    y3   y4\n#&gt; 1 10 10 10  8 8.04 9.14  7.46 6.58\n#&gt; 2  8  8  8  8 6.95 8.14  6.77 5.76\n#&gt; 3 13 13 13  8 7.58 8.74 12.74 7.71\n#&gt; 4  9  9  9  8 8.81 8.77  7.11 8.84\n#&gt; 5 11 11 11  8 8.33 9.26  7.81 8.47\n#&gt; 6 14 14 14  8 9.96 8.10  8.84 7.04\n\nIl dataset anscombe contiene quattro serie di dati (x1, y1), (x2, y2), (x3, y3) e (x4, y4), ognuna costituita da 11 coppie di valori (x, y). Per comprendere meglio le loro caratteristiche, iniziamo calcolando alcune statistiche descrittive. La funzione apply() consente di applicare una funzione (ad esempio, mean) a tutte le colonne di un data frame. Usandola sulle colonne di anscombe, otteniamo le medie di ciascuna delle otto variabili (le quattro x e le quattro y):\n\napply(anscombe, MARGIN = 2, mean)\n#&gt;  x1  x2  x3  x4  y1  y2  y3  y4 \n#&gt; 9.0 9.0 9.0 9.0 7.5 7.5 7.5 7.5\n\nOsserviamo che le medie delle quattro variabili x sono identiche fino a sei cifre decimali, e le medie delle quattro variabili y differiscono solo in modo trascurabile (circa lo 0.01%). Analogamente, se calcoliamo le deviazioni standard per ciascuna variabile, notiamo una notevole somiglianza:\n\napply(anscombe, MARGIN = 2, sd)\n#&gt;   x1   x2   x3   x4   y1   y2   y3   y4 \n#&gt; 3.32 3.32 3.32 3.32 2.03 2.03 2.03 2.03\n\nAnche in questo caso, le deviazioni standard per le x coincidono fino alla sesta cifra decimale, mentre quelle delle y differiscono di meno dello 0.06%. Inoltre, se calcoliamo i coefficienti di correlazione tra x e y in ognuno dei quattro dataset, scopriamo che sono quasi identici:\n\n# Calcoliamo la correlazione per ciascuna coppia (x1,y1), (x2,y2), (x3,y3), (x4,y4)\nfor (i in 1:4) {\n  x_var &lt;- anscombe[[paste0(\"x\", i)]]\n  y_var &lt;- anscombe[[paste0(\"y\", i)]]\n  \n  corr_value &lt;- cor(x_var, y_var)\n  cat(\"Correlazione tra x\", i, \"e y\", i, \":\", corr_value, \"\\n\")\n}\n#&gt; Correlazione tra x 1 e y 1 : 0.816 \n#&gt; Correlazione tra x 2 e y 2 : 0.816 \n#&gt; Correlazione tra x 3 e y 3 : 0.816 \n#&gt; Correlazione tra x 4 e y 4 : 0.817\n\nSe ci limitassimo a guardare queste statistiche (media, deviazione standard, correlazione), potremmo facilmente essere indotti a concludere che i quattro dataset sono tra loro sostanzialmente indistinguibili. Tuttavia, la realtà è molto diversa. Le statistiche descrittive, prese da sole, non offrono una visione completa e possono nascondere importanti differenze nella struttura dei dati.\nQuesta differenza diventa evidente non appena decidiamo di visualizzare i dati. La rappresentazione grafica fornisce informazioni che non emergono dalle sole statistiche riassuntive:\n\nanscombe_m &lt;- tibble()\n\nfor (i in 1:4) {\n  anscombe_m &lt;- rbind(\n    anscombe_m, tibble(set = i, x = anscombe[, i], y = anscombe[, i + 4])\n  )\n}\n\nggplot(anscombe_m, aes(x, y)) +\n  geom_point(size = 3, shape = 21, fill=\"orange\") +\n  geom_smooth(method = \"lm\", fill = NA, fullrange = TRUE) +\n  facet_wrap(~set, ncol = 2)\n\n\n\n\n\n\n\nOsservando i grafici, notiamo subito che i quattro dataset sono profondamente diversi:\n\n\nDataset 1: Qui la relazione tra x e y è approssimativamente lineare, e la correlazione riflette in modo appropriato il legame tra le due variabili.\n\nDataset 2: Sebbene la correlazione sia simile a quella del Dataset 1, i dati mostrano una relazione curvilinea e non lineare. La semplice correlazione lineare non ne cattura la forma.\n\nDataset 3: La presenza di un singolo outlier distorce la percezione della relazione tra le variabili, altrimenti lineare. La correlazione alta è influenzata in modo sproporzionato da questo punto anomalo.\n\nDataset 4: Qui i dati non mostrano alcuna relazione lineare. La correlazione elevata è il frutto di un pattern fortemente atipico (ad esempio, una sola coppia di punti allineata).\n\nIl quartetto di Anscombe mette in luce un principio fondamentale: statistiche descrittive come media, deviazione standard e correlazione non sono sempre sufficienti per comprendere la natura dei dati. La visualizzazione grafica è essenziale per cogliere relazioni, pattern non lineari, outlier e altre caratteristiche che le semplici statistiche non riescono a rivelare. In definitiva, questo esempio dimostra che analizzare i dati soltanto attraverso poche statistiche di sintesi può portare a conclusioni fuorvianti, mentre l’integrazione con la rappresentazione grafica fornisce una visione più completa e accurata.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Relazioni tra variabili</span>"
    ]
  },
  {
    "objectID": "chapters/eda/08_correlation.html#riflessioni-conclusive",
    "href": "chapters/eda/08_correlation.html#riflessioni-conclusive",
    "title": "21  Relazioni tra variabili",
    "section": "Riflessioni conclusive",
    "text": "Riflessioni conclusive\nLa covarianza e la correlazione sono strumenti preziosi per capire l’intensità e la direzione lineare di una relazione tra due variabili. Tuttavia, non dobbiamo confondere la loro semplicità con completezza d’informazione. Una correlazione nulla non implica assenza di relazione, ma solo assenza di una relazione lineare. Inoltre, il paradosso di Simpson e il paradosso di Berkson dimostrano che la semplice osservazione di correlazioni può essere fuorviante se non si tiene conto della struttura dei dati o del processo di selezione del campione.\nInfine, esempi come il quartetto di Anscombe sottolineano quanto sia fondamentale accompagnare le statistiche riassuntive con analisi grafiche e un’attenzione costante ai possibili pattern non lineari, agli outlier e alle caratteristiche peculiari del dataset. Nel prossimo capitolo esamineremo approcci che consentono di avvicinarsi alla comprensione causale dei fenomeni, andando oltre la semplice osservazione dell’associazione tra variabili.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Relazioni tra variabili</span>"
    ]
  },
  {
    "objectID": "chapters/eda/08_correlation.html#esercizi",
    "href": "chapters/eda/08_correlation.html#esercizi",
    "title": "21  Relazioni tra variabili",
    "section": "Esercizi",
    "text": "Esercizi\n\n\n\n\n\n\nEsercizio\n\n\n\n\n\nEsercizi Teorici\n\n\nDefinizioni fondamentali\n\nQual è la differenza tra associazione, correlazione e dipendenza?\n\nPerché la correlazione non implica causalità? Fai un esempio.\n\nIn quali situazioni la correlazione di Spearman è preferibile rispetto alla correlazione di Pearson?\n\n\n\nInterpretazione della correlazione\n\nQual è il range dei valori che può assumere la correlazione di Pearson?\n\nSe il coefficiente di correlazione tra due variabili è 0.85, come interpreteresti la loro relazione?\n\nSe il coefficiente di correlazione è -0.60, che tipo di relazione esiste tra le due variabili?\n\nQuali fattori potrebbero influenzare il valore della correlazione?\n\n\n\nCovarianza vs Correlazione\n\nQual è la differenza tra covarianza e correlazione?\n\nPerché la covarianza non è sempre interpretabile come misura della forza della relazione tra due variabili?\n\nQuali sono i vantaggi di usare la correlazione al posto della covarianza?\n\n\n\nGrafico a dispersione e correlazione\n\nOsservando un grafico a dispersione, quali caratteristiche ti permettono di identificare una relazione lineare positiva o negativa?\n\nDisegna (o descrivi verbalmente) un esempio di un dataset con una correlazione di circa 0, ma con una chiara relazione non lineare tra le variabili.\n\n\n\nEsercizi Pratici in R\n📌 Obiettivo: Analizzare le relazioni tra Satisfaction With Life Scale (SWLS) e Scala della Rete Sociale di Lubben (LSNS-6), calcolare covarianza e correlazione, e visualizzare i dati per individuare pattern.\nDati disponibili:\nUsa i dati raccolti per le variabili SWLS e LSNS, oltre al genere dei partecipanti.\n1. Esplorazione e Visualizzazione della Relazione tra SWLS e LSNS\n\n\nCarica i dati raccolti dagli studenti e verifica la struttura del dataset.\n\n\nCalcola le statistiche descrittive: media, deviazione standard, minimo, massimo e quartili delle variabili SWLS e LSNS.\n\n\nCrea un grafico a dispersione tra SWLS e LSNS:\n\nColora i punti in base al genere del partecipante.\n\nAggiungi una linea di regressione per evidenziare il trend della relazione.\n\n\n\n2. Calcolo della Covarianza e della Correlazione tra SWLS e LSNS\n\n\nCalcola la covarianza tra SWLS e LSNS usando la formula matematica della covarianza e confrontala con il valore ottenuto con cov().\n\n\nCalcola la correlazione di Pearson e commenta il risultato:\n\nLa relazione è forte o debole?\n\nHa segno positivo o negativo?\n\nÈ coerente con quanto osservato nel grafico a dispersione?\n\n\n\n\nCalcola la correlazione di Spearman e confrontala con quella di Pearson. Quale delle due è più appropriata per questi dati?\n\n3. Analisi delle Associazioni per Gruppi\n\n\nCalcola la correlazione separatamente per i partecipanti di genere maschile e femminile.\n\n\nConfronta i risultati: la relazione tra SWLS e LSNS è simile nei due gruppi o ci sono differenze?\n\n\nVisualizza i dati con due grafici a dispersione distinti per maschi e femmine.\n\n4. Correlazione Nulla e Pattern Non Lineari\n\n\nSimula un dataset in cui la correlazione di Pearson è vicina a 0, ma esiste una chiara relazione non lineare tra le variabili.\n\n\nCostruisci un grafico a dispersione per osservare il pattern nei dati.\n\n\nCalcola la correlazione di Spearman e confrontala con quella di Pearson. Quale delle due cattura meglio la relazione nei dati?\n\nConsegna il file .qmd compilato in PDF contenente il codice, le visualizzazioni e le interpretazioni.\n\n\n\n\n\n\n\n\n\nSoluzione\n\n\n\n\n\n1. Definizioni fondamentali\n1.1 Differenza tra associazione, correlazione e dipendenza:\n\n\nAssociazione: Indica una relazione generica tra due variabili, senza specificare la natura o la direzione della relazione.\n\nCorrelazione: Misura la forza e la direzione di una relazione lineare tra due variabili. Può essere positiva (variabili aumentano insieme) o negativa (una variabile aumenta mentre l’altra diminuisce).\n\nDipendenza: Indica che una variabile è influenzata da un’altra, ma non implica necessariamente una relazione lineare. La dipendenza può essere causale o statistica.\n\n1.2 Perché la correlazione non implica causalità:\nLa correlazione misura solo la relazione lineare tra due variabili, ma non indica se una variabile causa l’altra. Un esempio classico è la correlazione tra il consumo di gelati e il numero di annegamenti: entrambi aumentano in estate, ma non c’è un nesso causale diretto. Un terzo fattore (il caldo) influenza entrambe le variabili.\n1.3 Quando preferire la correlazione di Spearman rispetto a quella di Pearson:\n\nQuando i dati non sono distribuiti normalmente.\nQuando ci sono outlier che potrebbero distorcere la correlazione di Pearson.\nQuando la relazione tra le variabili è monotona (sempre crescente o decrescente) ma non lineare.\n\n2. Interpretazione della correlazione\n2.1 Range dei valori della correlazione di Pearson:\nLa correlazione di Pearson assume valori compresi tra -1 e 1: - 1: Correlazione lineare positiva perfetta. - -1: Correlazione lineare negativa perfetta. - 0: Nessuna correlazione lineare.\n2.2 Interpretazione di un coefficiente di 0.85:\nUn coefficiente di 0.85 indica una forte relazione lineare positiva tra le due variabili. All’aumentare di una variabile, l’altra tende ad aumentare in modo consistente.\n2.3 Interpretazione di un coefficiente di -0.60:\nUn coefficiente di -0.60 indica una relazione lineare negativa moderata. All’aumentare di una variabile, l’altra tende a diminuire.\n2.4 Fattori che influenzano la correlazione:\n\n\nOutlier: Possono distorcere il valore della correlazione.\n\nDistribuzione non lineare: La correlazione di Pearson non cattura relazioni non lineari.\n\nRange ristretto delle variabili: Se i dati coprono solo una piccola parte del range possibile, la correlazione potrebbe essere sottostimata.\n\n3. Covarianza vs Correlazione\n3.1 Differenza tra covarianza e correlazione:\n\n\nCovarianza: Misura la direzione della relazione tra due variabili, ma il suo valore dipende dalle unità di misura delle variabili.\n\nCorrelazione: Standardizza la covarianza, rendendola adimensionale e consentendo confronti tra diverse coppie di variabili.\n\n3.2 Perché la covarianza non è sempre interpretabile:\nLa covarianza non è standardizzata, quindi il suo valore non fornisce informazioni sulla forza della relazione. Ad esempio, una covarianza di 1000 potrebbe indicare una relazione forte o debole, a seconda delle unità di misura.\n3.3 Vantaggi della correlazione rispetto alla covarianza:\n\nÈ adimensionale, quindi può essere confrontata tra diverse coppie di variabili.\nAssume valori compresi tra -1 e 1, facilitando l’interpretazione della forza e della direzione della relazione.\n\n4. Grafico a dispersione e correlazione\n4.1 Caratteristiche di un grafico a dispersione per identificare relazioni lineari:\n\n\nRelazione lineare positiva: I punti si dispongono lungo una linea retta con pendenza positiva.\n\nRelazione lineare negativa: I punti si dispongono lungo una linea retta con pendenza negativa.\n\nNessuna relazione lineare: I punti sono sparsi senza un pattern evidente.\n\n4.2 Esempio di dataset con correlazione circa 0 ma relazione non lineare:\nImmagina un dataset in cui una variabile \\(x\\) assume valori simmetrici intorno a 0 (ad esempio, da -5 a 5), e la variabile \\(y\\) è uguale a \\(x^2\\). In questo caso:\n\nLa correlazione di Pearson sarà circa 0, perché non c’è una relazione lineare.\nTuttavia, esiste una chiara relazione non lineare (quadratica) tra \\(x\\) e \\(y\\).\n\nDescrizione verbale:\nI punti formano una parabola, con \\(y\\) che aumenta sia quando \\(x\\) è positivo che negativo. La correlazione di Pearson non cattura questa relazione, mentre la correlazione di Spearman potrebbe farlo.\n1. Esplorazione e Visualizzazione della Relazione tra SWLS e LSNS\n1.1 Carica i dati raccolti dagli studenti e verifica la struttura del dataset Simuliamo un dataset con 100 partecipanti, includendo le variabili SWLS (Soddisfazione di Vita), LSNS (Rete Sociale), e Genere.\n# Simulazione dei dati\nset.seed(123)\nn &lt;- 100\ngenere &lt;- sample(c(\"Maschio\", \"Femmina\"), n, replace = TRUE)\nswls &lt;- round(rnorm(n, mean = 20, sd = 5), 1)  # SWLS: Scala 5-35\nlsns &lt;- round(rnorm(n, mean = 12, sd = 4), 1)   # LSNS: Scala 0-30\n\n# Creazione del dataset\ndati &lt;- data.frame(Genere = genere, SWLS = swls, LSNS = lsns)\n\n# Verifica della struttura\nstr(dati)\nhead(dati)\n1.2 Calcola le statistiche descrittive\n# Statistiche descrittive per SWLS e LSNS\nsummary(dati$SWLS)\nsummary(dati$LSNS)\n\n# Media e deviazione standard\nmean(dati$SWLS)\nsd(dati$SWLS)\nmean(dati$LSNS)\nsd(dati$LSNS)\n1.3 Crea un grafico a dispersione\nlibrary(ggplot2)\n\nggplot(dati, aes(x = SWLS, y = LSNS, color = Genere)) +\n  geom_point(size = 3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\") +\n  labs(\n       x = \"Soddisfazione di Vita (SWLS)\",\n       y = \"Rete Sociale (LSNS)\") +\n  theme_minimal()\n2. Calcolo della Covarianza e della Correlazione tra SWLS e LSNS\n2.1 Calcola la covarianza\n# Covarianza manuale\ncov_manual &lt;- sum((dati$SWLS - mean(dati$SWLS)) * (dati$LSNS - mean(dati$LSNS))) / (n - 1)\ncov_manual\n\n# Covarianza con funzione R\ncov(dati$SWLS, dati$LSNS)\n2.2 Calcola la correlazione di Pearson\ncor_pearson &lt;- cor(dati$SWLS, dati$LSNS, method = \"pearson\")\ncor_pearson\n\n\nCommento: La correlazione è [valore], indicando una relazione [forte/debole] e [positiva/negativa]. Questo è coerente con il grafico a dispersione.\n\n2.3 Calcola la correlazione di Spearman\ncor_spearman &lt;- cor(dati$SWLS, dati$LSNS, method = \"spearman\")\ncor_spearman\n\n\nConfronto: La correlazione di Spearman è più appropriata se i dati non sono distribuiti normalmente o presentano outlier.\n\n3. Analisi delle Associazioni per Gruppi\n3.1 Calcola la correlazione separatamente per genere\n# Maschi\ncor_maschi &lt;- cor(dati$SWLS[dati$Genere == \"Maschio\"], dati$LSNS[dati$Genere == \"Maschio\"], method = \"pearson\")\n\n# Femmine\ncor_femmine &lt;- cor(dati$SWLS[dati$Genere == \"Femmina\"], dati$LSNS[dati$Genere == \"Femmina\"], method = \"pearson\")\n\ncor_maschi\ncor_femmine\n3.2 Confronta i risultati\n\n\nCommento: La correlazione è [simile/diversa] tra maschi e femmine, suggerendo [presenza/assenza] di differenze di genere.\n\n3.3 Visualizza i dati con grafici distinti\nggplot(dati, aes(x = SWLS, y = LSNS, color = Genere)) +\n  geom_point(size = 3) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(~ Genere) +\n  labs(\n       x = \"Soddisfazione di Vita (SWLS)\",\n       y = \"Rete Sociale (LSNS)\") +\n  theme_minimal()\n4. Correlazione Nulla e Pattern Non Lineari\n4.1 Simula un dataset con correlazione nulla ma relazione non lineare\nset.seed(123)\nx &lt;- rnorm(100, mean = 0, sd = 1)\ny &lt;- x^2 + rnorm(100, mean = 0, sd = 0.5)  # Relazione quadratica\ndati_non_lineari &lt;- data.frame(x = x, y = y)\n\n# Correlazione di Pearson\ncor_pearson_non_lineare &lt;- cor(dati_non_lineari$x, dati_non_lineari$y, method = \"pearson\")\ncor_pearson_non_lineare  # Dovrebbe essere vicina a 0\n4.2 Costruisci un grafico a dispersione\nggplot(dati_non_lineari, aes(x = x, y = y)) +\n  geom_point(size = 3) +\n  labs(\n       x = \"Variabile X\",\n       y = \"Variabile Y\") \n4.3 Calcola la correlazione di Spearman\ncor_spearman_non_lineare &lt;- cor(dati_non_lineari$x, dati_non_lineari$y, method = \"spearman\")\ncor_spearman_non_lineare  # Dovrebbe catturare la relazione non lineare\n\n\nConfronto: La correlazione di Spearman è più adatta per catturare relazioni non lineari.\n\n\n\n\n\n\n\n\n\n\nInformazioni sull’ambiente di sviluppo\n\n\n\n\n\n\nsessionInfo()\n#&gt; R version 4.5.1 (2025-06-13)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.6.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] readr_2.1.5           pillar_1.11.0         tinytable_0.13.0     \n#&gt;  [4] patchwork_1.3.2       ggdist_3.3.3          tidybayes_3.0.7      \n#&gt;  [7] bayesplot_1.14.0      ggplot2_3.5.2         reliabilitydiag_0.2.1\n#&gt; [10] priorsense_1.1.1      posterior_1.6.1       loo_2.8.0            \n#&gt; [13] rstan_2.32.7          StanHeaders_2.32.10   brms_2.22.0          \n#&gt; [16] Rcpp_1.1.0            sessioninfo_1.2.3     conflicted_1.2.0     \n#&gt; [19] janitor_2.2.1         matrixStats_1.5.0     modelr_0.1.11        \n#&gt; [22] tibble_3.3.0          dplyr_1.1.4           tidyr_1.3.1          \n#&gt; [25] rio_1.2.3             here_1.0.1           \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;  [1] gridExtra_2.3         inline_0.3.21         sandwich_3.1-1       \n#&gt;  [4] rlang_1.1.6           magrittr_2.0.3        multcomp_1.4-28      \n#&gt;  [7] snakecase_0.11.1      compiler_4.5.1        mgcv_1.9-3           \n#&gt; [10] systemfonts_1.2.3     vctrs_0.6.5           stringr_1.5.1        \n#&gt; [13] pkgconfig_2.0.3       arrayhelpers_1.1-0    fastmap_1.2.0        \n#&gt; [16] backports_1.5.0       labeling_0.4.3        utf8_1.2.6           \n#&gt; [19] rmarkdown_2.29        tzdb_0.5.0            ragg_1.5.0           \n#&gt; [22] purrr_1.1.0           xfun_0.53             cachem_1.1.0         \n#&gt; [25] jsonlite_2.0.0        broom_1.0.9           parallel_4.5.1       \n#&gt; [28] R6_2.6.1              stringi_1.8.7         RColorBrewer_1.1-3   \n#&gt; [31] lubridate_1.9.4       estimability_1.5.1    knitr_1.50           \n#&gt; [34] zoo_1.8-14            R.utils_2.13.0        pacman_0.5.1         \n#&gt; [37] Matrix_1.7-4          splines_4.5.1         timechange_0.3.0     \n#&gt; [40] tidyselect_1.2.1      abind_1.4-8           yaml_2.3.10          \n#&gt; [43] codetools_0.2-20      curl_7.0.0            pkgbuild_1.4.8       \n#&gt; [46] lattice_0.22-7        withr_3.0.2           bridgesampling_1.1-2 \n#&gt; [49] coda_0.19-4.1         evaluate_1.0.5        survival_3.8-3       \n#&gt; [52] RcppParallel_5.1.11-1 tensorA_0.36.2.1      checkmate_2.3.3      \n#&gt; [55] stats4_4.5.1          distributional_0.5.0  generics_0.1.4       \n#&gt; [58] rprojroot_2.1.1       hms_1.1.3             rstantools_2.5.0     \n#&gt; [61] scales_1.4.0          xtable_1.8-4          glue_1.8.0           \n#&gt; [64] emmeans_1.11.2-8      tools_4.5.1           data.table_1.17.8    \n#&gt; [67] mvtnorm_1.3-3         grid_4.5.1            QuickJSR_1.8.0       \n#&gt; [70] colorspace_2.1-1      nlme_3.1-168          cli_3.6.5            \n#&gt; [73] textshaping_1.0.3     svUnit_1.0.8          Brobdingnag_1.2-9    \n#&gt; [76] V8_7.0.0              gtable_0.3.6          R.methodsS3_1.8.2    \n#&gt; [79] digest_0.6.37         TH.data_1.1-4         htmlwidgets_1.6.4    \n#&gt; [82] farver_2.1.2          R.oo_1.27.1           memoise_2.0.1        \n#&gt; [85] htmltools_0.5.8.1     lifecycle_1.0.4       MASS_7.3-65",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Relazioni tra variabili</span>"
    ]
  },
  {
    "objectID": "chapters/eda/08_correlation.html#bibliografia",
    "href": "chapters/eda/08_correlation.html#bibliografia",
    "title": "21  Relazioni tra variabili",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nAnscombe, F. J. (1973). Graphs in statistical analysis. The American Statistician, 27(1), 17–21.\n\n\nHamaker, E. (2024). The curious case of the cross-sectional correlation. Multivariate Behavioral Research, 59(6), 1111–1122.\n\n\nPearson, K. (1911). The Grammar of Science: Part I. Physical. Adam & Charles Black.\n\n\nZetsche, U., Buerkner, P.-C., & Renneberg, B. (2019). Future expectations in clinical depression: biased or realistic? Journal of Abnormal Psychology, 128(7), 678–688.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Relazioni tra variabili</span>"
    ]
  },
  {
    "objectID": "chapters/eda/09_causality.html",
    "href": "chapters/eda/09_causality.html",
    "title": "22  Causalità dai dati osservazionali",
    "section": "",
    "text": "Introduzione\nQuando in psicologia ci chiediamo perché un fenomeno si verifica, stiamo entrando nel territorio della causalità. Per esempio: l’esposizione a situazioni di stress causa davvero un aumento dei sintomi depressivi? Oppure si tratta solo di una correlazione, magari dovuta a un terzo fattore che non abbiamo considerato?\nQuesta distinzione tra correlazione e causalità è fondamentale:\nIn psicologia e nelle scienze sociali non possiamo quasi mai fare esperimenti perfettamente controllati, come avviene in laboratorio nelle scienze naturali. Per questo motivo, abbiamo bisogno di strumenti teorici e statistici che ci aiutino a ragionare in termini causali anche quando i dati provengono da studi osservativi.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Causalità dai dati osservazionali</span>"
    ]
  },
  {
    "objectID": "chapters/eda/09_causality.html#introduzione",
    "href": "chapters/eda/09_causality.html#introduzione",
    "title": "22  Causalità dai dati osservazionali",
    "section": "",
    "text": "la correlazione ci dice che due variabili variano insieme;\n\nla causalità implica invece che una variabile produce un cambiamento nell’altra.\n\n\nPanoramica del capitolo\n\nIl problema della causalità in assenza di esperimenti.\nI quattro confondenti fondamentali (catena, biforcazione, collider, discendente).\nLe inferenze dai dati osservazionali.\n\n\n\n\n\n\n\nPrerequisiti\n\n\n\n\n\n\nLeggere Statistical Rethinking. Focalizzati sul capitolo 1 The Golem of Prague.\nLeggere Causal inference with observational data and unobserved confounding variables di Byrnes & Dee (2024).\nLeggere Causal design patterns for data anal\\(Y\\)sts (Riederer, 2021). Questo post sul blog fornisce una panoramica di diversi approcci per fare affermazioni causali dai dati osservazionali.\nLeggere The Effect: An Introduction to Research Design and Causality. Focalizzati sul capitolo 10 Treatment Effects.\nLeggere Causal Inference di Scott Cunningham. Focalizzati sul capitolo 3 Directed Acyclic Graphs.\nLeggere Telling Stories with Data (Alexander, 2023). Concentrati sul capitolo 15 Causalit\\(Y\\) from observational data.\n\n\n\n\n\n\n\n\n\n\nPreparazione del Notebook\n\n\n\n\n\n\nhere::here(\"code\", \"_common.R\") |&gt; \n  source()\nconflicts_prefer(ggplot2::theme_void)",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Causalità dai dati osservazionali</span>"
    ]
  },
  {
    "objectID": "chapters/eda/09_causality.html#il-ruolo-dei-modelli",
    "href": "chapters/eda/09_causality.html#il-ruolo-dei-modelli",
    "title": "22  Causalità dai dati osservazionali",
    "section": "\n22.1 Il ruolo dei modelli",
    "text": "22.1 Il ruolo dei modelli\nImmaginiamo di osservare che studenti che dormono poco tendono a riportare più ansia. Possiamo dire che dormire poco causa ansia? Oppure è possibile che l’ansia causi insonnia? O magari entrambe le variabili sono influenzate da un terzo fattore, ad esempio periodi di esami universitari? Questo semplice esempio mostra che i dati da soli non bastano: dobbiamo avere un modello teorico che ci guidi nell’interpretazione.\nI modelli statistici ci aiutano a formalizzare queste ipotesi causali:\n\nun modello puramente descrittivo si limita a dire che due variabili sono associate;\n\nun modello causale, invece, esplicita ipotesi su come e perché una variabile influenza un’altra.\n\nLo scopo di questo capitolo è introdurre alcuni strumenti concettuali di base per distinguere tra correlazione e causalità e per capire come i modelli possano essere usati in psicologia per affrontare domande causali.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Causalità dai dati osservazionali</span>"
    ]
  },
  {
    "objectID": "chapters/eda/09_causality.html#correlazione-non-significa-causalità",
    "href": "chapters/eda/09_causality.html#correlazione-non-significa-causalità",
    "title": "22  Causalità dai dati osservazionali",
    "section": "\n22.2 Correlazione non significa causalità",
    "text": "22.2 Correlazione non significa causalità\nUno degli errori più comuni in psicologia è confondere la correlazione con la causalità. Se due variabili si muovono insieme, non significa necessariamente che una sia la causa dell’altra.\n\n22.2.1 Esempio classico\nSupponiamo di osservare che, durante l’estate, aumenta sia il consumo di gelati sia il numero di persone che hanno colpi di calore. È forse il consumo di gelati a causare i colpi di calore? Oppure sono i colpi di calore a spingere le persone a mangiare più gelati? La risposta, ovviamente, è che entrambe le variabili sono influenzate da un fattore esterno: la temperatura. Il caldo estivo aumenta sia la voglia di gelato sia il rischio di colpo di calore.\n\n22.2.2 Un esempio psicologico\nDurante i periodi d’esame universitari aumentano sia lo stress sia i disturbi del sonno. Dobbiamo concludere che lo stress causa direttamente insonnia? Oppure che l’insonnia genera stress? La spiegazione più plausibile è che entrambe le variabili dipendano da un fattore esterno comune: la pressione del contesto degli esami.\nQuesta situazione, in cui due variabili sono legate da una causa comune, si chiama confondimento.\n\n22.2.3 Perché è importante in psicologia\nNegli studi psicologici, spesso osserviamo correlazioni interessanti: ad esempio, tra uso dei social media e benessere psicologico. Ma senza un modello causale, non possiamo dire se:\n\nsono i social media a ridurre il benessere,\n\nse chi ha meno benessere tende a usare di più i social,\n\noppure se entrambe le cose dipendono da un terzo fattore, come la solitudine.\n\n22.2.4 La lezione da ricordare\nLa correlazione è un punto di partenza utile, ma non basta per stabilire una relazione di causa-effetto. Per andare oltre, dobbiamo introdurre strumenti che ci aiutino a rappresentare e testare ipotesi causali, come vedremo nelle sezioni successive.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Causalità dai dati osservazionali</span>"
    ]
  },
  {
    "objectID": "chapters/eda/09_causality.html#rappresentare-le-relazioni-causali",
    "href": "chapters/eda/09_causality.html#rappresentare-le-relazioni-causali",
    "title": "22  Causalità dai dati osservazionali",
    "section": "\n22.3 Rappresentare le relazioni causali",
    "text": "22.3 Rappresentare le relazioni causali\nPer ragionare in modo chiaro sulle relazioni di causa-effetto è utile avere un linguaggio formale. Uno degli strumenti più potenti a questo scopo è rappresentare le relazioni tra variabili con un grafo causale.\n\n22.3.1 Che cos’è un grafo causale\nUn grafo causale è un diagramma formato da:\n\n\nnodi, che rappresentano le variabili di interesse (ad esempio: stress, ansia, sonno);\n\n\nfrecce, che indicano ipotesi di influenza causale (ad esempio: lo stress → ansia).\n\nQuesti grafi non sono semplici figure illustrative: sono veri e propri modelli che esplicitano le nostre ipotesi su come le variabili si influenzano.\n\n22.3.2 Un esempio psicologico\nImmaginiamo di voler studiare il legame tra stress e rendimento universitario. Possiamo proporre un grafo causale come questo:\n\nStress → Qualità del sonno → Rendimento agli esami\n\nQui stiamo ipotizzando che lo stress riduca la qualità del sonno, e che a sua volta un sonno peggiore abbassi il rendimento. In altre parole, il sonno agisce da variabile mediatore.\n\n22.3.3 Perché è utile\nRappresentare i dati in questo modo ci permette di:\n\ndistinguere tra relazioni dirette e indirette.\n\nchiarire il ruolo di possibili variabili di confondimento.\n\nprogettare meglio i nostri studi (per esempio, decidere quali variabili misurare per testare una certa ipotesi).\n\n22.3.4 Un avvertimento\nUn grafo causale non ci dice automaticamente quale ipotesi è vera: rappresenta solo ciò che crediamo sia plausibile. Sta poi ai dati e alle analisi statistiche confermare o mettere in discussione queste ipotesi.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Causalità dai dati osservazionali</span>"
    ]
  },
  {
    "objectID": "chapters/eda/09_causality.html#confondimento-mediazione-e-altre-relazioni-causali",
    "href": "chapters/eda/09_causality.html#confondimento-mediazione-e-altre-relazioni-causali",
    "title": "22  Causalità dai dati osservazionali",
    "section": "\n22.4 Confondimento, mediazione e altre relazioni causali",
    "text": "22.4 Confondimento, mediazione e altre relazioni causali\nNei modelli causali non tutte le variabili hanno lo stesso ruolo. È importante distinguere tra diversi tipi di relazioni, perché ognuna porta a interpretazioni diverse.\n\n22.4.1 Variabili di confondimento\nUna variabile di confondimento è una variabile che influenza sia la presunta causa sia l’effetto. Se non la consideriamo, rischiamo di attribuire una relazione causale dove in realtà non c’è.\nEsempio. Osserviamo che gli studenti che usano più spesso i social media riportano più sintomi depressivi. Potremmo pensare che siano i social media a causare depressione. Ma un possibile confondente è la solitudine: gli studenti più soli potrebbero usare di più i social e allo stesso tempo sentirsi più depressi. Se non teniamo conto della solitudine, rischiamo di trarre conclusioni errate.\n\n22.4.2 Variabili mediatrici\nUna variabile mediatrice è invece un “anello di passaggio” nella catena causale. Non distorce la relazione, ma la spiega.\nEsempio. Stress → Qualità del sonno → Ansia. Qui il sonno media l’effetto dello stress sull’ansia: lo stress peggiora il sonno, e il cattivo sonno aumenta l’ansia.\n\n22.4.3 Variabili moderatrici\nUn’altra distinzione utile è quella delle variabili moderatrici: fattori che non spiegano la relazione, ma ne modificano la forza o la direzione.\nEsempio. L’effetto dello stress sull’ansia potrebbe essere più forte negli studenti al primo anno rispetto a quelli degli anni successivi. Qui l’“anno di corso” agisce da moderatore.\nCapire se una variabile è un confondente, un mediatore o un moderatore è fondamentale per interpretare correttamente i dati psicologici e per costruire modelli causali realistici.\n\n22.4.4 Le quattro configurazioni fondamentali nei DAG\nPer capire bene come funzionano i grafi causali, è utile conoscere le quattro strutture di base con cui due variabili possono essere collegate attraverso una terza.\n\n\nCatena (chain)\\(X \\rightarrow Z \\rightarrow Y\\)\nQui \\(Z\\) è un mediatore: trasmette l’effetto di \\(X\\) su \\(Y\\).\n\nSe non controlliamo per \\(Z\\), \\(X\\) e \\(Y\\) appaiono correlati.\n\nSe controlliamo per \\(Z\\), l’effetto di \\(X\\) su \\(Y\\) “scompare”, perché passa interamente da \\(Z\\).\n\n\n\nBiforcazione (fork)\\(X \\leftarrow Z \\rightarrow Y\\)\nQui \\(Z\\) è un confondente: causa sia \\(X\\) che \\(Y\\).\n\nSe non controlliamo per \\(Z\\), vediamo una correlazione spuriosa tra \\(X\\) e \\(Y\\).\n\nSe controlliamo per \\(Z\\), la correlazione spuria scompare.\n\n\n\nCollider\\(X \\rightarrow Z \\leftarrow Y\\)\nQui \\(Z\\) è un effetto comune di \\(X\\) e \\(Y\\).\n\nSe non controlliamo per \\(Z\\), \\(X\\) e \\(Y\\) sono indipendenti.\n\nSe invece controlliamo per \\(Z\\) (o per un suo discendente), creiamo artificialmente una correlazione spuriosa.\n\n\n\nDiscendente\\(X \\rightarrow Y \\rightarrow Z\\)\nQui \\(Z\\) è un discendente di \\(Y\\): porta informazioni sull’effetto di \\(X\\), ma non va confuso con un confondente.\n\nControllare un discendente può introdurre bias, perché significa “tagliare” il percorso naturale della causalità.\n\n\n\n\n22.4.4.1 Perché è importante\nQueste quattro configurazioni sono le “mattonelle di base” con cui si costruiscono tutti i DAG più complessi. Capire quando conviene controllare una variabile (catena, fork) e quando invece non bisogna farlo (collider, discendente) è essenziale per evitare errori nell’inferenza causale.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Causalità dai dati osservazionali</span>"
    ]
  },
  {
    "objectID": "chapters/eda/09_causality.html#dai-grafi-causali-allinferenza-statistica",
    "href": "chapters/eda/09_causality.html#dai-grafi-causali-allinferenza-statistica",
    "title": "22  Causalità dai dati osservazionali",
    "section": "\n22.5 Dai grafi causali all’inferenza statistica",
    "text": "22.5 Dai grafi causali all’inferenza statistica\nFinora abbiamo visto che i grafi causali servono a rappresentare le nostre ipotesi. Ma come possiamo collegarli ai dati e verificare se un modello causale è plausibile?\n\n22.5.1 Indipendenze e dipendenze\nUn’idea chiave è che un grafo causale non descrive solo “chi influenza chi”, ma implica anche certe relazioni di indipendenza tra le variabili.\nQueste relazioni sono importanti perché ci permettono di confrontare il grafo con i dati: se le indipendenze predette non si verificano, il grafo non può essere corretto.\n\n22.5.1.1 Esempio 1: Catena (mediazione)\nSupponiamo di ipotizzare che: Stress → Sonno → Ansia. Qui il sonno media l’effetto dello stress sull’ansia. Se non conosciamo il livello di sonno, stress e ansia risulteranno correlati. Ma condizionando sul sonno, lo stress non ci dà più informazioni aggiuntive sull’ansia.\nIn termini statistici:\n\\[\n\\text{Ansia} \\;\\perp\\!\\!\\!\\perp\\; \\text{Stress} \\mid \\text{Sonno}\n\\]\n\n22.5.1.2 Esempio 2: Fork (causa comune)\nConsideriamo ora: Sonno → Stress; Sonno → Ansia. Qui il sonno è un confondente: influenza sia stress che ansia. Se non controlliamo per il sonno, vediamo una correlazione spuriosa tra stress e ansia. Se invece condizioniamo sul sonno, questa correlazione scompare.\nFormalmente anche qui:\n\\[\n\\text{Ansia} \\;\\perp\\!\\!\\!\\perp\\; \\text{Stress} \\mid \\text{Sonno}\n\\]\n\n22.5.1.3 Esempio 3: Collider (effetto comune)\nOra immaginiamo: Stress → Ansia ← Sonno. Qui l’ansia è un collider, cioè un effetto comune di stress e sonno. In questo caso accade l’opposto:\n\nsenza condizionare su ansia, stress e sonno sono indipendenti;\n\nse invece controlliamo per ansia (o per una sua conseguenza), introduciamo artificialmente una correlazione spuriosa.\n\nFormalmente:\n\\[\n\\text{Stress} \\;\\perp\\!\\!\\!\\perp\\; \\text{Sonno} \\quad \\text{ma non} \\quad \\text{Stress} \\;\\perp\\!\\!\\!\\perp\\; \\text{Sonno} \\mid \\text{Ansia}\n\\]\n\n22.5.2 Tabella riassuntiva\n\n\n\n\n\n\n\n\nStruttura\nForma\nRelazione tra \\(X\\) e \\(Y\\)\n\nDopo aver controllato \\(Z\\)\n\n\n\n\n\nCatena (mediazione)\n\\(X \\to Z \\to Y\\)\nDipendenti\nIndipendenti\n\n\n\nFork (causa comune)\n\\(X \\leftarrow Z \\to Y\\)\nDipendenti (spuria)\nIndipendenti\n\n\n\nCollider (effetto comune)\n\\(X \\to Z \\leftarrow Y\\)\nIndipendenti\nDipendenti (spuri)\n\n\n\n22.5.3 Le quattro configurazioni nei DAG\nDi seguito trovi quattro figure minime che illustrano Catena, Fork, Collider e Discendente.\n\n\n\n\n\n\n\nFigura 22.1: Quattro configurazioni base dei DAG: Catena, Fork, Collider, Discendente.\n\n\n\n\n\n\n\n\n\n\nErrore comune: controllare i collider!\n\n\n\n\n\nCollider: struttura \\(X \\to Z \\leftarrow Y\\).\n\nSenza controllare \\(Z\\): \\(X\\) e \\(Y\\) sono indipendenti.\n\n\nControllando \\(Z\\) (o un suo discendente): si crea una correlazione spuriosa tra \\(X\\) e \\(Y\\).\n\n\n\nMorale pratica:\n- Con catena e fork conviene spesso controllare \\(Z\\) (per rimuovere associazioni non causali o isolare l’effetto diretto).\n- Con i collider (e loro discendenti) non bisogna controllare \\(Z\\): si introduce bias invece di rimuoverlo.\n\n\n\n22.5.4 Perché è importante\nQuesti esempi mostrano che il significato di indipendenza condizionata dipende dalla struttura del grafo. In pratica:\n\ncon catene e fork, condizionare “rompe” la correlazione;\n\ncon i collider, condizionare la crea.\n\nSapere in quali casi conviene controllare una variabile e in quali no è essenziale per fare inferenza causale corretta.\n\n22.5.5 Come si verifica l’indipendenza condizionata?\nCon i dati, possiamo testare questa ipotesi in diversi modi:\n\n\nCorrelazioni parziali: calcoliamo la correlazione tra stress e ansia tenendo costante il sonno.\nSe il modello è corretto, questa correlazione dovrebbe essere vicina a zero.\n\n\nRegressioni multiple: stimiamo una regressione dell’ansia sia sullo stress che sul sonno.\nSe il grafo è corretto, una volta incluso il sonno nel modello, lo stress non dovrebbe più avere un effetto sull’ansia.\n\n\nTest di indipendenza condizionata (nei metodi più avanzati): procedure statistiche apposite che verificano se due variabili sono indipendenti, dati certi controlli.\n\n22.5.6 Perché è importante\nQuesto collegamento tra grafi causali e indipendenze nei dati è il cuore dell’inferenza causale:\n\ni grafi rappresentano le nostre ipotesi teoriche,\nle indipendenze condizionali ci dicono se i dati sono compatibili con quelle ipotesi.\n\nSe un grafo predice un’indipendenza che nei dati non si verifica, dobbiamo concludere che il grafo (cioè la nostra ipotesi causale) è sbagliato o incompleto.\n\n22.5.7 Da ricordare\nQuando diciamo che servono metodi statistici appropriati, intendiamo proprio questi strumenti (correlazioni parziali, regressioni multiple, test di indipendenza) che ci permettono di confrontare le previsioni del grafo con i dati. In questo modo, il linguaggio dei DAG non resta un esercizio astratto, ma diventa un ponte concreto tra teoria psicologica e analisi empirica. In pratica, per gli psicologi, questo significa imparare a usare strumenti come regressioni multiple, correlazioni parziali e modelli di equazioni strutturali: tutti metodi che mettono alla prova le previsioni implicite dei grafi causali.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Causalità dai dati osservazionali</span>"
    ]
  },
  {
    "objectID": "chapters/eda/09_causality.html#come-studiare-la-causalità-in-psicologia",
    "href": "chapters/eda/09_causality.html#come-studiare-la-causalità-in-psicologia",
    "title": "22  Causalità dai dati osservazionali",
    "section": "\n22.6 Come studiare la causalità in psicologia",
    "text": "22.6 Come studiare la causalità in psicologia\nAbbiamo visto che i grafi causali aiutano a rappresentare le ipotesi e che i dati possono confermarle o metterle in dubbio. Ma come si fa, concretamente, a studiare la causalità in psicologia?\n\n22.6.1 Esperimenti controllati\nIl metodo più forte per stabilire una relazione causale è l’esperimento. In un esperimento:\n\ni partecipanti vengono assegnati in modo casuale a due o più gruppi (randomizzazione);\n\nsolo alcuni gruppi ricevono la manipolazione sperimentale (per esempio: una tecnica di rilassamento);\nsi confrontano i risultati nei diversi gruppi.\n\nGrazie alla randomizzazione, eventuali differenze iniziali tra i gruppi vengono “bilanciate”. In questo modo, se emergono differenze negli esiti, possiamo attribuirle con buona sicurezza alla manipolazione.\n\n22.6.2 Quasi-esperimenti\nSpesso però, in psicologia, non possiamo randomizzare i partecipanti. Per esempio: non possiamo decidere chi subisce un trauma o chi sviluppa un disturbo. In questi casi si ricorre ai quasi-esperimenti, in cui cerchiamo di avvicinarci il più possibile alla logica sperimentale usando:\n\ngruppi di confronto selezionati con attenzione,\n\nmisure ripetute prima e dopo un evento,\n\ntecniche statistiche per ridurre le differenze iniziali tra gruppi.\n\n22.6.3 Studi osservativi\nInfine, gran parte della ricerca psicologica si basa su studi osservativi, in cui registriamo ciò che accade naturalmente, senza manipolare nulla. Qui il problema del confondimento è particolarmente serio: dobbiamo usare modelli statistici e controlli accurati per cercare di distinguere tra correlazione e causalità.\nIn sintesi, possiamo dire che:\n\ngli esperimenti sono lo strumento più solido per l’inferenza causale,\n\ni quasi-esperimenti sono un compromesso utile quando la randomizzazione non è possibile,\n\ngli studi osservativi richiedono grande cautela e modelli ben costruiti per trarre conclusioni causali.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Causalità dai dati osservazionali</span>"
    ]
  },
  {
    "objectID": "chapters/eda/09_causality.html#strategie-statistiche-per-affrontare-il-confondimento",
    "href": "chapters/eda/09_causality.html#strategie-statistiche-per-affrontare-il-confondimento",
    "title": "22  Causalità dai dati osservazionali",
    "section": "\n22.7 Strategie statistiche per affrontare il confondimento",
    "text": "22.7 Strategie statistiche per affrontare il confondimento\nQuando non possiamo condurre esperimenti, dobbiamo cercare di “imitare” le condizioni sperimentali con strumenti statistici. Queste tecniche non eliminano del tutto il problema del confondimento, ma ci aiutano a ridurne l’impatto.\n\n22.7.1 Regressione\nIl metodo più semplice e diffuso è la regressione. Inserendo nella regressione variabili che potrebbero agire come confondenti, possiamo stimare l’effetto di una variabile indipendente “a parità di” altre condizioni.\nEsempio: se vogliamo studiare l’effetto dell’uso dei social media sul benessere psicologico, possiamo includere anche la solitudine come predittore. In questo modo, l’effetto stimato dei social media sarà corretto per le differenze di solitudine tra le persone.\n\n22.7.2 Matching\nUn’altra strategia è il matching: si confrontano persone simili tra loro per tutte le caratteristiche rilevanti, tranne che per la variabile di interesse. Per esempio, possiamo confrontare studenti che usano molto i social con altri che li usano poco, ma che sono simili per età, genere, rendimento scolastico e livello di solitudine.\n\n22.7.3 Variabili strumentali\nQuando una variabile di confondimento non è osservata o non può essere misurata, anche i metodi di regressione non bastano. In questi casi, una possibile strategia è usare una variabile strumentale (IV, instrumental variable).\n\n22.7.3.1 Cos’è una variabile strumentale?\nUna variabile \\(Z\\) è un buon strumento per stimare l’effetto di \\(X\\) su \\(Y\\) se rispetta due condizioni:\n\n\nRilevanza: \\(Z\\) influenza \\(X\\).\n\n\nEsogeneità: \\(Z\\) non influenza \\(Y\\) direttamente, né è correlata con i confondenti di \\(X\\) e \\(Y\\).\n\nIn altre parole, lo strumento agisce come una “fonte di variazione casuale” di \\(X\\), che possiamo sfruttare per stimare il suo effetto su \\(Y\\).\n\n22.7.3.2 Perché serve?\nImmagina di voler stimare l’effetto delle ore di sonno (\\(X\\)) sul rendimento a un test (\\(Y\\)). Il problema è che la motivazione (\\(U\\)) è un confondente: studenti più motivati tendono a dormire meglio e a ottenere voti più alti. Se non possiamo misurare la motivazione, la regressione non ci aiuta.\n\n22.7.3.3 Come interviene lo strumento\nSupponiamo di scoprire che il rumore notturno vicino alla residenza universitaria (\\(Z\\)) influenza quante ore di sonno gli studenti riescono a fare (\\(X\\)), ma non influisce direttamente sul loro rendimento (\\(Y\\)).\n\n\n\\(Z\\) → influenza il sonno (\\(X\\)).\n\n\n\\(Z\\) non ha effetti diretti su rendimento, né è legato alla motivazione (\\(U\\)).\n\nIn questo caso, il rumore agisce come strumento: possiamo usare la variazione di sonno “indotta dal rumore” per stimare l’effetto causale del sonno sul rendimento.\n\n22.7.3.4 Da ricordare\n\nLe variabili strumentali sono molto potenti, ma difficili da trovare: serve una variabile che influenzi solo la causa e non l’effetto.\n\nQuando c’è, però, ci permette di “simulare” un esperimento naturale, isolando una fonte quasi-casuale di variazione nella variabile di interesse.\n\nNella pratica psicologica, strumenti così puliti sono rari: l’idea serve più a capire il principio che a trovare sempre uno strumento perfetto.\nQueste tecniche non sostituiscono l’esperimento, ma rappresentano strumenti preziosi per trarre conclusioni causali quando siamo limitati a dati osservativi.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Causalità dai dati osservazionali</span>"
    ]
  },
  {
    "objectID": "chapters/eda/09_causality.html#un-esempio-numerico-di-confondimento",
    "href": "chapters/eda/09_causality.html#un-esempio-numerico-di-confondimento",
    "title": "22  Causalità dai dati osservazionali",
    "section": "\n22.8 Un esempio numerico di confondimento",
    "text": "22.8 Un esempio numerico di confondimento\nImmaginiamo di voler stimare se lo studio pomeridiano (\\(X\\)) migliora il rendimento a un test (\\(Y\\)). Supponiamo però che esista una variabile di confondimento: la motivazione (\\(U\\)).\n\nGli studenti molto motivati tendono sia a studiare più spesso nel pomeriggio (\\(X\\)), sia ad avere voti più alti (\\(Y\\)).\n\nSe non controlliamo per la motivazione, potremmo concludere che il semplice studiare nel pomeriggio causi voti migliori.\n\nEcco dei dati simulati:\n\nset.seed(123)\nn &lt;- 200\nmotivazione &lt;- rbinom(n, 1, 0.5) # 0 = bassa, 1 = alta\nstudio &lt;- rbinom(n, 1, 0.3 + 0.4*motivazione) # più motivati studiano più spesso\nrendimento &lt;- rbinom(n, 1, 0.2 + 0.5*motivazione) # motivazione influenza il voto\n\ntable(studio, rendimento)\n#&gt;       rendimento\n#&gt; studio  0  1\n#&gt;      0 52 43\n#&gt;      1 45 60\n\nSe confrontiamo il rendimento senza considerare la motivazione, otteniamo:\n\nprop.table(table(studio, rendimento), 1)\n#&gt;       rendimento\n#&gt; studio     0     1\n#&gt;      0 0.547 0.453\n#&gt;      1 0.429 0.571\n\nSembra che chi studia nel pomeriggio abbia risultati migliori. Ma in realtà, l’effetto non dipende dallo studio, bensì dalla motivazione, che è il vero fattore causale. Solo controllando per \\(U\\) (ad esempio con una regressione) possiamo separare l’effetto reale da quello spurio.\n\n22.8.1 Cosa impariamo\nQuesto esempio numerico mostra in pratica ciò che i DAG ci aiutano a visualizzare:\n\nquando esiste una variabile di confondimento (come la motivazione), si crea un percorso “indiretto” tra la causa ipotizzata (\\(X\\), lo studio) e l’effetto (\\(Y\\), il rendimento);\n\nquesto percorso indiretto può dare l’illusione di un effetto causale anche quando non c’è.\n\nIn letteratura, questi percorsi indesiderati sono chiamati percorsi back-door: collegamenti che partono “da dietro” la variabile di interesse (\\(X\\)) e portano a \\(Y\\) passando per un’altra variabile.\nPer ottenere una stima corretta dell’effetto causale, dobbiamo bloccare questi percorsi, cioè tenerne conto nell’analisi. Possiamo farlo in diversi modi:\n\nincludendo la variabile di confondimento in una regressione,\n\nselezionando gruppi di confronto simili (matching),\n\noppure, idealmente, usando la randomizzazione sperimentale, che rende i gruppi equivalenti su fattori non osservati.\n\nIn sintesi: senza controllare per la motivazione, i dati ci ingannano; includendo la motivazione, possiamo distinguere tra correlazione spuria e relazione causale reale.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Causalità dai dati osservazionali</span>"
    ]
  },
  {
    "objectID": "chapters/eda/09_causality.html#riflessioni-conclusive",
    "href": "chapters/eda/09_causality.html#riflessioni-conclusive",
    "title": "22  Causalità dai dati osservazionali",
    "section": "Riflessioni conclusive",
    "text": "Riflessioni conclusive\nStudiare la causalità non è solo un esercizio teorico. Ogni volta che uno psicologo legge un articolo, progetta un esperimento o valuta un trattamento, deve chiedersi: ‘Questa relazione che osservo è davvero causale o è il frutto di un confondimento?’ Imparare a distinguere questi casi significa fare scienza psicologica più solida e utile.\nIn questo capitolo abbiamo visto che studiare la causalità in psicologia è complesso, ma non impossibile. Abbiamo distinto tra correlazione e causalità, imparato a rappresentare le nostre ipotesi con i grafi causali, e discusso il ruolo di concetti come confondimento, mediazione e moderazione.\nAbbiamo poi esaminato i principali strumenti per testare le ipotesi causali:\n\n\nesperimenti controllati, il metodo più solido,\n\n\nquasi-esperimenti, utili quando non possiamo randomizzare,\n\n\nstudi osservativi, che richiedono l’uso di tecniche statistiche per ridurre i rischi di interpretazioni errate.\n\nLa lezione principale è che i dati, da soli, non “parlano”: hanno bisogno di un modello teorico che guidi l’interpretazione. Un’analisi statistica, per quanto sofisticata, non può dirci nulla di causale se non abbiamo prima formulato ipotesi chiare sul meccanismo che vogliamo studiare.\n\n\n\n\n\n\nInformazioni sull’ambiente di sviluppo\n\n\n\n\n\n\nsessionInfo()\n#&gt; R version 4.5.1 (2025-06-13)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.6.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] grid      stats     graphics  grDevices utils     datasets  methods  \n#&gt; [8] base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] ggdag_0.2.13          pillar_1.11.0         tinytable_0.13.0     \n#&gt;  [4] patchwork_1.3.2       ggdist_3.3.3          tidybayes_3.0.7      \n#&gt;  [7] bayesplot_1.14.0      ggplot2_3.5.2         reliabilitydiag_0.2.1\n#&gt; [10] priorsense_1.1.1      posterior_1.6.1       loo_2.8.0            \n#&gt; [13] rstan_2.32.7          StanHeaders_2.32.10   brms_2.22.0          \n#&gt; [16] Rcpp_1.1.0            sessioninfo_1.2.3     conflicted_1.2.0     \n#&gt; [19] janitor_2.2.1         matrixStats_1.5.0     modelr_0.1.11        \n#&gt; [22] tibble_3.3.0          dplyr_1.1.4           tidyr_1.3.1          \n#&gt; [25] rio_1.2.3             here_1.0.1           \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;  [1] gridExtra_2.3         inline_0.3.21         sandwich_3.1-1       \n#&gt;  [4] rlang_1.1.6           magrittr_2.0.3        multcomp_1.4-28      \n#&gt;  [7] snakecase_0.11.1      compiler_4.5.1        systemfonts_1.2.3    \n#&gt; [10] vctrs_0.6.5           stringr_1.5.1         pkgconfig_2.0.3      \n#&gt; [13] arrayhelpers_1.1-0    fastmap_1.2.0         backports_1.5.0      \n#&gt; [16] labeling_0.4.3        ggraph_2.2.2          rmarkdown_2.29       \n#&gt; [19] ragg_1.5.0            purrr_1.1.0           xfun_0.53            \n#&gt; [22] cachem_1.1.0          jsonlite_2.0.0        tweenr_2.0.3         \n#&gt; [25] broom_1.0.9           parallel_4.5.1        R6_2.6.1             \n#&gt; [28] stringi_1.8.7         RColorBrewer_1.1-3    boot_1.3-32          \n#&gt; [31] lubridate_1.9.4       estimability_1.5.1    knitr_1.50           \n#&gt; [34] zoo_1.8-14            Matrix_1.7-4          splines_4.5.1        \n#&gt; [37] igraph_2.1.4          timechange_0.3.0      tidyselect_1.2.1     \n#&gt; [40] viridis_0.6.5         abind_1.4-8           yaml_2.3.10          \n#&gt; [43] codetools_0.2-20      curl_7.0.0            dagitty_0.3-4        \n#&gt; [46] pkgbuild_1.4.8        lattice_0.22-7        withr_3.0.2          \n#&gt; [49] bridgesampling_1.1-2  coda_0.19-4.1         evaluate_1.0.5       \n#&gt; [52] survival_3.8-3        polyclip_1.10-7       RcppParallel_5.1.11-1\n#&gt; [55] tensorA_0.36.2.1      checkmate_2.3.3       stats4_4.5.1         \n#&gt; [58] distributional_0.5.0  generics_0.1.4        rprojroot_2.1.1      \n#&gt; [61] rstantools_2.5.0      scales_1.4.0          xtable_1.8-4         \n#&gt; [64] glue_1.8.0            emmeans_1.11.2-8      tools_4.5.1          \n#&gt; [67] graphlayouts_1.2.2    mvtnorm_1.3-3         tidygraph_1.3.1      \n#&gt; [70] QuickJSR_1.8.0        colorspace_2.1-1      nlme_3.1-168         \n#&gt; [73] ggforce_0.5.0         cli_3.6.5             textshaping_1.0.3    \n#&gt; [76] svUnit_1.0.8          viridisLite_0.4.2     Brobdingnag_1.2-9    \n#&gt; [79] V8_7.0.0              gtable_0.3.6          digest_0.6.37        \n#&gt; [82] ggrepel_0.9.6         TH.data_1.1-4         htmlwidgets_1.6.4    \n#&gt; [85] farver_2.1.2          memoise_2.0.1         htmltools_0.5.8.1    \n#&gt; [88] lifecycle_1.0.4       MASS_7.3-65",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Causalità dai dati osservazionali</span>"
    ]
  },
  {
    "objectID": "chapters/eda/09_causality.html#bibliografia",
    "href": "chapters/eda/09_causality.html#bibliografia",
    "title": "22  Causalità dai dati osservazionali",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nAlexander, R. (2023). Telling Stories with Data: With Applications in R. Chapman; Hall/CRC.\n\n\nByrnes, J. E., & Dee, L. E. (2024). Causal inference with observational data and unobserved confounding variables. bioRxiv, 2024–2002.\n\n\nRiederer, E. (2021). Causal design patterns for data analysts. https://emilyriederer.netlify.app/post/causal-design-patterns/",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Causalità dai dati osservazionali</span>"
    ]
  },
  {
    "objectID": "chapters/eda/10_outlier.html",
    "href": "chapters/eda/10_outlier.html",
    "title": "23  Outlier",
    "section": "",
    "text": "Introduzione\nQuando analizziamo dati reali, ci imbattiamo spesso in osservazioni che sembrano molto diverse dalla maggior parte delle altre. Questi valori anomali, chiamati outlier, possono avere origini diverse. Ad esempio, potrebbero derivare da errori di misura o inserimento dati, oppure essere casi estremi ma comunque validi.\nIdentificare e trattare gli outlier in modo appropriato è importante per evitare che distorcano i risultati dell’analisi. Tuttavia, non esiste una definizione universale di outlier: dipende dal contesto e dall’obiettivo dell’analisi. In questo capitolo, esploreremo diversi metodi per individuare gli outlier, concentrandoci su tecniche robuste che minimizzano l’influenza di questi valori anomali sulle statistiche descrittive (Simmons et al., 2011).",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Outlier</span>"
    ]
  },
  {
    "objectID": "chapters/eda/10_outlier.html#introduzione",
    "href": "chapters/eda/10_outlier.html#introduzione",
    "title": "23  Outlier",
    "section": "",
    "text": "Panoramica del capitolo\n\ncomprendere il ruolo e gli effetti degli outlier;\nindividuare outlier con metodi univariati e multivariati;\nutilizzare il pacchetto {performance} in R per rilevarli;\ndocumentare e rendere riproducibili le procedure;\nconsiderare alternative (es. winsorizzazione) e preregistrare le scelte.\n\n\n\n\n\n\n\nPrerequisiti\n\n\n\n\n\n\nLeggere “Check your outliers! An introduction to identifying statistical outliers in R with easystats” (Thériault et al., 2024).\n\n\n\n\n\n\n\n\n\n\nPreparazione del Notebook\n\n\n\n\n\n\nhere::here(\"code\", \"_common.R\") |&gt; \n  source()\n\n# Load packages\nif (!requireNamespace(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(performance, see, datawizard, MASS)",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Outlier</span>"
    ]
  },
  {
    "objectID": "chapters/eda/10_outlier.html#individuare-e-gestire-gli-outlier",
    "href": "chapters/eda/10_outlier.html#individuare-e-gestire-gli-outlier",
    "title": "23  Outlier",
    "section": "\n23.1 Individuare e gestire gli outlier",
    "text": "23.1 Individuare e gestire gli outlier\nIdentificare ed eventualmente eliminare gli outlier rappresenta una fase cruciale dell’analisi dei dati, poiché la presenza di valori anomali può influenzare fortemente le conclusioni che si traggono da analisi statistiche. Gli outlier possono infatti alterare notevolmente statistiche descrittive come media e deviazione standard, ma anche misure di relazione come correlazioni e regressioni. Ciò avviene perché molte tecniche statistiche comuni (ad esempio, la media aritmetica o la regressione lineare con metodo dei minimi quadrati) sono particolarmente sensibili ai valori estremi.\nAd esempio, se stiamo analizzando il reddito medio di un gruppo di persone e includiamo erroneamente dati di reddito estremamente elevati o inseriti per errore, la media risultante sarà molto più alta del reale valore tipico del gruppo, producendo una rappresentazione fuorviante della situazione.\n\n23.1.1 L’importanza della visualizzazione dei dati\nLa rappresentazione grafica dei dati è uno strumento fondamentale per individuare rapidamente la presenza di outlier. Grafici come boxplot, istogrammi e scatterplot consentono di identificare visivamente valori anomali che si discostano dalla distribuzione generale. Tuttavia, queste tecniche sono efficaci principalmente per outlier unidimensionali o bidimensionali. Nel caso di outlier multidimensionali, l’analisi visiva diventa insufficiente e si rende necessario l’utilizzo di metodi statistici più avanzati, come il calcolo della distanza di Mahalanobis.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Outlier</span>"
    ]
  },
  {
    "objectID": "chapters/eda/10_outlier.html#come-identificare-gli-outlier",
    "href": "chapters/eda/10_outlier.html#come-identificare-gli-outlier",
    "title": "23  Outlier",
    "section": "\n23.2 Come identificare gli outlier",
    "text": "23.2 Come identificare gli outlier\nOltre alla visualizzazione grafica, esistono tecniche statistiche specifiche che consentono di identificare gli outlier in modo sistematico.\n\n23.2.1 I boxplot\nUno strumento semplice e intuitivo per individuare gli outlier è il boxplot. Il boxplot riassume la distribuzione di una variabile mostrando la mediana, il primo e il terzo quartile (Q1 e Q3) e due estremi, detti “whiskers”. I punti al di fuori di questi whiskers sono considerati potenziali outlier.\nEsempio in R:\n\ndata &lt;- data.frame(\n  value = c(rnorm(100, mean = 10, sd = 2), 30)\n  ) # Aggiungiamo un outlier\n\nggplot(data, aes(y = value)) +\n  geom_boxplot() +\n  coord_flip()\n\n\n\n\n\n\n\nSe il boxplot mostra un punto isolato lontano dagli altri dati, potrebbe essere un outlier.\n\n23.2.2 Metodi basati sulla variabilità\n\n23.2.2.1 Intervallo interquartile (IQR)\nJohn Tukey ha introdotto una definizione operativa di outlier basata sull’Interquartile Range (IQR), ovvero la differenza tra il terzo e il primo quartile:\n\nI valori inferiori a \\(Q1 - 1.5 \\times IQR\\) o superiori a \\(Q3 + 1.5 \\times IQR\\) sono considerati outlier moderati.\nI valori oltre \\(Q1 - 3 \\times IQR\\) o \\(Q3 + 3 \\times IQR\\) sono definiti far out outliers.\n\nEsempio in R:\n\nQ1 &lt;- quantile(data$value, 0.25)\nQ3 &lt;- quantile(data$value, 0.75)\nIQR_value &lt;- Q3 - Q1\nlower_bound &lt;- Q1 - 1.5 * IQR_value\nupper_bound &lt;- Q3 + 1.5 * IQR_value\n\noutliers &lt;- data$value[data$value &lt; lower_bound | data$value &gt; upper_bound]\noutliers\n#&gt; [1] 30\n\nQuesto metodo è efficace per distribuzioni simmetriche, ma potrebbe non funzionare bene con dati asimmetrici.\n\n23.2.2.2 Median Absolute Deviation (MAD)\nUn metodo più robusto rispetto all’IQR è il Median Absolute Deviation (MAD), che utilizza la mediana anziché la media per stimare la dispersione:\n\nmad_value &lt;- mad(data$value)\nthreshold &lt;- 3 * mad_value # Soglia classica per gli outlier\n\noutliers_mad &lt;- data$value[abs(data$value - median(data$value)) &gt; threshold]\noutliers_mad\n#&gt; [1] 30\n\nIl MAD è meno sensibile agli outlier rispetto alla deviazione standard ed è spesso preferito per dati con distribuzioni non normali.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Outlier</span>"
    ]
  },
  {
    "objectID": "chapters/eda/10_outlier.html#outlier-multivariati",
    "href": "chapters/eda/10_outlier.html#outlier-multivariati",
    "title": "23  Outlier",
    "section": "\n23.3 Outlier multivariati",
    "text": "23.3 Outlier multivariati\nQuando si considerano più variabili contemporaneamente, un valore potrebbe non apparire anomalo su una singola variabile, ma esserlo nel contesto dell’intero dataset. Un metodo comune per individuare questi outlier è la Distanza di Mahalanobis, che tiene conto delle correlazioni tra variabili.\n\nCon la distanza “normale” (come quella che misuri con un righello), se una persona è più alta o più pesante della media, la distanza è calcolata in modo “isolato”, senza considerare che altezza e peso sono spesso correlate (persone più alte tendono a pesare di più).\nCon la distanza di Mahalanobis, invece, si osserva il “contesto” dei dati. Se tutti nel gruppo hanno un’altezza e un peso che crescono in modo coordinato (ad esempio, ogni 10 cm in più corrispondono a 8 kg in più), questa distanza valuta se la nuova persona si allontana da questo schema generale. Ad esempio, una persona molto alta ma con peso medio potrebbe essere considerata più “anomala” di una persona altrettanto alta ma più pesante, perché viola la relazione tipica del gruppo.\n\nPer comprendere intuitivamente la distanza di Mahalanobis, immaginiamo di avere una nube di punti che rappresentano individui, ciascuno con i propri valori di altezza e peso. Il “centro” di questa nube è un punto ideale che rappresenta una sorta di media multivariata (tenendo conto sia dell’altezza sia del peso). La distanza di Mahalanobis misura quanto ogni singolo individuo si allontana da questo centro, considerando la variabilità congiunta delle variabili (ad esempio, la correlazione tra altezza e peso). Se un individuo presenta caratteristiche molto diverse rispetto alla maggioranza, la sua distanza di Mahalanobis sarà elevata, segnalando un potenziale outlier.\n\n\n\n\n\nFigura 23.1: Soglie per la detezione degli outliers (bande grigie) nel caso di una metrica unidimensionale (pannello di sinistra) e nel caso di una rappresentazione multivariata della varianza (pannello di destra) – figura creata da Sergen Cansiz.\n\n\n\n\n\n\n\n\nDistanza di Mahalanobis\n\n\n\n\n\nConsideriamo ora una definizione della distanza di Mahalanobis nel caso bivariato (due variabili). Immaginiamo di avere due variabili come altezza (\\(X\\)) e peso (\\(Y\\)), con:\n\n\nMedie: \\(\\mu_X\\) (altezza media del gruppo), \\(\\mu_Y\\) (peso medio del gruppo).\n\n\nVarianze: \\(\\sigma_X^2\\) (quanto varia l’altezza), \\(\\sigma_Y^2\\) (quanto varia il peso).\n\n\nCorrelazione: \\(\\rho\\) (quanto \\(X\\) e \\(Y\\) sono legate, ad esempio: se l’altezza aumenta, di quanto aumenta solitamente il peso?).\n\nPer un nuovo individuo con altezza \\(x\\) e peso \\(y\\), la distanza di Mahalanobis (\\(D\\)) si calcola così:\n\n\nCalcolare le differenze rispetto alla media:\n\nQuanto si discosta l’altezza: \\((x - \\mu_X)\\).\nQuanto si discosta il peso: \\((y - \\mu_Y)\\).\n\n\n\nScalare le differenze con le varianze:\n\n\nDividere ogni differenza per la sua “variabilità tipica” (deviazione standard \\(\\sigma_X\\) e \\(\\sigma_Y\\)):\n\\[\n\\frac{(x - \\mu_X)}{\\sigma_X} \\quad \\text{e} \\quad \\frac{(y - \\mu_Y)}{\\sigma_Y} .\n\\]\n\n\n\n\nCorreggere per la correlazione:\n\nSe \\(X\\) e \\(Y\\) sono correlate (\\(\\rho \\neq 0\\)), modifica le differenze per tenere conto di come di solito si “muovono insieme”.\n\nLa formula finale combina tutto in un unico valore:\\[\nD = \\sqrt{ \\frac{ \\left( \\frac{(x - \\mu_X)}{\\sigma_X} \\right)^2 + \\left( \\frac{(y - \\mu_Y)}{\\sigma_Y} \\right)^2 - 2 \\rho \\left( \\frac{(x - \\mu_X)}{\\sigma_X} \\right)\\left( \\frac{(y - \\mu_Y)}{\\sigma_Y} \\right) }{1 - \\rho^2} }\n\\]\n\n\n\n\nSpiegazione:\n\nSenza correlazione (\\(\\rho = 0\\)), sarebbe come una distanza Euclidea “scalata” dalle varianze.\n\nCon correlazione (\\(\\rho \\neq 0\\)), sottrai un termine che “aggiusta” la distanza in base a quanto \\(X\\) e \\(Y\\) tendono a variare insieme.\n\nIl denominatore \\(1 - \\rho^2\\) normalizza il risultato, per evitare che la correlazione distorca troppo la misura.\n\nEsempio:\nSe tutti gli alti sono anche pesanti (\\(\\rho\\) positivo), un individuo alto ma magro avrà una distanza di Mahalanobis maggiore rispetto a uno altrettanto alto ma pesante, perché viola la relazione tipica del gruppo.\n\n\n\n\n\n\n\n\n\nDistanza Eucliea\n\n\n\n\n\nRicordiamo che la distanza euclidea tra due punti \\((x_1, y_1)\\) e \\((x_2, y_2)\\) in un piano cartesiano è definita come:\n\\[\nd = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}\n\\]\n\n\n\nEsempio in R:\n\nX &lt;- as.matrix(mtcars[, c(\"mpg\", \"hp\")])\ncenter &lt;- colMeans(X)\ncov_matrix &lt;- cov(X)\nmahal_dist &lt;- mahalanobis(X, center, cov_matrix)\n\nthreshold &lt;- qchisq(0.975, df = ncol(X)) # Soglia al 97.5%\noutliers_mahal &lt;- X[mahal_dist &gt; threshold, ]\noutliers_mahal\n#&gt; mpg  hp \n#&gt;  15 335\n\nQuesto metodo è utile per dataset con più variabili correlate, come misure biometriche (altezza e peso).\nTuttavia, la versione classica di questa misura non è particolarmente robusta: la presenza stessa di outlier può distorcere il calcolo del “centro” e della variabilità complessiva, rendendo meno affidabile l’individuazione di altri valori anomali. Per questo motivo, si preferisce utilizzare una variante più resistente, la Minimum Covariance Determinant (MCD), che diminuisce l’influenza degli outlier stessi nel processo di identificazione.\nAll’interno del pacchetto {performance} in R, è possibile applicare questa variante robusta utilizzando la funzione check_outliers() con l’argomento method = \"mcd\". In questo modo, è possibile individuare gli outlier multivariati in maniera più solida e coerente, anche quando si lavora con dati fortemente influenzati da valori estremi.\n\nd &lt;- mtcars[, c(\"mpg\", \"hp\")]\noutliers &lt;- performance::check_outliers(d, method = \"mcd\", verbose = FALSE)\noutliers\n#&gt; 2 outliers detected: cases 20, 31.\n#&gt; - Based on the following method and threshold: mcd (13.816).\n#&gt; - For variables: mpg, hp.\n\nSi possono poi visualizzare questi outlier:\n\nplot(outliers)\n\n\n\n\n\n\n\nSono disponibili anche altre varianti multivariate documentate nella help page della funzione.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Outlier</span>"
    ]
  },
  {
    "objectID": "chapters/eda/10_outlier.html#cosa-fare-con-gli-outlier",
    "href": "chapters/eda/10_outlier.html#cosa-fare-con-gli-outlier",
    "title": "23  Outlier",
    "section": "\n23.4 Cosa fare con gli outlier?",
    "text": "23.4 Cosa fare con gli outlier?\nUna volta identificati gli outlier, dobbiamo decidere se rimuoverli, correggerli o mantenerli (Leys et al., 2019). Alcuni approcci comuni includono:\n\n\nVerificare la fonte del dato: un errore di inserimento può essere corretto.\n\nRimuovere gli outlier estremi: utile se il valore è chiaramente un errore di misura.\n\nUsare metodi robusti: strumenti come la mediana o il MAD sono meno influenzati dagli outlier.\n\nTrasformare i dati: applicare logaritmi o altre trasformazioni può ridurre l’impatto degli outlier.\n\nWinsorizzazione: invece di rimuovere gli outlier, possiamo limitarli a un massimo accettabile.\n\n\n\n\n\n\n\nWinsorizzazione\n\n\n\n\n\nNella Winsorizzazione, invece di eliminare gli outlier, si sostituiscono i valori troppo alti o troppo bassi con i valori più vicini considerati “accettabili”, mantenendo però la struttura generale dei dati.\nCome funziona?\n1. Definisci i limiti:\n\nDecidi una “soglia” per identificare gli outlier, ad esempio il 5° percentile (valore sotto cui cade il 5% dei dati più bassi) e il 95° percentile (valore sopra cui cade il 5% dei dati più alti).\n\nQueste soglie dipendono dal contesto: puoi usare percentili diversi (es. 1° e 99°) in base a quanto vuoi essere severo nel definire gli outlier.\n\n\n\nSostituisci gli outlier:\n\n\nValori troppo bassi: Tutti i dati sotto il 5° percentile vengono sostituiti con il valore del 5° percentile.\n\n\nValori troppo alti: Tutti i dati sopra il 95° percentile vengono sostituiti con il valore del 95° percentile.\n\n\n\nEsempio concreto:\nSupponiamo di avere i seguenti dati su 10 esami (ordinati):40, 55, 60, 65, 70, 75, 80, 85, 90, 200\n\n\n5° percentile: 55 (il valore sotto cui cade il 5% dei dati).\n\n\n95° percentile: 90 (il valore sopra cui cade il 5% dei dati).\n\nDopo la Winsorizzazione:\n- Il valore più basso (40) diventa 55.\n- Il valore più alto (200) diventa 90.\nNuovi dati: 55, 55, 60, 65, 70, 75, 80, 85, 90, 90.\nPerché usarla?\n\n\nMantiene la dimensione del dataset: Non si eliminano dati, ma si modificano solo gli outlier.\n\nRiduce la distorsione: Gli outlier estremi non “trascinano” la media o altre statistiche.\n\n\nUtile in contesti sensibili: Ad esempio, in finanza (per gestire rendimenti anomali) o nelle analisi mediche (per evitare che valori estremi falsino i risultati).\n\n\n\n\nNel pacchetto easystats, la funzione winsorize() di datawizard semplifica il compito di Winsorizzazione:\nwinsorized_data &lt;- \n  winsorize(data$value, method = \"zscore\", robust = TRUE, threshold = 3)",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Outlier</span>"
    ]
  },
  {
    "objectID": "chapters/eda/10_outlier.html#importanza-della-trasparenza",
    "href": "chapters/eda/10_outlier.html#importanza-della-trasparenza",
    "title": "23  Outlier",
    "section": "\n23.5 Importanza della trasparenza",
    "text": "23.5 Importanza della trasparenza\nQualunque decisione va documentata chiaramente: quanti outlier sono stati individuati, con quale metodo, a quale threshold, come sono stati gestiti, e preferibilmente con il codice R utilizzato. La preregistrazione e la condivisione dei dati e del codice (ad es. su OSF) sono pratiche consigliate per garantire riproducibilità e trasparenza.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Outlier</span>"
    ]
  },
  {
    "objectID": "chapters/eda/10_outlier.html#riflessioni-conclusive",
    "href": "chapters/eda/10_outlier.html#riflessioni-conclusive",
    "title": "23  Outlier",
    "section": "Riflessioni conclusive",
    "text": "Riflessioni conclusive\nAbbiamo mostrato come identificare gli outlier in modo coerente e trasparente, allineandoci alle buone pratiche correnti. Tuttavia, la buona pratica non si limita alla scelta degli algoritmi: è fondamentale anche preregistrare le decisioni, essere coerenti, trasparenti e fornire giustificazioni.\n\n\n\n\n\n\nProblemi 1\n\n\n\n\n\nDomande teoriche\n\nCos’è un outlier?\nPerché è importante identificare e trattare correttamente gli outlier?\nDescrivi brevemente il metodo del boxplot per identificare gli outlier.\nCosa si intende per Interquartile Range (IQR) e come viene utilizzato per individuare gli outlier?\nQual è la differenza tra il metodo IQR e il Median Absolute Deviation (MAD) per l’identificazione degli outlier?\nCos’è la distanza di Mahalanobis e in che modo può aiutare nell’identificazione degli outlier multivariati?\nPerché la distanza di Mahalanobis classica potrebbe non essere robusta? Come si può migliorare l’approccio?\nQuali sono le opzioni per gestire gli outlier una volta identificati?\nCos’è la Winsorizzazione e in quali casi potrebbe essere utile?\nPerché è importante la trasparenza nelle decisioni riguardanti gli outlier?\n\n\n\n\n\n\n\n\n\n\nSoluzioni 1\n\n\n\n\n\n\n\nCos’è un outlier?\n\nUn outlier è un’osservazione che si discosta significativamente dalla maggior parte delle altre osservazioni in un insieme di dati. Può essere dovuto ad errori di misura, errori di inserimento dati o a casi estremi ma validi.\n\n\n\nPerché è importante identificare e trattare correttamente gli outlier?\n\nGli outlier possono distorcere i risultati dell’analisi statistica, portando a conclusioni errate. Identificarli e trattarli correttamente aiuta a ridurre l’effetto di questi valori anomali sulle statistiche descrittive e sulle inferenze statistiche.\n\n\n\nDescrivi brevemente il metodo del boxplot per identificare gli outlier.\n\nIl boxplot visualizza la distribuzione di una variabile, mostrando la mediana, il primo e il terzo quartile, e due estremi (“whiskers”). I punti al di fuori di questi whiskers sono considerati potenziali outlier.\n\n\n\nCosa si intende per Interquartile Range (IQR) e come viene utilizzato per individuare gli outlier?\n\nL’IQR è la differenza tra il terzo e il primo quartile di un insieme di dati. Valori inferiori a \\(Q1 - 1.5 \\times IQR\\) o superiori a \\(Q3 + 1.5 \\times IQR\\) sono considerati outlier moderati. Valori oltre \\(Q1 - 3 \\times IQR\\) o \\(Q3 + 3 \\times IQR\\) sono definiti “far out” outliers.\n\n\n\nQual è la differenza tra il metodo IQR e il Median Absolute Deviation (MAD) per l’identificazione degli outlier?\n\nIl metodo IQR si basa sulla differenza tra il terzo e il primo quartile, mentre il MAD utilizza la mediana delle deviazioni assolute dalla mediana per stimare la dispersione. Il MAD è meno sensibile agli outlier rispetto all’IQR e alla deviazione standard, rendendolo preferibile per dati con distribuzioni non normali.\n\n\n\nCos’è la distanza di Mahalanobis e in che modo può aiutare nell’identificazione degli outlier multivariati?\n\nLa distanza di Mahalanobis misura quanto un punto si discosta dal centro della distribuzione di un set di dati multivariato, tenendo conto della correlazione tra le variabili. Valori con distanze di Mahalanobis elevate sono potenziali outlier multivariati.\n\n\n\nPerché la distanza di Mahalanobis classica potrebbe non essere robusta? Come si può migliorare l’approccio?\n\nLa distanza di Mahalanobis classica può essere distorta dalla presenza di outlier, che influenzano il calcolo del centro e della variabilità complessiva. Un approccio più robusto è la Minimum Covariance Determinant (MCD), che riduce l’influenza degli outlier nel processo di identificazione.\n\n\n\nQuali sono le opzioni per gestire gli outlier una volta identificati?\n\nLe opzioni includono: verificare la fonte del dato per possibili errori, rimuovere gli outlier estremi, usare metodi robusti come la mediana o il MAD, trasformare i dati (ad esempio, logaritmi), e limitare gli outlier attraverso la Winsorizzazione.\n\n\n\nCos’è la Winsorizzazione e in quali casi potrebbe essere utile?\n\nLa Winsorizzazione è una tecnica che consiste nel sostituire gli outlier estremi con il valore massimo o minimo accettabile. È utile quando si vuole mantenere la dimensione del dataset e ridurre l’impatto degli outlier senza rimuoverli completamente.\n\n\nPerché è importante la trasparenza nelle decisioni riguardanti gli outlier?\n\n\nLa trasparenza aiuta a garantire la riproducibilità e la validità dell’analisi. Documentare le decisioni, inclusi i metodi e i threshold utilizzati, consente ad altri di capire e valutare l’impatto di queste decisioni sui risultati dell’analisi.\n\n\n\n\n\n\n\n\n\n\nProblemi 2\n\n\n\n\n\nEsercizio: Gestione degli Outlier nella Scala di Soddisfazione di Vita (SWLS)\nScopo:\nImparare a individuare e correggere gli outlier in un dataset che misura la soddisfazione di vita (SWLS). L’esercizio prevede l’inserimento artificiale di due outlier (uno molto alto e uno molto basso) nei dati raccolti, per poi gestirli con i metodi discussi nel capitolo. Infine, bisognerà consegnare:\n\nUn file .qmd (Quarto) con tutto il codice e i commenti delle operazioni svolte.\n\nUn file CSV finale con i dati “puliti” (ossia senza i due outlier anomali) o con i valori modificati mediante il metodo scelto (winsorizzazione, rimozione, correzione, ecc.).\n\nFasi e Istruzioni\n\n\nScarica o carica il dataset SWLS\n\nNominare il dataset originale, ad esempio SWLS_raw.csv, contenente i punteggi dei partecipanti sulla Scala di Soddisfazione di Vita (SWLS).\n\nAssicurati di avere nel dataset almeno le colonne:\n\n\nid (identificatore univoco del partecipante)\n\n\nswls_score (punteggio totale alla scala SWLS)\n\n\n\n\n\nCrea due outlier artificiali\n\nScegli un partecipante al quale assegnare un valore estremamente basso di swls_score (es. -999) e un altro partecipante con un valore estremamente alto (es. 999).\n\nSpiega brevemente nel .qmd dove e come hai inserito questi valori.\n\n\n\nAnalizza i dati alla ricerca di outlier\n\nVisualizza la distribuzione tramite un boxplot e/o un istogramma.\n\nCalcola i valori soglia utilizzando almeno uno dei metodi visti:\n\nIQR (intervallo interquartile)\n\nMAD (Median Absolute Deviation)\n\n\n\nMostra quali osservazioni vengono segnalate come potenziali outlier.\n\n\n\nDecidi come gestire gli outlier\n\nScegli se rimuoverli, winsorizzarli o correggerli.\n\nGiustifica la tua scelta: spiega perché quel metodo è appropriato per questi dati o perché preferisci un approccio rispetto a un altro.\n\n\n\nGenera i dati “puliti”\n\nApplica il metodo selezionato.\n\nSalva il dataset risultante (senza i valori anomali o con i valori modificati) in un file CSV chiamato SWLS_clean.csv.\n\n\n\nDocumenta tutto in un file .qmd\n\nIncludi codice R, commenti e brevi spiegazioni testuali dei vari passaggi.\n\nMostra i risultati rilevanti (boxplot, calcolo dei soglie IQR/MAD, elenco degli outlier individuati, ecc.).\n\nAssicurati di eseguire il rendering del .qmd in modo che l’istruttore possa vedere sia l’output che il codice.\n\n\n\nConsegnare i file\n\n\nFile .qmd: deve contenere tutto il codice e i passaggi effettuati (inclusi grafici, calcoli e spiegazioni).\n\n\nFile CSV “pulito” (SWLS_clean.csv): con i dati finali dopo il trattamento degli outlier.\n\n\n\nSuggerimenti\n\n\nStruttura il tuo .qmd in sezioni (ad es. Caricamento dati, Creazione outlier artificiali, Identificazione outlier, Gestione outlier, Salvataggio dati puliti).\n\n\nMotiva sempre le scelte, soprattutto se rimuovi o modifichi i dati originali: spiega perché il valore appare come un errore di misura o un valore estremo.\n\n\nFai controlli incrociati: potresti usare più di un metodo (boxplot, IQR, MAD) per vedere se l’outlier viene segnalato in tutti i casi.\n\n\nDocumenta la tua strategia di trasparenza nell’analisi: note sull’eventuale preregistrazione di come avresti gestito gli outlier o su come hai deciso i threshold.\n\n\n\n\n\n\n\n\n\n\nInformazioni sull’ambiente di sviluppo\n\n\n\n\n\n\nsessionInfo()\n#&gt; R version 4.5.1 (2025-06-13)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.6.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] MASS_7.3-65           datawizard_1.2.0      see_0.11.0           \n#&gt;  [4] performance_0.15.1    pillar_1.11.0         tinytable_0.13.0     \n#&gt;  [7] patchwork_1.3.2       ggdist_3.3.3          tidybayes_3.0.7      \n#&gt; [10] bayesplot_1.14.0      ggplot2_3.5.2         reliabilitydiag_0.2.1\n#&gt; [13] priorsense_1.1.1      posterior_1.6.1       loo_2.8.0            \n#&gt; [16] rstan_2.32.7          StanHeaders_2.32.10   brms_2.22.0          \n#&gt; [19] Rcpp_1.1.0            sessioninfo_1.2.3     conflicted_1.2.0     \n#&gt; [22] janitor_2.2.1         matrixStats_1.5.0     modelr_0.1.11        \n#&gt; [25] tibble_3.3.0          dplyr_1.1.4           tidyr_1.3.1          \n#&gt; [28] rio_1.2.3             here_1.0.1           \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;  [1] gridExtra_2.3         inline_0.3.21         sandwich_3.1-1       \n#&gt;  [4] rlang_1.1.6           magrittr_2.0.3        multcomp_1.4-28      \n#&gt;  [7] snakecase_0.11.1      compiler_4.5.1        systemfonts_1.2.3    \n#&gt; [10] vctrs_0.6.5           stringr_1.5.1         pkgconfig_2.0.3      \n#&gt; [13] arrayhelpers_1.1-0    fastmap_1.2.0         backports_1.5.0      \n#&gt; [16] labeling_0.4.3        rmarkdown_2.29        ragg_1.5.0           \n#&gt; [19] purrr_1.1.0           xfun_0.53             cachem_1.1.0         \n#&gt; [22] jsonlite_2.0.0        broom_1.0.9           parallel_4.5.1       \n#&gt; [25] R6_2.6.1              stringi_1.8.7         RColorBrewer_1.1-3   \n#&gt; [28] lubridate_1.9.4       estimability_1.5.1    knitr_1.50           \n#&gt; [31] zoo_1.8-14            pacman_0.5.1          Matrix_1.7-4         \n#&gt; [34] splines_4.5.1         timechange_0.3.0      tidyselect_1.2.1     \n#&gt; [37] abind_1.4-8           codetools_0.2-20      curl_7.0.0           \n#&gt; [40] pkgbuild_1.4.8        lattice_0.22-7        withr_3.0.2          \n#&gt; [43] bridgesampling_1.1-2  coda_0.19-4.1         evaluate_1.0.5       \n#&gt; [46] survival_3.8-3        RcppParallel_5.1.11-1 tensorA_0.36.2.1     \n#&gt; [49] checkmate_2.3.3       stats4_4.5.1          insight_1.4.2        \n#&gt; [52] distributional_0.5.0  generics_0.1.4        rprojroot_2.1.1      \n#&gt; [55] rstantools_2.5.0      scales_1.4.0          xtable_1.8-4         \n#&gt; [58] glue_1.8.0            emmeans_1.11.2-8      tools_4.5.1          \n#&gt; [61] mvtnorm_1.3-3         grid_4.5.1            QuickJSR_1.8.0       \n#&gt; [64] colorspace_2.1-1      nlme_3.1-168          cli_3.6.5            \n#&gt; [67] textshaping_1.0.3     svUnit_1.0.8          viridisLite_0.4.2    \n#&gt; [70] Brobdingnag_1.2-9     V8_7.0.0              gtable_0.3.6         \n#&gt; [73] digest_0.6.37         TH.data_1.1-4         htmlwidgets_1.6.4    \n#&gt; [76] farver_2.1.2          memoise_2.0.1         htmltools_0.5.8.1    \n#&gt; [79] lifecycle_1.0.4",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Outlier</span>"
    ]
  },
  {
    "objectID": "chapters/eda/10_outlier.html#bibliografia",
    "href": "chapters/eda/10_outlier.html#bibliografia",
    "title": "23  Outlier",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nLeys, C., Delacre, M., Mora, Y. L., Lakens, D., & Ley, C. (2019). How to classify, detect, and manage univariate and multivariate outliers, with emphasis on pre-registration. International Review of Social Psychology, 32(1).\n\n\nSimmons, J. P., Nelson, L. D., & Simonsohn, U. (2011). False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. Psychological science, 22(11), 1359–1366.\n\n\nThériault, R., Ben-Shachar, M. S., Patil, I., Lüdecke, D., Wiernik, B. M., & Makowski, D. (2024). Check your outliers! An introduction to identifying statistical outliers in R with easystats. Behavior Research Methods, 56(4), 4162–4172.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Outlier</span>"
    ]
  },
  {
    "objectID": "chapters/epiloque/epiloque.html",
    "href": "chapters/epiloque/epiloque.html",
    "title": "Considerazioni Conclusive",
    "section": "",
    "text": "Limiti dell’Inferenza Frequentista\nL’inferenza bayesiana rappresenta un approccio rigoroso e trasparente per integrare conoscenze pregresse e dati empirici nell’analisi psicologica. A differenza dei metodi frequentisti, l’approccio bayesiano consente di quantificare l’incertezza e di costruire modelli che riflettono le nostre aspettative iniziali. Questa flessibilità è particolarmente preziosa in psicologia, dove teorie e ipotesi svolgono un ruolo centrale nel guidare la ricerca. L’inferenza bayesiana rende esplicite le nostre assunzioni a priori e ci permette di valutare come i dati influenzano la nostra comprensione dei fenomeni psicologici.\nIn questo corso, abbiamo esaminato i limiti dell’inferenza frequentista, specialmente quando utilizzata come “filtro” per distinguere risultati scientifici rilevanti da quelli trascurabili. L’eccessiva dipendenza dai valori-p è stata ampiamente criticata per la sua associazione con inferenze inadeguate. Gli effetti possono essere sovrastimati, talvolta anche nella direzione sbagliata, quando la stima è vincolata alla significatività statistica in presenza di dati altamente variabili (Loken & Gelman, 2017).\nNonostante le critiche di lunga data e i dibattiti sul loro uso improprio (Gardner e Altman, 1986; Cohen, 1994; Anderson et al., 2000; Fidler et al., 2004; Finch et al., 2004), i valori-p persistono come indicatore di significatività. Questa tenacia riflette forse la necessità dei ricercatori di avere strumenti intuitivi, sebbene semplificati, per interpretare i dati. Tuttavia, l’uso rigido di soglie arbitrarie (ad esempio, 0.05, 0.01, 0.001) ha trasformato il raggiungimento della significatività in un obiettivo fine a se stesso, piuttosto che in uno strumento per comprendere i fenomeni sottostanti (Cohen, 1994; Kirk, 1996). Inoltre, i valori-p possono solo rifiutare l’ipotesi nulla, ma non confermarla, poiché un risultato non significativo non implica l’assenza di effetti o differenze (Wagenmakers, 2007; Amrhein et al., 2019).\nL’uso improprio dei valori-p, noto come “p-hacking” (Simmons et al., 2011), ha favorito pratiche scientifiche discutibili, contribuendo alla crisi di riproducibilità nella psicologia (Chambers et al., 2014; Szucs e Ioannidis, 2016).",
    "crumbs": [
      "Epilogo",
      "Considerazioni Conclusive"
    ]
  },
  {
    "objectID": "chapters/epiloque/epiloque.html#la-crisi-della-replicabilità",
    "href": "chapters/epiloque/epiloque.html#la-crisi-della-replicabilità",
    "title": "Considerazioni Conclusive",
    "section": "La Crisi della Replicabilità",
    "text": "La Crisi della Replicabilità\nLa crisi della replicabilità rappresenta una delle principali sfide che affliggono la ricerca scientifica contemporanea, con effetti particolarmente rilevanti nel campo della psicologia. Quando i risultati di uno studio non possono essere riprodotti in condizioni simili, si mette in discussione non solo la validità delle teorie su cui si basano interventi clinici e politiche pubbliche, ma anche la fiducia generale nella scienza. Questo problema va oltre l’ambito accademico, influenzando direttamente l’efficacia delle applicazioni pratiche delle ricerche.\n\nLe Cause Sottostanti\nUno dei fattori principali della crisi è legato all’uso di metodologie di ricerca e analisi dei dati insufficientemente rigorose, che spesso portano a falsi positivi. Sebbene siano stati fatti appelli per migliorare le pratiche scientifiche, tali problemi persistono, indicando che il fenomeno non è semplicemente frutto di errori o mancanza di comprensione. Secondo Smaldino e McElreath (2016), la causa radicale risiede nei sistemi di incentivi distorti che favoriscono la quantità piuttosto che la qualità della ricerca. In questo contesto, la pressione a pubblicare risultati significativi diventa prioritaria rispetto alla rigorosità metodologica, alimentando un circolo vizioso in cui la “scienza scadente” si perpetua.\nPratiche comuni, come il p-hacking (manipolazione statistica per ottenere risultati significativi) o la selezione selettiva dei dati, vengono adottate inconsciamente o intenzionalmente per massimizzare le probabilità di pubblicazione. Questo meccanismo, descritto come una forma di “selezione naturale della scienza scadente”, premia approcci che facilitano la produzione di risultati spettacolari, ma non necessariamente veritieri.\n\n\nVerso una Soluzione: Cambiare la Cultura Scientifica\nPer superare questa crisi, è fondamentale operare un cambiamento culturale all’interno della comunità scientifica. Non basta correggere gli errori metodologici; occorre modificare radicalmente gli incentivi per premiare la qualità, la trasparenza e la replicabilità della ricerca. Alcune strategie chiave includono:\n\nPromozione di Pratiche Rigorose:\n\nAdottare protocolli trasparenti, preregistrare gli studi prima del loro avvio e condividere apertamente dati e materiali.\nImplementare procedure di peer review più severe e incoraggiare la revisione post-pubblicazione.\n\nValorizzazione della Replicazione:\n\nDare maggiore riconoscimento agli studi che replicano risultati precedenti, anziché privilegiare esclusivamente nuove scoperte.\nCreare riviste specializzate che si concentriano sulla verifica e sulla riproducibilità degli studi.\n\nRiforma dei Criteri di Valutazione:\n\nSpostare l’attenzione dalla quantità delle pubblicazioni alla loro qualità e impatto scientifico.\nIncorporare metriche alternative, come l’impatto sociale e la contribuzione alla conoscenza consolidata.\n\n\n\n\nNuovi Approcci Metodologici\nAlcune proposte innovative mirano a migliorare la qualità delle analisi statistiche e ridurre la propensione a falsi positivi. Un esempio è l’adozione dell’inferenza bayesiana, che offre vantaggi significativi rispetto ai metodi tradizionali:\n\nMaggiore flessibilità nell’analisi di dati rumorosi o campioni piccoli.\nMinore propensione agli errori di tipo I.\nPossibilità di incorporare conoscenze pregresse nel processo decisionale.\n\nTuttavia, l’inferenza bayesiana da sola non può risolvere completamente il problema. È necessario affrontare anche le cause strutturali, come il sistema accademico che premia la produttività quantitativa piuttosto che la qualità.\nUn’altra prospettiva promettente è quella avanzata da Richard McElreath, che suggerisce di passare da un approccio descrittivo a uno che descrive formalmente i meccanismi generativi dei dati. Questo significa formulare ipotesi esplicite sui processi sottostanti e testarle attraverso confronti quantitativi tra modelli. Tecniche come la validazione incrociata bayesiana Leave-One-Out (LOO) permettono di valutare la robustezza dei modelli e la loro capacità di generalizzare a nuovi contesti.\nInoltre, la “rivoluzione causale” cerca di identificare relazioni causali in contesti naturali, superando i limiti degli esperimenti controllati tradizionali. Questo approccio richiede ai ricercatori di formulare ipotesi causali esplicite e confrontare modelli alternativi, migliorando così la comprensione dei fenomeni studiati.\n\n\nImplicazioni Sociali e Educative\nLa crisi della replicabilità ha implicazioni concrete al di là del mondo accademico. Interventi clinici, politiche pubbliche e decisioni basate su ricerche non replicabili rischiano di essere inefficaci o dannose. Pertanto, garantire la replicabilità e l’affidabilità delle scoperte scientifiche è essenziale non solo per preservare l’integrità accademica, ma anche per assumersi responsabilità sociali.\nUna revisione dei metodi didattici e dei programmi accademici è altrettanto cruciale. Gli studenti devono essere formati per comprendere e applicare inferenze basate su dati empirici. Studiosi come Mine Dogucu hanno sottolineato l’importanza di integrare approcci bayesiani e causalità nei corsi di formazione, e la presente dispensa si inserisce in questo sforzo (Dogucu & Çetinkaya-Rundel, 2021; Dogucu & Hu, 2022; Johnson et al., 2022; Rosenberg et al., 2022).",
    "crumbs": [
      "Epilogo",
      "Considerazioni Conclusive"
    ]
  },
  {
    "objectID": "chapters/epiloque/epiloque.html#conclusioni",
    "href": "chapters/epiloque/epiloque.html#conclusioni",
    "title": "Considerazioni Conclusive",
    "section": "Conclusioni",
    "text": "Conclusioni\nAffrontare e superare la crisi della replicabilità rappresenta una sfida fondamentale per la comunità scientifica, richiedendo un impegno collettivo per riformare profondamente la cultura della ricerca. Modificare gli incentivi che favoriscono quantità piuttosto che qualità, promuovere pratiche metodologiche rigorose e valorizzare la replicazione sono passi essenziali per costruire una scienza più affidabile. Solo attraverso un approccio multidimensionale sarà possibile ripristinare la fiducia nella psicologia scientifica e garantire che le sue applicazioni pratiche siano fondate su basi solide e verificabili.\nIn questo contesto, l’inferenza bayesiana emerge come uno strumento di grande valore per l’analisi dei dati psicologici. Offrendo metodi avanzati per gestire l’incertezza, integrare conoscenze pregresse e adattarsi a modelli complessi, essa si dimostra particolarmente utile per esplorare i fenomeni legati alla mente umana e al comportamento. La sua capacità di fornire previsioni robuste e di aggiornare le ipotesi in base a nuovi dati la rende un approccio ideale per affrontare le sfide poste dalla natura intrinsecamente dinamica del campo psicologico.\nTuttavia, è importante sottolineare che l’adozione di metodi bayesiani non costituisce da sola una soluzione completa alla crisi della replicabilità. Per migliorare realmente la qualità della ricerca, è necessario integrare queste tecniche con pratiche metodologiche rigorose. Tra queste, spiccano la formalizzazione di modelli generativi, che consentono di descrivere esplicitamente i processi sottostanti ai dati osservati, e il confronto tra modelli alternativi, fondamentale per valutare l’adeguatezza delle teorie proposte. Inoltre, l’adozione di una prospettiva causale esplicita è cruciale per identificare correttamente le relazioni di causa-effetto, superando i limiti degli studi correlazionali o degli esperimenti tradizionali.\nIn conclusione, solo un approccio integrato, che combini l’inferenza bayesiana con pratiche metodologiche avanzate e una riflessione critica sui sistemi di incentivi accademici, permetterà di progredire verso una scienza psicologica più affidabile e riproducibile. Questo sforzo collettivo non solo migliorerà la qualità delle ricerche, ma contribuirà anche a fornire una comprensione più profonda e accurata del comportamento umano, consolidando così la posizione della psicologia come disciplina scientifica solida e credibile.",
    "crumbs": [
      "Epilogo",
      "Considerazioni Conclusive"
    ]
  },
  {
    "objectID": "chapters/epiloque/epiloque.html#bibliografia",
    "href": "chapters/epiloque/epiloque.html#bibliografia",
    "title": "Considerazioni Conclusive",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nDogucu, M., & Çetinkaya-Rundel, M. (2021). Web scraping in the statistics and data science curriculum: Challenges and opportunities. Journal of Statistics and Data Science Education, 29(sup1), S112–S122.\n\n\nDogucu, M., & Hu, J. (2022). The current state of undergraduate Bayesian education and recommendations for the future. The American Statistician, 76(4), 405–413.\n\n\nJohnson, A. A., Ott, M., & Dogucu, M. (2022). Bayes Rules! An Introduction to Bayesian Modeling with R. CRC Press.\n\n\nLoken, E., & Gelman, A. (2017). Measurement Error and the Replication Crisis. Science, 355(6325), 584–585.\n\n\nRosenberg, J. M., Kubsch, M., Wagenmakers, E.-J., & Dogucu, M. (2022). Making sense of uncertainty in the science classroom: A Bayesian approach. Science & Education, 31(5), 1239–1262.",
    "crumbs": [
      "Epilogo",
      "Considerazioni Conclusive"
    ]
  }
]