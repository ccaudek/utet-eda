{
  "hash": "f2ac380a7468c41b0ac42e33baf7d17d",
  "result": {
    "engine": "knitr",
    "markdown": "# Flusso di lavoro per la pulizia dei dati {#sec-data-cleaning}\n\n::: {.epigraph}\n> “Se i dati non sono puliti, le statistiche non hanno senso.”\n>\n> -- **E.T. Jaynes**, Fisico e Statistico\n:::\n\n## Introduzione {.unnumbered .unlisted}\n\nNonostante la fase più interessante di un progetto di analisi dei dati sia quella in cui si riesce a rispondere alla domanda che ha dato avvio all'indagine, gran parte del tempo di un analista è in realtà dedicata a una fase preliminare: la pulizia e il preprocessing dei dati, operazioni che vengono svolte ancor prima dell'analisi esplorativa.\n\nIn questo capitolo, esamineremo un caso concreto di *data cleaning* e preprocessing, seguendo il tutorial di [Crystal Lewis](https://cghlewis.com/blog/data_clean_03/). Il problema viene presentato come segue:\n\n> I am managing data for a longitudinal randomized controlled trial (RCT) study. For this RCT, schools are randomized to either a treatment or control group. Students who are in a treatment school receive a program to boost their math self-efficacy. Data is collected on all students in two waves (wave 1 is in the fall of a school year, and wave 2 is collected in the spring). At this point in time, we have collected wave 1 of our student survey on a paper form and we set up a data entry database for staff to enter the information into. Data has been double-entered, checked for entry errors, and has been exported in a csv format (“w1_mathproj_stu_svy_raw.csv”) to a folder (called “data”) where it is waiting to be cleaned.\n\nCrystal Lewis elenca i seguenti passaggi da seguire nel processo di data cleaning:\n\n1. Revisione dei dati.\n2. Regolazione del numero di casi.\n3. De-identificazione dei dati.\n4. Eliminazione delle colonne irrilevanti.\n5. Divisione delle colonne, se necessario.\n6. Ridenominazione delle variabili.\n7. Trasformazione/normalizzazione delle variabili.\n8. Standardizzazione delle variabili.\n9. Aggiornamento dei tipi di variabili, se necessario.\n10. Ricodifica delle variabili.\n11. Creazione di eventuali variabili necessarie.\n12. Gestione dei valori mancanti, se necessario.\n13. Aggiunta di metadati, se necessario.\n14. Validazione dei dati.\n15. Fusione e/o unione dei dati, se necessario.\n16. Trasformazione dei dati, se necessario.\n17. Salvataggio dei dati puliti.\n\nSebbene l'ordine di questi passaggi sia flessibile e possa essere adattato alle esigenze specifiche, c'è un passaggio che non dovrebbe mai essere saltato: il primo, ovvero la revisione dei dati. Senza una revisione preliminare, l'analista rischia di sprecare ore a pulire i dati per poi scoprire che mancano dei partecipanti, che i dati non sono organizzati come previsto o, peggio ancora, che si sta lavorando con i dati sbagliati.\n\n\n### Panoramica del capitolo {.unnumbered .unlisted}\n\n- Verificare, pulire e trasformare i dati per l'analisi.\n- Documentare il dataset con un dizionario e note esplicative.\n- Assicurare validità, unicità e organizzazione dei dati.\n- Applicare regole coerenti per denominazione e codifica.\n\n::: {.callout-tip collapse=true}\n## Prerequisiti\n\n- Leggere [Cleaning sample data in standardized way](https://cghlewis.com/blog/data_clean_03/) di Crystal Lewis.\n- Leggere [Getting Started Creating Data Dictionaries: How to Create a Shareable Data Set](https://journals.sagepub.com/doi/full/10.1177/2515245920928007) di @buchanan2021getting.\n- Consultare il capitolo [Documentation](https://datamgmtinedresearch.com/document#document-dictionary) di *Data Management in Large-Scale Education Research*.\n- Consultare [How to Make a Data Dictionary](https://help.osf.io/article/217-how-to-make-a-data-dictionary).\n- Consultare [data dictionary template](https://osf.io/ynqcu).\n:::\n\n::: {.callout-caution collapse=true title=\"Preparazione del Notebook\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nhere::here(\"code\", \"_common.R\") |> \n  source()\n\n# Load packages\nif (!requireNamespace(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(mice, labelled, haven, pointblank, mice, purrr, knitr)\n```\n:::\n\n:::\n\n\n## Tutorial\n\nQuesto tutorial segue i passaggi descritti da [Crystal Lewis](https://cghlewis.com/blog/data_clean_03/) per illustrare le buone pratiche nella gestione e pulizia dei dati.\n\n\n### Organizzazione dei dati\n\nUn principio fondamentale nella gestione dei dati è preservare l'integrità dei dati grezzi. I dati originali *non devono mai essere modificati direttamente*. È quindi consigliabile strutturare i dati in una directory denominata `data`, suddivisa in due sottocartelle:\n\n- `raw`: contiene i dati originali, mantenuti inalterati.  \n- `processed`: destinata ai dati ripuliti e preprocessati.  \n\nAd esempio, per avviare il processo di pulizia, importiamo i dati da un file denominato `w1_mathproj_stu_svy_raw.csv`. Tutte le operazioni dovranno essere effettuate utilizzando percorsi relativi alla *home directory* del progetto, che definiremo come primo passo.\n\n### Passaggi del tutorial\n\n#### Importare e esaminare i dati\n\nImportiamo i dati utilizzando la funzione `import()` della libreria `rio` e visualizziamo i primi valori di ciascuna colonna per verificarne la corretta importazione:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Importa i dati\nsvy <- rio::import(here::here(\"data\", \"w1_mathproj_stu_svy_raw.csv\"))\n\n# Esamina la struttura del dataset\nglimpse(svy)\n#> Rows: 6\n#> Columns: 7\n#> $ stu_id      <int> 1347, 1368, 1377, 1387, 1347, 1399\n#> $ svy_date    <IDate> 2023-02-13, 2023-02-13, 2023-02-13, 2023-02-13, 2023-02-14…\n#> $ grade_level <int> 9, 10, 9, 11, 9, 12\n#> $ math1       <int> 2, 3, 4, 3, 2, 4\n#> $ math2       <chr> \"1\", \"2\", \"\\n4\", \"3\", \"2\", \"1\"\n#> $ math3       <int> 3, 2, 4, NA, 4, 3\n#> $ math4       <int> 3, 2, 4, NA, 2, 1\n```\n:::\n\n\nPer controllare visivamente i dati, possiamo esaminare le prime e le ultime righe del `data frame`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Visualizza le prime righe\nsvy |> \n  head()\n#>   stu_id   svy_date grade_level math1 math2 math3 math4\n#> 1   1347 2023-02-13           9     2     1     3     3\n#> 2   1368 2023-02-13          10     3     2     2     2\n#> 3   1377 2023-02-13           9     4   \\n4     4     4\n#> 4   1387 2023-02-13          11     3     3    NA    NA\n#> 5   1347 2023-02-14           9     2     2     4     2\n#> 6   1399 2023-02-14          12     4     1     3     1\n\n# Visualizza le ultime righe\nsvy |> \n  tail()\n#>   stu_id   svy_date grade_level math1 math2 math3 math4\n#> 1   1347 2023-02-13           9     2     1     3     3\n#> 2   1368 2023-02-13          10     3     2     2     2\n#> 3   1377 2023-02-13           9     4   \\n4     4     4\n#> 4   1387 2023-02-13          11     3     3    NA    NA\n#> 5   1347 2023-02-14           9     2     2     4     2\n#> 6   1399 2023-02-14          12     4     1     3     1\n```\n:::\n\n\n#### Individuare e rimuovere i duplicati\n\nIn questa fase, eseguiamo alcune modifiche necessarie al `data frame`, come rimuovere duplicati e ordinare i dati:\n\n- *Verifica duplicati*: controlliamo i record duplicati nel dataset.\n- *Rimuovi duplicati*: manteniamo solo la prima occorrenza.\n- *Ordina per data*: organizziamo i record in ordine crescente rispetto alla variabile `svy_date`.\n- *Esamina i dati puliti*: controlliamo il risultato delle modifiche.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Identifica i duplicati basati su 'stu_id'\nduplicates <- \n  svy[duplicated(svy$stu_id) | duplicated(svy$stu_id, fromLast = TRUE), ]\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Visualizza i duplicati trovati\nduplicates\n#>   stu_id   svy_date grade_level math1 math2 math3 math4\n#> 1   1347 2023-02-13           9     2     1     3     3\n#> 5   1347 2023-02-14           9     2     2     4     2\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Ordina per 'svy_date' in ordine crescente\nsvy <- svy[order(svy$svy_date), ]\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Rimuove i duplicati mantenendo la prima occorrenza\nsvy <- svy[!duplicated(svy$stu_id), ]\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Esamina il dataset finale\nprint(svy)\n#>   stu_id   svy_date grade_level math1 math2 math3 math4\n#> 1   1347 2023-02-13           9     2     1     3     3\n#> 2   1368 2023-02-13          10     3     2     2     2\n#> 3   1377 2023-02-13           9     4   \\n4     4     4\n#> 4   1387 2023-02-13          11     3     3    NA    NA\n#> 6   1399 2023-02-14          12     4     1     3     1\n```\n:::\n\n\nVerifichiamo le dimensioni del dataset pulito per assicurarci che le operazioni siano state eseguite correttamente:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Controlla il numero di righe e colonne\nsvy |> \n  dim()\n#> [1] 5 7\n```\n:::\n\n\n#### De-identificazione dei dati\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Rimuovi la colonna 'svy_date'\nsvy <- svy |>\n  dplyr::select(-svy_date)\n\n# Mostra i nomi delle colonne rimaste\nnames(svy)\n#> [1] \"stu_id\"      \"grade_level\" \"math1\"       \"math2\"       \"math3\"      \n#> [6] \"math4\"\n```\n:::\n\n\n#### Rimuovere le colonne non necessarie\n\nNel caso presente, la rimozione di colonne non è necessaria. Tuttavia, in molti progetti di analisi dei dati, soprattutto quando i dati vengono raccolti utilizzando software di terze parti o strumenti specifici per esperimenti psicologici, è comune trovarsi con colonne che non sono pertinenti allo studio in corso. \n\nQueste colonne possono includere dati come identificatori interni, timestamp generati automaticamente, informazioni di debug, o variabili che non sono rilevanti per l'analisi che si intende condurre. Quando tali colonne sono irrilevanti per la ricerca, possono essere rimosse per semplificare il dataset e ridurre il rischio di confusione o errori durante l'analisi. Rimuovere le colonne non necessarie non solo rende il dataset più gestibile, ma aiuta anche a focalizzare l'analisi sulle variabili che realmente importano per rispondere alle domande di ricerca.\n\n#### Dividere le colonne secondo necessità\n\nNel caso presente, questa operazione non è necessaria. Tuttavia, se si lavora con un dataset che include una colonna chiamata \"NomeCompleto\", contenente sia il nome che il cognome di uno studente, per esempio, è buona pratica separare questa colonna in due colonne distinte, \"Nome\" e \"Cognome\". Questa suddivisione facilita l'analisi e la manipolazione dei dati, rendendoli più organizzati e accessibili.\n\n#### Rinominare le colonne\n\nÈ importante assegnare nomi chiari alle colonne del dataset. Utilizzare nomi di variabili comprensibili aiuta a rendere l'analisi dei dati più intuitiva e a ridurre il rischio di errori interpretativi.\n\nEsempi di buone pratiche:\n\n- Evita nomi di colonne come \"x\" o acronimi incomprensibili. Questi possono creare confusione durante l'analisi, specialmente se il dataset viene condiviso con altri ricercatori o se viene ripreso dopo un lungo periodo di tempo.\n- Invece, cerca di utilizzare nomi di variabili che descrivano chiaramente il contenuto della colonna. Ad esempio, invece di \"x1\" o \"VAR123\", un nome come \"ansia_base\" o \"liv_autoefficacia\" è molto più comprensibile e immediato.\n- Per i nomi composti, utilizza un separatore come il trattino basso `_`. Ad esempio, se stai lavorando con dati relativi a un test psicologico, potresti avere colonne chiamate \"test_ansia_pre\" e \"test_ansia_post\" per indicare i risultati del test di ansia prima e dopo un intervento.\n\nEsempi di nomi di colonne ben scelti:\n\n- *Nome generico:* `TS`, `AE`\n  - *Nome migliore:* `tempo_studio`, `auto_efficacia`\n- *Nome generico:* `S1`, `S2`\n  - *Nome migliore:* `stress_situazione1`, `stress_situazione2`\n- *Nome generico:* `Q1`, `Q2`\n  - *Nome migliore:* `qualità_sonno_sett1`, `qualità_sonno_sett2`\n\n#### Trasformare le variabili\n\nNel caso presente non si applica, ma è un passo importante in molte analisi dei dati. \n\nEsempi di trasformazione delle variabili:\n\n- *Logaritmo di una variabile:* Immaginiamo di avere una variabile che misura i tempi di reazione dei partecipanti a un esperimento. Se i tempi di reazione hanno una distribuzione fortemente asimmetrica (con alcuni valori molto elevati), potrebbe essere utile applicare una trasformazione logaritmica per rendere la distribuzione più simmetrica e migliorare l'interpretabilità dei risultati.\n\n- *Codifica delle variabili categoriche:* Se è presente una variabile categorica come il \"tipo di intervento\" con valori come \"cognitivo\", \"comportamentale\" e \"farmacologico\", potrebbe essere necessario trasformare questa variabile in variabili dummy (ad esempio, `intervento_cognitivo`, `intervento_comportamentale`, `intervento_farmacologico`), dove ogni variabile assume il valore 0 o 1 a seconda della presenza o meno di quel tipo di intervento. Questo è utile quando si utilizzano tecniche di regressione.\n\n\n#### Standardizzazione delle variabili\n\nLa standardizzazione è utile quando si desidera rendere comparabili variabili misurate su scale diverse, ad esempio per confronti tra gruppi o per inclusione in modelli di regressione.  \n\n**Esempio.**  Supponiamo di avere una variabile che misura il livello di ansia su una scala da 0 a 100. Per standardizzarla:  \n\n1. *Sottrai la media* del campione dalla variabile.  \n2. *Dividi per la deviazione standard*.  \n\nIl risultato è una variabile con media pari a 0 e deviazione standard pari a 1.  \n\n**Vantaggi della standardizzazione**:  \n\n- facilita l’interpretazione dei coefficienti in un modello di regressione; \n- permette un confronto diretto tra variabili che hanno unità di misura diverse.\n\n#### Normalizzazione delle variabili\n\nLa normalizzazione consiste nel ridimensionare i dati su una scala predefinita, spesso compresa tra 0 e 1. Questo processo è particolarmente utile in analisi multivariate, dove variabili con scale molto diverse potrebbero influenzare in modo sproporzionato i risultati.  \n\n**Esempio.** Hai dati su:  \n\n- Ore di sonno (misurate in ore, da 0 a 24).  \n- Livello di stress (misurato su una scala da 1 a 50).  \n- Auto-efficacia (misurata su una scala da 0 a 100).  \n\nPer garantire che ogni variabile abbia lo stesso peso nell’analisi, puoi normalizzarle usando una formula come:  \n\n$$\nx_{\\text{norm}} = \\frac{x - \\text{min}(x)}{\\text{max}(x) - \\text{min}(x)} .\n$$\n\n**Perché Standardizzare o Normalizzare?**\n\nTrasformare le variabili è cruciale per:  \n\n1. *Gestire scale diverse*: Riduce il rischio che variabili con valori numericamente più grandi dominino i risultati.  \n2. *Garantire validità e interpretabilità*: Facilita il confronto tra dati provenienti da fonti o gruppi diversi.  \n3. *Evitare problemi numerici nei modelli statistici*: Alcuni algoritmi di machine learning o tecniche di ottimizzazione richiedono che i dati siano su scale simili per funzionare correttamente.\n\nIn conclusione, la scelta tra standardizzazione e normalizzazione dipende dal contesto dell’analisi e dagli obiettivi specifici. Entrambi i processi sono strumenti indispensabili per garantire che i dati siano trattati in modo adeguato, portando a risultati robusti e facilmente interpretabili.\n\n#### Aggiornare i tipi delle variabili\n\nNel caso presente non è necessario. Supponiamo invece di avere una colonna in un dataset psicologico che contiene punteggi di un questionario, ma i dati sono stati importati come stringhe (testo) invece che come numeri. Per eseguire calcoli statistici, sarà necessario convertire questa colonna da stringa a numerico. \n\nIn `R`, si potrebbe usare il seguente codice:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Supponiamo di avere un data frame chiamato 'df' con una colonna 'punteggio' importata come carattere\ndf$punteggio <- as.numeric(df$punteggio)\n\n# Ora la colonna 'punteggio' è stata convertita in un tipo numerico ed è possibile eseguire calcoli su di essa\n```\n:::\n\n\nIn questo esempio, la funzione `as.numeric()` viene utilizzata per convertire la colonna `punteggio` in un formato numerico, permettendo di eseguire analisi quantitative sui dati. \n\nUn altro caso molto comune si verifica quando si importano dati da file Excel. Spesso capita che, all'interno di una cella di una colonna che dovrebbe contenere solo valori numerici, venga inserito erroneamente uno o più caratteri alfanumerici. Di conseguenza, l'intera colonna viene interpretata come di tipo alfanumerico, anche se i valori dovrebbero essere numerici. In questi casi, è fondamentale individuare la cella problematica, correggere manualmente il valore errato, e poi riconvertire l'intera colonna da alfanumerica a numerica.\n\n#### Ricodificare le variabili\n\nAnche se in questo caso non è necessario, la ricodifica delle variabili è una pratica molto comune nelle analisi dei dati psicologici.\n\nPer esempio, consideriamo una variabile categoriale con modalità descritte da stringhe poco comprensibili, che vengono ricodificate con nomi più chiari e comprensibili.\n\nSupponiamo di avere un DataFrame chiamato `df` con una colonna `tipo_intervento` che contiene le modalità `\"CT\"`, `\"BT\"`, e `\"MT\"` per rappresentare rispettivamente \"Terapia Cognitiva\", \"Terapia Comportamentale\" e \"Terapia Mista\". Queste abbreviazioni potrebbero non essere immediatamente chiare a chiunque analizzi i dati, quindi decidiamo di ricodificarle con nomi più espliciti. Ecco come farlo in `R`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Tibble chiamato 'df' con una colonna 'tipo_intervento'\ndf <- tibble(tipo_intervento = c(\"CT\", \"BT\", \"MT\", \"CT\", \"BT\"))\ndf\n#> # A tibble: 5 × 1\n#>   tipo_intervento\n#>   <chr>          \n#> 1 CT             \n#> 2 BT             \n#> 3 MT             \n#> 4 CT             \n#> 5 BT\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Ricodifica delle modalità della variabile 'tipo_intervento' in nomi più comprensibili\ndf <- df %>%\n  mutate(tipo_intervento_ricodificato = dplyr::recode(\n    tipo_intervento,\n    \"CT\" = \"Terapia Cognitiva\",\n    \"BT\" = \"Terapia Comportamentale\",\n    \"MT\" = \"Terapia Mista\"\n  ))\n\n# Mostra il tibble con la nuova colonna ricodificata\ndf\n#> # A tibble: 5 × 2\n#>   tipo_intervento tipo_intervento_ricodificato\n#>   <chr>           <chr>                       \n#> 1 CT              Terapia Cognitiva           \n#> 2 BT              Terapia Comportamentale     \n#> 3 MT              Terapia Mista               \n#> 4 CT              Terapia Cognitiva           \n#> 5 BT              Terapia Comportamentale\n```\n:::\n\n\n#### Aggiungere nuove variabili nel data frame\n\nNel caso presente non è richiesto, ma aggiungere nuove variabili a un DataFrame è un'operazione comune durante l'analisi dei dati. Un esempio è il calcolo dell'indice di massa corporea (BMI).\n\nSupponiamo di avere un DataFrame chiamato `df` che contiene le colonne `peso_kg` (peso in chilogrammi) e `altezza_m` (altezza in metri) per ciascun partecipante a uno studio psicologico. Per arricchire il dataset, possiamo calcolare il BMI per ogni partecipante e aggiungerlo come una nuova variabile.\n\nIl BMI si calcola con la formula:\n\n$$ \\text{BMI} = \\frac{\\text{peso in kg}}{\\text{altezza in metri}^2} .$$\n\nEcco come aggiungere la nuova colonna.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Supponiamo di avere un tibble chiamato 'df' con le colonne 'peso_kg' e 'altezza_m'\ndf <- tibble(\n  peso_kg = c(70, 85, 60, 95),\n  altezza_m = c(1.75, 1.80, 1.65, 1.90)\n)\ndf\n#> # A tibble: 4 × 2\n#>   peso_kg altezza_m\n#>     <dbl>     <dbl>\n#> 1      70      1.75\n#> 2      85      1.8 \n#> 3      60      1.65\n#> 4      95      1.9\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Calcola il BMI e aggiungilo come una nuova colonna 'BMI'\ndf <- df %>%\n  mutate(BMI = peso_kg / (altezza_m^2))\n\n# Mostra il tibble con la nuova variabile aggiunta\ndf\n#> # A tibble: 4 × 3\n#>   peso_kg altezza_m   BMI\n#>     <dbl>     <dbl> <dbl>\n#> 1      70      1.75  22.9\n#> 2      85      1.8   26.2\n#> 3      60      1.65  22.0\n#> 4      95      1.9   26.3\n```\n:::\n\n\n### Affrontare il problema dei dati mancanti\n\nI dati mancanti sono un problema comune nelle ricerche psicologiche e in molte altre discipline. Quando mancano delle informazioni in un dataset, possono verificarsi gravi problemi per l'analisi statistica, come risultati distorti, riduzione della precisione delle stime o, in alcuni casi, l'impossibilità di applicare alcuni algoritmi.\n\n#### Perché i dati mancanti sono un problema?\n\nImmagina di voler capire il rendimento medio di una classe in un test, ma alcuni studenti hanno lasciato delle risposte in bianco. Se ignoriamo le risposte mancanti o eliminiamo gli studenti con dati incompleti, rischiamo di ottenere una stima che non rappresenta correttamente la realtà. Questo succede perché:\n\n- *Bias dei risultati*: Se i dati mancanti non sono casuali (ad esempio, i dati mancanti sono presenti più spesso in studenti con basso rendimento), le conclusioni possono essere errate.\n- *Riduzione della potenza statistica*: Eliminare dati incompleti riduce il numero totale di osservazioni, rendendo più difficile trovare risultati significativi.\n- *Impossibilità di applicare alcuni metodi*: Molti algoritmi statistici richiedono dati completi e non funzionano con valori mancanti.\n\n#### Come gestire i dati mancanti?\n\nCi sono diversi modi per affrontare i dati mancanti. Vediamo prima i metodi più semplici e poi quelli più avanzati.\n\nEsaminiamo il data frame iniziale:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsvy |> \n  summary()\n#>      stu_id      grade_level       math1        math2               math3     \n#>  Min.   :1347   Min.   : 9.0   Min.   :2.0   Length:5           Min.   :2.00  \n#>  1st Qu.:1368   1st Qu.: 9.0   1st Qu.:3.0   Class :character   1st Qu.:2.75  \n#>  Median :1377   Median :10.0   Median :3.0   Mode  :character   Median :3.00  \n#>  Mean   :1376   Mean   :10.2   Mean   :3.2                      Mean   :3.00  \n#>  3rd Qu.:1387   3rd Qu.:11.0   3rd Qu.:4.0                      3rd Qu.:3.25  \n#>  Max.   :1399   Max.   :12.0   Max.   :4.0                      Max.   :4.00  \n#>                                                                 NA's   :1     \n#>      math4     \n#>  Min.   :1.00  \n#>  1st Qu.:1.75  \n#>  Median :2.50  \n#>  Mean   :2.50  \n#>  3rd Qu.:3.25  \n#>  Max.   :4.00  \n#>  NA's   :1\n```\n:::\n\n\nSi noti la presenza di dati mancanti sulle variabili `math3` e `math4`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndim(svy)\n#> [1] 5 6\n```\n:::\n\n\n1. **Esclusione dei Casi Incompleti (Complete Case Analysis).**  \n   Un approccio comune, ma spesso non ideale, è quello di analizzare solo i casi completi, eliminando tutte le righe con valori mancanti. In R, questo si può fare con il seguente comando:\n\n\n   ::: {.cell layout-align=\"center\"}\n   \n   ```{.r .cell-code}\n   svy_comp <- svy |>\n     drop_na()\n   ```\n   :::\n\n   \n   In questo modo abbiamo escluso tutte le righe nelle quali sono presenti dei dati mancanti (in questo caso, una sola riga).\n   \n\n   ::: {.cell layout-align=\"center\"}\n   \n   ```{.r .cell-code}\n   dim(svy_comp)\n   #> [1] 4 6\n   ```\n   :::\n\n   \n   **Limite**: Questo metodo può introdurre bias se i dati mancanti non sono casuali e riduce il campione, compromettendo l'affidabilità delle analisi.\n\n2. **Imputazione Semplice**  \n   Sostituire i valori mancanti con stime semplici:\n   - *Media o mediana*: Per le variabili numeriche, sostituire i valori mancanti con la media o la mediana. Questo metodo è facile da implementare, ma può ridurre la variabilità nei dati.\n   - *Moda*: Per le variabili categoriche, sostituire i valori mancanti con il valore più frequente. Tuttavia, può introdurre distorsioni se i dati sono molto eterogenei.\n\n3. **Imputazione Multipla**  \n   Un approccio più avanzato è l'imputazione multipla, che utilizza modelli statistici per stimare i valori mancanti in modo iterativo. L'idea di base è semplice: ogni valore mancante viene stimato tenendo conto delle relazioni con tutte le altre variabili.\n\n   *Vantaggi*:\n   - Mantiene la variabilità dei dati.\n   - Preserva le relazioni tra le variabili.\n   - Riduce il rischio di bias rispetto ai metodi semplici.\n\n#### Applicazione pratica: imputazione multipla con `mice` in R\n\nSupponiamo di avere un dataset con alcune colonne numeriche che contengono valori mancanti. Possiamo utilizzare il pacchetto `mice` per imputare i dati mancanti.\n\nSelezioniamo le colonne numeriche da imputare.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnumeric_columns <- c(\"math1\", \"math2\", \"math3\", \"math4\")\nsvy <- svy %>%\n  mutate(across(all_of(numeric_columns), as.numeric))\n```\n:::\n\n\nEseguiamo l'imputazione multipla.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nimputed <- mice(\n  svy[numeric_columns], \n  m = 1, \n  maxit = 10, \n  method = \"norm.predict\", \n  seed = 123\n)\n#> \n#>  iter imp variable\n#>   1   1  math3  math4\n#>   2   1  math3  math4\n#>   3   1  math3  math4\n#>   4   1  math3  math4\n#>   5   1  math3  math4\n#>   6   1  math3  math4\n#>   7   1  math3  math4\n#>   8   1  math3  math4\n#>   9   1  math3  math4\n#>   10   1  math3  math4\n```\n:::\n\n\nOttieniamo il dataset con i valori imputati.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsvy_imputed <- complete(imputed)\n```\n:::\n\n\nArrotondiamo i valori imputati (se necessario).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsvy_imputed <- svy_imputed %>%\n  mutate(across(everything(), round))\n```\n:::\n\n\nSostituiamo i valori imputati nel dataset originale.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsvy[numeric_columns] <- svy_imputed\n```\n:::\n\n\nEsaminiamo il risultato ottenuto.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsvy |> \n  summary()\n#>      stu_id      grade_level       math1         math2         math3    \n#>  Min.   :1347   Min.   : 9.0   Min.   :2.0   Min.   :1.0   Min.   :2.0  \n#>  1st Qu.:1368   1st Qu.: 9.0   1st Qu.:3.0   1st Qu.:1.0   1st Qu.:3.0  \n#>  Median :1377   Median :10.0   Median :3.0   Median :2.0   Median :3.0  \n#>  Mean   :1376   Mean   :10.2   Mean   :3.2   Mean   :2.2   Mean   :3.2  \n#>  3rd Qu.:1387   3rd Qu.:11.0   3rd Qu.:4.0   3rd Qu.:3.0   3rd Qu.:4.0  \n#>  Max.   :1399   Max.   :12.0   Max.   :4.0   Max.   :4.0   Max.   :4.0  \n#>      math4    \n#>  Min.   :1.0  \n#>  1st Qu.:2.0  \n#>  Median :3.0  \n#>  Mean   :2.8  \n#>  3rd Qu.:4.0  \n#>  Max.   :4.0\n```\n:::\n\n\n#### Come funziona l’imputazione multipla?\n\n1. Ogni variabile con valori mancanti viene modellata come funzione delle altre variabili.\n2. I valori mancanti vengono stimati iterativamente. In ogni iterazione, si utilizza l'output precedente come input per migliorare le stime.\n3. Dopo un numero sufficiente di iterazioni, le stime si stabilizzano (convergenza).\n\nIn conclusione, l'imputazione multipla è una tecnica avanzata che permette di gestire i dati mancanti preservando la qualità delle analisi. Rispetto ai metodi semplici, consente di mantenere la variabilità e ridurre il rischio di bias, rendendola una scelta ideale per analisi psicologiche e di ricerca. \n\n#### Aggiungere i metadati\n\nI *metadati* sono informazioni che descrivono i dati stessi, come etichette di variabili, etichette di valori, informazioni sull'origine dei dati, unità di misura e altro ancora. Questi metadati sono essenziali per comprendere, documentare e condividere correttamente un dataset.\n\nIn R, i metadati sono gestiti in modo molto dettagliato e strutturato attraverso pacchetti come `haven`, `labelled`, e `Hmisc`. Questi pacchetti consentono di associare etichette ai dati, come etichette di variabili e di valori, e persino di gestire i valori mancanti con etichette specifiche.\n\n- *Etichette di variabili*: Si possono aggiungere direttamente alle colonne di un DataFrame usando funzioni come `labelled::set_variable_labels()`.\n- *Etichette di valori*: Possono essere aggiunte a variabili categoriali utilizzando `labelled::labelled()`.\n- *Valori mancanti*: In R, è possibile etichettare specifici valori come mancanti usando `labelled::na_values<-`.\n\nQuesti strumenti rendono molto facile documentare un dataset all'interno del processo di analisi, assicurando che tutte le informazioni critiche sui dati siano facilmente accessibili e ben documentate.\n\nEsaminiamo un esempio pratico. Consideriamo nuovamente il data set `svy`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nglimpse(svy)\n#> Rows: 5\n#> Columns: 6\n#> $ stu_id      <int> 1347, 1368, 1377, 1387, 1399\n#> $ grade_level <int> 9, 10, 9, 11, 12\n#> $ math1       <dbl> 2, 3, 4, 3, 4\n#> $ math2       <dbl> 1, 2, 4, 3, 1\n#> $ math3       <dbl> 3, 2, 4, 4, 3\n#> $ math4       <dbl> 3, 2, 4, 4, 1\n```\n:::\n\n\nSi noti che la variabile `math2` contiene un valore inamissibile, probabilmente un errore di battitura. Questo fa in modo che `math2` sia di tipo `char` mentre dovrebbe essere una variabile numerica. Correggiamo.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Correzione di `math2`: Rimuovi valori non validi e converti in numerico\nsvy$math2 <- gsub(\"\\\\n\", \"\", svy$math2)  # Rimuovi caratteri non validi come '\\n'\nsvy$math2 <- as.numeric(svy$math2)      # Converte la variabile in numerico\n\n# Visualizzazione del dataset corretto\nprint(svy)\n#>   stu_id grade_level math1 math2 math3 math4\n#> 1   1347           9     2     1     3     3\n#> 2   1368          10     3     2     2     2\n#> 3   1377           9     4     4     4     4\n#> 4   1387          11     3     3     4     4\n#> 6   1399          12     4     1     3     1\n```\n:::\n\n\nDefiniamo le etichette di valore per le variabili `math1:math4`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nvalue_labels_math <- set_names(\n  as.numeric(names(c(\n    `1` = \"strongly disagree\",\n    `2` = \"disagree\",\n    `3` = \"agree\",\n    `4` = \"strongly agree\"\n  ))),\n  c(\"strongly disagree\", \"disagree\", \"agree\", \"strongly agree\")\n)\n```\n:::\n\n\nAggiungiamo le etichette di valore alle colonne `math1:math4`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsvy <- svy %>%\n  mutate(across(starts_with(\"math\"), ~ labelled(., labels = value_labels_math)))\n```\n:::\n\n\nVerifica delle etichette.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nval_labels(svy$math1)\n#> strongly disagree          disagree             agree    strongly agree \n#>                 1                 2                 3                 4\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsvy\n#>   stu_id grade_level math1 math2 math3 math4\n#> 1   1347           9     2     1     3     3\n#> 2   1368          10     3     2     2     2\n#> 3   1377           9     4     4     4     4\n#> 4   1387          11     3     3     4     4\n#> 6   1399          12     4     1     3     1\n```\n:::\n\n\n##### Utilizzo delle etichette in R con variabili numeriche\n\nLe etichette dei valori (*value labels*) vengono utilizzate per rendere più leggibili e interpretabili le variabili numeriche, associando ad ogni valore un'etichetta descrittiva. Questo approccio è particolarmente utile in ambiti come la ricerca  psicologica, dove le risposte ai questionari sono spesso codificate con numeri (ad esempio, 1 = \"Strongly Disagree\", 2 = \"Disagree\", ecc.) ma rappresentano concetti qualitativi.\n\n**Vantaggi dell’Uso delle Etichette**\n\n1. *Chiarezza nelle Analisi*: Le etichette descrittive rendono i dati più facilmente comprensibili senza dover ricordare il significato numerico di ciascun valore.  \n2. *Documentazione Integrata*: Permettono di incorporare metadati direttamente nelle variabili, migliorando la trasparenza e riducendo il rischio di interpretazioni errate.\n3. *Compatibilità con Software Statistici*: Molti strumenti (ad esempio, SPSS o Stata) utilizzano etichette di valori. Il pacchetto `haven` in R consente di gestire facilmente i dati etichettati esportati/importati da questi software.\n\n**Manipolazione di Variabili Etichettate**\n\nAnche se una variabile numerica è etichettata con `labelled` (ad esempio, tramite il pacchetto `haven`), essa conserva la sua natura numerica e può essere utilizzata in calcoli, modelli statistici, e trasformazioni. Le etichette non alterano il valore sottostante, ma lo arricchiscono con informazioni aggiuntive. \n\nIn conclusione, le etichette dei valori migliorano l'interpretabilità dei dati senza comprometterne la manipolabilità. Questo approccio è ideale per mantenere le variabili numeriche pienamente funzionali per analisi statistiche, mentre le etichette descrittive forniscono un contesto chiaro e leggibile.\n\n#### Validazione dei dati\n\nLa validazione dei dati è un passaggio fondamentale per garantire che il dataset soddisfi i criteri previsti e sia pronto per le analisi successive. Questo processo include il controllo della coerenza e della correttezza dei dati in base a specifiche regole definite dal dizionario dei dati. Alcune verifiche comuni includono:\n\n- *Unicità delle righe*: Assicurarsi che ogni riga sia unica, verificando l'assenza di ID duplicati.\n- *Validità degli ID*: Controllare che gli ID rientrino in un intervallo previsto (es. numerico).\n- *Valori accettabili nelle variabili categoriali*: Verificare che variabili come `grade_level`, `int` e le colonne `math` contengano esclusivamente valori appartenenti a un set di valori validi.\n\nIl pacchetto [pointblank](https://rstudio.github.io/pointblank/) fornisce strumenti flessibili e intuitivi per eseguire verifiche di validazione e generare report dettagliati. Questo pacchetto consente di:\n\n- *Definire le regole di validazione*: Specificare controlli come unicità, intervalli di valori e appartenenza a insiemi predefiniti.\n- *Eseguire i controlli*: Applicare le regole di validazione su un dataset per identificare eventuali discrepanze.\n- *Generare report interattivi*: Creare un riepilogo chiaro e visivo dei controlli, evidenziando eventuali errori o anomalie.\n\nCon `pointblank`, è possibile integrare la validazione dei dati come parte di un workflow strutturato, garantendo la qualità dei dati in modo sistematico e ripetibile.\n\n```r\ncreate_agent(svy) %>%\n  rows_distinct(columns = vars(stu_id)) %>%\n  col_vals_between(columns = c(stu_id), \n                   left = 1300, right = 1400, na_pass = TRUE) %>%\n  col_vals_in_set(columns = c(grade_level), \n                  set = c(9, 10, 11, 12, NA)) %>%\n  col_vals_in_set(columns = c(int),\n                  set = c(0, 1, NA)) %>%\n  col_vals_in_set(columns = c(math1:math4),\n                  set = c(1, 2, 3, 4, NA)) %>%\n  interrogate()\n```\n\nIl dataset ripulito soddisfa tutte le aspettative delineate da Crystal Lewis.\n\n- *Completo*: Tutti i dati raccolti sono stati inseriti e/o recuperati. Non dovrebbero esserci dati estranei che non appartengono al dataset (come duplicati o partecipanti non autorizzati). \n- *Valido*: Le variabili rispettano i vincoli definiti nel tuo dizionario dei dati. Ricorda che il dizionario dei dati specifica i nomi delle variabili, i tipi, i range, le categorie e altre informazioni attese.\n- *Accurato*: Sebbene non sia sempre possibile determinare l'accuratezza dei valori durante il processo di pulizia dei dati (ovvero, se un valore è realmente corretto o meno), in alcuni casi è possibile valutarla sulla base della conoscenza pregressa riguardante quel partecipante o caso specifico.\n- *Coerente*: I valori sono allineati tra le varie fonti. Ad esempio, la data di nascita raccolta attraverso un sondaggio studentesco dovrebbe avere un formato corrispondere alla data di nascita raccolta dal distretto scolastico.\n- *Uniforme*: I dati sono standardizzati attraverso i moduli e nel tempo. Ad esempio, lo stato di partecipazione ai programmi di pranzo gratuito o a prezzo ridotto è sempre fornito come una variabile numerica con la stessa rappresentazione, oppure il nome della scuola è sempre scritto in modo coerente in tutto il dataset.\n- *De-identificato*: Tutte le informazioni personali identificabili (PII) sono state rimosse dal dataset per proteggere la riservatezza dei partecipanti (se richiesto dal comitato etico/consenso informato).\n- *Interpretabile*: I dati hanno nomi di variabili leggibili sia da umani che dal computer, e sono presenti etichette di variabili e valori laddove necessario per facilitare l'interpretazione.\n- *Analizzabile*: Il dataset è in un formato rettangolare (righe e colonne), leggibile dal computer e conforme alle regole di base della struttura dei dati.\n\nUna volta completati i 14 passaggi precedenti, è possibile esportare questo dataset ripulito nella cartella `processed` per le successive analisi statistiche.\n\n#### Unire e/o aggiungere dati se necessario\n\nIn questo passaggio, è possibile unire o aggiungere colonne o righe presenti in file diversi. È importante eseguire nuovamente i controlli di validazione dopo l'unione/aggiunta di nuovi dati.\n\n#### Trasformare i dati se necessario\n\nEsistono vari motivi per cui potrebbe essere utile memorizzare i dati in formato `long` o `wide`. In questo passaggio, è possibile ristrutturare i dati secondo le esigenze.\n\n#### Salvare il dataset pulito finale\n\nL'ultimo passaggio del processo di pulizia consiste nell'esportare o salvare il dataset pulito. Come accennato in precedenza, può essere utile esportare/salvare il dataset in più di un formato di file (ad esempio, un file `.csv` e un file `.parquet`).\n\n## Organizzazione dei file e informazioni aggiuntive\n\nInfine, è essenziale includere una documentazione adeguata per garantire che le informazioni siano interpretate correttamente, sia da altri utenti che da te stesso, se dovessi tornare a lavorare su questo progetto in futuro. La documentazione minima da fornire dovrebbe includere:\n\n- *Documentazione a livello di progetto*: Questa sezione fornisce informazioni contestuali sul perché e come i dati sono stati raccolti. È utile per chiunque voglia comprendere lo scopo e la metodologia del progetto.\n- *Metadati a livello di progetto*: Se condividi i dati in un repository pubblico o privato, è importante includere metadati a livello di progetto. Questi metadati forniscono informazioni dettagliate che facilitano la ricerca, la comprensione e la consultabilità dei dati. I metadati a livello di progetto possono includere descrizioni generali del progetto, parole chiave, e riferimenti bibliografici.\n- *Dizionario dei dati*: Un documento che descrive tutte le variabili presenti nel dataset, inclusi i loro nomi, tipi, range di valori, categorie e qualsiasi altra informazione rilevante. Questo strumento è fondamentale per chiunque voglia comprendere o analizzare i dati.\n- *README*: Un file che fornisce una panoramica rapida dei file inclusi nel progetto, spiegando cosa contengono e come sono interconnessi. Il README è spesso il primo documento consultato e serve a orientare l'utente tra i vari file e risorse del progetto. \nQuesta documentazione non solo aiuta a mantenere il progetto organizzato, ma è anche cruciale per facilitare la collaborazione e l'archiviazione a lungo termine.\n\n## Dizionario dei dati\n\nApprofondiamo qui il problema della creazione del Dizionario dei dati. \n\nUn dizionario dei dati è un documento che descrive le caratteristiche di ciascuna variabile in un dataset. Include informazioni come il nome della variabile, il tipo di dato, il range di valori, le categorie (per le variabili categoriche), e altre informazioni rilevanti. Questo strumento è essenziale per comprendere e analizzare correttamente il dataset.\n\nSi presti particolare attenzione alle [guide di stile](https://datamgmtinedresearch.com/style) per la denominazione delle variabili e la codifica dei valori delle risposte.\n\n### Esempio in `R`\n\nEcco come tradurre i passi per creare un dizionario dei dati in R, utilizzando il pacchetto `tibble` per creare il dizionario e `writexl` o `readr` per esportarlo in formato `.xlsx` o `.csv`.\n\n1. *Identificare le variabili*: Elencare tutte le variabili presenti nel dataset.\n2. *Descrivere ogni variabile*: Per ciascuna variabile, definire il tipo (ad esempio, `integer`, `numeric`, `character`), il range di valori accettabili o le categorie, e fornire una descrizione chiara.\n3. *Salvare il dizionario dei dati*: Il dizionario può essere salvato in un file `.csv` o `.xlsx` per una facile consultazione.\n\n```r\nsvy\n```\n\nCreeremo un dizionario dei dati per un dataset di esempio e lo salveremo sia in formato CSV che Excel.\n\n```r\nlibrary(tibble)\nlibrary(readr)\nlibrary(writexl)\n\n# Creazione del Dizionario dei Dati\ndata_dict <- tibble(\n  `Variable Name` = c(\n    \"stu_id\",\n    \"svy_date\",\n    \"grade_level\",\n    \"math1\",\n    \"math2\",\n    \"math3\",\n    \"math4\"\n  ),\n  `Type` = c(\n    \"integer\",\n    \"datetime\",\n    \"integer\",\n    \"integer\",\n    \"integer\",\n    \"numeric\",\n    \"numeric\"\n  ),\n  `Description` = c(\n    \"Student ID\",\n    \"Survey Date\",\n    \"Grade Level\",\n    \"Math Response 1 (1: Strongly Disagree, 4: Strongly Agree)\",\n    \"Math Response 2 (1: Strongly Disagree, 4: Strongly Agree)\",\n    \"Math Response 3 (1: Strongly Disagree, 4: Strongly Agree)\",\n    \"Math Response 4 (1: Strongly Disagree, 4: Strongly Agree)\"\n  ),\n  `Range/Values` = c(\n    \"1347-1399\",\n    \"2023-02-13 to 2023-02-14\",\n    \"9-12\",\n    \"1-4\",\n    \"1-4\",\n    \"1.0-4.0 (NA allowed)\",\n    \"1.0-4.0 (NA allowed)\"\n  )\n)\n\n# Visualizza il Dizionario dei Dati\nprint(data_dict)\n\n# Salva il Dizionario dei Dati in un file CSV\nwrite_csv(data_dict, here::here(\"data\", \"processed\", \"data_dictionary.csv\"))\n\n# Salva il Dizionario dei Dati in un file Excel\nwrite_xlsx(data_dict, here::here(\"data\", \"processed\", \"data_dictionary.xlsx\"))\n```\n\nOutput Atteso: file CSV (`data_dictionary.csv`).\n\n```r\ndata_dict <- rio::import(\n  here::here(\"data\", \"processed\", \"data_dictionary.csv\")\n)\n```\n\n```r\nprint(data_dict)\n```\n\n#### Uso del pacchetto `dataMeta`\n\nIl pacchetto `dataMeta` è progettato per generare metadati e dizionari dei dati in modo strutturato.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(dataMeta)\nlibrary(tibble)\n\n# Descrizioni delle variabili\nvariable_descriptions <- c(\n  \"Student ID\",\n  \"Grade Level\",\n  \"Math Response 1 (1: Strongly Disagree, 4: Strongly Agree)\",\n  \"Math Response 2 (1: Strongly Disagree, 4: Strongly Agree)\",\n  \"Math Response 3 (1: Strongly Disagree, 4: Strongly Agree)\",\n  \"Math Response 4 (1: Strongly Disagree, 4: Strongly Agree)\"\n)\n\nvar_type <- c(1, 0, 0, 0, 0, 0)\n\nlinker <- build_linker(\n  svy, \n  variable_description = variable_descriptions, \n  variable_type = var_type\n)\n\ndict <- build_dict(\n  my.data = svy, \n  linker = linker, \n  option_description = NULL, \n  prompt_varopts = FALSE\n)\n\nkable(dict, format = \"html\", caption = \"Data dictionary for original dataset\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table>\n<caption>Data dictionary for original dataset</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> variable_name </th>\n   <th style=\"text-align:left;\"> variable_description </th>\n   <th style=\"text-align:left;\"> variable_options </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> grade_level </td>\n   <td style=\"text-align:left;\"> Grade Level </td>\n   <td style=\"text-align:left;\"> 9 to 12 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> math1 </td>\n   <td style=\"text-align:left;\"> Math Response 1 (1: Strongly Disagree, 4: Strongly Agree) </td>\n   <td style=\"text-align:left;\"> 2 to 4 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> math2 </td>\n   <td style=\"text-align:left;\"> Math Response 2 (1: Strongly Disagree, 4: Strongly Agree) </td>\n   <td style=\"text-align:left;\"> 1 to 4 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> math3 </td>\n   <td style=\"text-align:left;\"> Math Response 3 (1: Strongly Disagree, 4: Strongly Agree) </td>\n   <td style=\"text-align:left;\"> 2 to 4 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> math4 </td>\n   <td style=\"text-align:left;\"> Math Response 4 (1: Strongly Disagree, 4: Strongly Agree) </td>\n   <td style=\"text-align:left;\"> 1 to 4 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> stu_id </td>\n   <td style=\"text-align:left;\"> Student ID </td>\n   <td style=\"text-align:left;\"> 1347 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:left;\"> 1368 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:left;\"> 1377 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:left;\"> 1387 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:left;\"> 1399 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n#### Uso del pacchetto `skimr`\n\nIl pacchetto `skimr` è utile per generare riassunti dettagliati delle variabili, che possono essere utilizzati come base per un dizionario.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(skimr)\n\n# Riassunto del dataset\nskim_dict <- skim(svy)\n\nkable(skim_dict, format = \"html\", caption = \"Data dictionary for original dataset\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table>\n<caption>Data dictionary for original dataset</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> skim_type </th>\n   <th style=\"text-align:left;\"> skim_variable </th>\n   <th style=\"text-align:right;\"> n_missing </th>\n   <th style=\"text-align:right;\"> complete_rate </th>\n   <th style=\"text-align:right;\"> numeric.mean </th>\n   <th style=\"text-align:right;\"> numeric.sd </th>\n   <th style=\"text-align:right;\"> numeric.p0 </th>\n   <th style=\"text-align:right;\"> numeric.p25 </th>\n   <th style=\"text-align:right;\"> numeric.p50 </th>\n   <th style=\"text-align:right;\"> numeric.p75 </th>\n   <th style=\"text-align:right;\"> numeric.p100 </th>\n   <th style=\"text-align:left;\"> numeric.hist </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> numeric </td>\n   <td style=\"text-align:left;\"> stu_id </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1375.6 </td>\n   <td style=\"text-align:right;\"> 19.718 </td>\n   <td style=\"text-align:right;\"> 1347 </td>\n   <td style=\"text-align:right;\"> 1368 </td>\n   <td style=\"text-align:right;\"> 1377 </td>\n   <td style=\"text-align:right;\"> 1387 </td>\n   <td style=\"text-align:right;\"> 1399 </td>\n   <td style=\"text-align:left;\"> ▃▁▇▃▃ </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> numeric </td>\n   <td style=\"text-align:left;\"> grade_level </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 10.2 </td>\n   <td style=\"text-align:right;\"> 1.304 </td>\n   <td style=\"text-align:right;\"> 9 </td>\n   <td style=\"text-align:right;\"> 9 </td>\n   <td style=\"text-align:right;\"> 10 </td>\n   <td style=\"text-align:right;\"> 11 </td>\n   <td style=\"text-align:right;\"> 12 </td>\n   <td style=\"text-align:left;\"> ▇▃▁▃▃ </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> numeric </td>\n   <td style=\"text-align:left;\"> math1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 3.2 </td>\n   <td style=\"text-align:right;\"> 0.837 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:left;\"> ▃▁▇▁▇ </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> numeric </td>\n   <td style=\"text-align:left;\"> math2 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 2.2 </td>\n   <td style=\"text-align:right;\"> 1.304 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:left;\"> ▇▃▁▃▃ </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> numeric </td>\n   <td style=\"text-align:left;\"> math3 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 3.2 </td>\n   <td style=\"text-align:right;\"> 0.837 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:left;\"> ▃▁▇▁▇ </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> numeric </td>\n   <td style=\"text-align:left;\"> math4 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 2.8 </td>\n   <td style=\"text-align:right;\"> 1.304 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:left;\"> ▃▃▁▃▇ </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n## Riflessioni conclusive {.unnumbered .unlisted}\n\nNel processo di analisi dei dati, la fase di pulizia e pre-elaborazione è cruciale per garantire la qualità e l'integrità dei risultati finali. Sebbene questa fase possa sembrare meno interessante rispetto all'analisi vera e propria, essa costituisce la base su cui si costruiscono tutte le successive elaborazioni e interpretazioni. Attraverso una serie di passaggi strutturati, come quelli illustrati in questo capitolo, è possibile trasformare dati grezzi e disordinati in un dataset pulito, coerente e pronto per l'analisi. La cura nella gestione dei dati, dalla rimozione di duplicati alla creazione di un dizionario dei dati, è fondamentale per ottenere risultati affidabili e riproducibili.\n\n\n## Esercizi {.unnumbered .unlisted}\n\n::: {.callout-tip title=\"Esercizio\" collapse=\"true\"}\nIn questo esercizio, applicherai le tecniche di pulizia e preprocessing dei dati utilizzando il dataset SWLS. Il tuo compito è seguire i passaggi descritti per trasformare il dataset in una forma pronta per l'analisi.\n\n**Istruzioni**\n\n1. **Importa i dati SWLS** (Survey of Life Satisfaction Scale). Introduci almeno due dati mancanti nei dati e almeno un duplicato.\n2. **Controlla i dati**: verifica la struttura e individua eventuali anomalie.\n3. **Pulisci i dati**:\n   - Rimuovi eventuali duplicati.\n   - Gestisci i valori mancanti in modo appropriato.\n   - Rinomina le variabili per una maggiore chiarezza.\n   - Standardizza alcune variabili per l'analisi.\n4. **Documenta le modifiche effettuate**.\n5. **Esporta il dataset pulito**.\n\n**Consegna**\n\n- Salva il tuo file Quarto con il nome `swls_cleaning.qmd`.\n- Usa questo header YAML:\n\n  ```\n  ---\n  title: \"Assegnamento: Pulizia e Preprocessing dei Dati SWLS\"\n  author: \"Nome Studente\"\n  date: \"2025-09-09\"\n  format: html\n  ---\n```\n- Assicurati che il codice sia commentato e spiegato chiaramente.\n- Esporta il dataset pulito e allegalo alla consegna.\n:::\n\n\n::: {.callout-note collapse=true title=\"Informazioni sull'ambiente di sviluppo\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsessionInfo()\n#> R version 4.5.1 (2025-06-13)\n#> Platform: aarch64-apple-darwin20\n#> Running under: macOS Sequoia 15.6.1\n#> \n#> Matrix products: default\n#> BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \n#> LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n#> \n#> locale:\n#> [1] C/UTF-8/C/C/C/C\n#> \n#> time zone: Europe/Rome\n#> tzcode source: internal\n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets  methods   base     \n#> \n#> other attached packages:\n#>  [1] skimr_2.2.1           dataMeta_0.1.1        knitr_1.50           \n#>  [4] purrr_1.1.0           pointblank_0.12.2     haven_2.5.5          \n#>  [7] labelled_2.14.1       mice_3.18.0           pillar_1.11.0        \n#> [10] tinytable_0.13.0      patchwork_1.3.2       ggdist_3.3.3         \n#> [13] tidybayes_3.0.7       bayesplot_1.14.0      ggplot2_3.5.2        \n#> [16] reliabilitydiag_0.2.1 priorsense_1.1.1      posterior_1.6.1      \n#> [19] loo_2.8.0             rstan_2.32.7          StanHeaders_2.32.10  \n#> [22] brms_2.22.0           Rcpp_1.1.0            sessioninfo_1.2.3    \n#> [25] conflicted_1.2.0      janitor_2.2.1         matrixStats_1.5.0    \n#> [28] modelr_0.1.11         tibble_3.3.0          dplyr_1.1.4          \n#> [31] tidyr_1.3.1           rio_1.2.3             here_1.0.1           \n#> \n#> loaded via a namespace (and not attached):\n#>   [1] RColorBrewer_1.1-3    tensorA_0.36.2.1      jsonlite_2.0.0       \n#>   [4] shape_1.4.6.1         magrittr_2.0.3        TH.data_1.1-4        \n#>   [7] estimability_1.5.1    jomo_2.7-6            farver_2.1.2         \n#>  [10] nloptr_2.2.1          rmarkdown_2.29        ragg_1.5.0           \n#>  [13] vctrs_0.6.5           memoise_2.0.1         minqa_1.2.8          \n#>  [16] base64enc_0.1-3       htmltools_0.5.8.1     forcats_1.0.0        \n#>  [19] distributional_0.5.0  curl_7.0.0            broom_1.0.9          \n#>  [22] mitml_0.4-5           htmlwidgets_1.6.4     sandwich_3.1-1       \n#>  [25] emmeans_1.11.2-8      zoo_1.8-14            lubridate_1.9.4      \n#>  [28] cachem_1.1.0          lifecycle_1.0.4       iterators_1.0.14     \n#>  [31] pkgconfig_2.0.3       Matrix_1.7-4          R6_2.6.1             \n#>  [34] fastmap_1.2.0         rbibutils_2.3         snakecase_0.11.1     \n#>  [37] digest_0.6.37         colorspace_2.1-1      rprojroot_2.1.1      \n#>  [40] textshaping_1.0.3     timechange_0.3.0      abind_1.4-8          \n#>  [43] compiler_4.5.1        withr_3.0.2           backports_1.5.0      \n#>  [46] inline_0.3.21         QuickJSR_1.8.0        pkgbuild_1.4.8       \n#>  [49] R.utils_2.13.0        pan_1.9               MASS_7.3-65          \n#>  [52] tools_4.5.1           nnet_7.3-20           R.oo_1.27.1          \n#>  [55] glue_1.8.0            nlme_3.1-168          grid_4.5.1           \n#>  [58] checkmate_2.3.3       generics_0.1.4        gtable_0.3.6         \n#>  [61] R.methodsS3_1.8.2     data.table_1.17.8     hms_1.1.3            \n#>  [64] utf8_1.2.6            foreach_1.5.2         stringr_1.5.1        \n#>  [67] splines_4.5.1         lattice_0.22-7        survival_3.8-3       \n#>  [70] tidyselect_1.2.1      reformulas_0.4.1      arrayhelpers_1.1-0   \n#>  [73] gridExtra_2.3         V8_7.0.0              stats4_4.5.1         \n#>  [76] xfun_0.53             bridgesampling_1.1-2  stringi_1.8.7        \n#>  [79] pacman_0.5.1          boot_1.3-32           evaluate_1.0.5       \n#>  [82] codetools_0.2-20      cli_3.6.5             blastula_0.3.6       \n#>  [85] RcppParallel_5.1.11-1 rpart_4.1.24          xtable_1.8-4         \n#>  [88] systemfonts_1.2.3     Rdpack_2.6.4          repr_1.1.7           \n#>  [91] coda_0.19-4.1         svUnit_1.0.8          parallel_4.5.1       \n#>  [94] rstantools_2.5.0      Brobdingnag_1.2-9     lme4_1.1-37          \n#>  [97] glmnet_4.1-10         mvtnorm_1.3-3         scales_1.4.0         \n#> [100] rlang_1.1.6           multcomp_1.4-28\n```\n:::\n\n:::\n\n## Bibliografia {.unnumbered .unlisted}\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}